// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.11
// 	protoc        v6.33.2
// source: qclaogui/aiplatform/v1beta1/prediction_service.proto

package aiplatformpb

import (
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"

	_ "google.golang.org/genproto/googleapis/api/annotations"
	httpbody "google.golang.org/genproto/googleapis/api/httpbody"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	structpb "google.golang.org/protobuf/types/known/structpb"
	timestamppb "google.golang.org/protobuf/types/known/timestamppb"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Blocked reason enumeration.
type GenerateContentResponse_PromptFeedback_BlockedReason int32

const (
	// Unspecified blocked reason.
	GenerateContentResponse_PromptFeedback_BLOCKED_REASON_UNSPECIFIED GenerateContentResponse_PromptFeedback_BlockedReason = 0
	// Candidates blocked due to safety.
	GenerateContentResponse_PromptFeedback_SAFETY GenerateContentResponse_PromptFeedback_BlockedReason = 1
	// Candidates blocked due to other reason.
	GenerateContentResponse_PromptFeedback_OTHER GenerateContentResponse_PromptFeedback_BlockedReason = 2
	// Candidates blocked due to the terms which are included from the
	// terminology blocklist.
	GenerateContentResponse_PromptFeedback_BLOCKLIST GenerateContentResponse_PromptFeedback_BlockedReason = 3
	// Candidates blocked due to prohibited content.
	GenerateContentResponse_PromptFeedback_PROHIBITED_CONTENT GenerateContentResponse_PromptFeedback_BlockedReason = 4
)

// Enum value maps for GenerateContentResponse_PromptFeedback_BlockedReason.
var (
	GenerateContentResponse_PromptFeedback_BlockedReason_name = map[int32]string{
		0: "BLOCKED_REASON_UNSPECIFIED",
		1: "SAFETY",
		2: "OTHER",
		3: "BLOCKLIST",
		4: "PROHIBITED_CONTENT",
	}
	GenerateContentResponse_PromptFeedback_BlockedReason_value = map[string]int32{
		"BLOCKED_REASON_UNSPECIFIED": 0,
		"SAFETY":                     1,
		"OTHER":                      2,
		"BLOCKLIST":                  3,
		"PROHIBITED_CONTENT":         4,
	}
)

func (x GenerateContentResponse_PromptFeedback_BlockedReason) Enum() *GenerateContentResponse_PromptFeedback_BlockedReason {
	p := new(GenerateContentResponse_PromptFeedback_BlockedReason)
	*p = x
	return p
}

func (x GenerateContentResponse_PromptFeedback_BlockedReason) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (GenerateContentResponse_PromptFeedback_BlockedReason) Descriptor() protoreflect.EnumDescriptor {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_enumTypes[0].Descriptor()
}

func (GenerateContentResponse_PromptFeedback_BlockedReason) Type() protoreflect.EnumType {
	return &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_enumTypes[0]
}

func (x GenerateContentResponse_PromptFeedback_BlockedReason) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use GenerateContentResponse_PromptFeedback_BlockedReason.Descriptor instead.
func (GenerateContentResponse_PromptFeedback_BlockedReason) EnumDescriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{21, 0, 0}
}

// Request message for
// [PredictionService.Predict][google.cloud.aiplatform.v1beta1.PredictionService.Predict].
type PredictRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the Endpoint requested to serve the prediction.
	// Format:
	// `projects/{project}/locations/{location}/endpoints/{endpoint}`
	Endpoint string `protobuf:"bytes,1,opt,name=endpoint,proto3" json:"endpoint,omitempty"`
	// Required. The instances that are the input to the prediction call.
	// A DeployedModel may have an upper limit on the number of instances it
	// supports per request, and when it is exceeded the prediction call errors
	// in case of AutoML Models, or, in case of customer created Models, the
	// behaviour is as documented by that Model.
	// The schema of any single instance may be specified via Endpoint's
	// DeployedModels'
	// [Model's][google.cloud.aiplatform.v1beta1.DeployedModel.model]
	// [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
	// [instance_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri].
	Instances []*structpb.Value `protobuf:"bytes,2,rep,name=instances,proto3" json:"instances,omitempty"`
	// The parameters that govern the prediction. The schema of the parameters may
	// be specified via Endpoint's DeployedModels' [Model's
	// ][google.cloud.aiplatform.v1beta1.DeployedModel.model]
	// [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
	// [parameters_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.parameters_schema_uri].
	Parameters    *structpb.Value `protobuf:"bytes,3,opt,name=parameters,proto3" json:"parameters,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *PredictRequest) Reset() {
	*x = PredictRequest{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PredictRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PredictRequest) ProtoMessage() {}

func (x *PredictRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PredictRequest.ProtoReflect.Descriptor instead.
func (*PredictRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{0}
}

func (x *PredictRequest) GetEndpoint() string {
	if x != nil {
		return x.Endpoint
	}
	return ""
}

func (x *PredictRequest) GetInstances() []*structpb.Value {
	if x != nil {
		return x.Instances
	}
	return nil
}

func (x *PredictRequest) GetParameters() *structpb.Value {
	if x != nil {
		return x.Parameters
	}
	return nil
}

// Response message for
// [PredictionService.Predict][google.cloud.aiplatform.v1beta1.PredictionService.Predict].
type PredictResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The predictions that are the output of the predictions call.
	// The schema of any single prediction may be specified via Endpoint's
	// DeployedModels' [Model's
	// ][google.cloud.aiplatform.v1beta1.DeployedModel.model]
	// [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
	// [prediction_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.prediction_schema_uri].
	Predictions []*structpb.Value `protobuf:"bytes,1,rep,name=predictions,proto3" json:"predictions,omitempty"`
	// ID of the Endpoint's DeployedModel that served this prediction.
	DeployedModelId string `protobuf:"bytes,2,opt,name=deployed_model_id,json=deployedModelId,proto3" json:"deployed_model_id,omitempty"`
	// Output only. The resource name of the Model which is deployed as the
	// DeployedModel that this prediction hits.
	Model string `protobuf:"bytes,3,opt,name=model,proto3" json:"model,omitempty"`
	// Output only. The version ID of the Model which is deployed as the
	// DeployedModel that this prediction hits.
	ModelVersionId string `protobuf:"bytes,5,opt,name=model_version_id,json=modelVersionId,proto3" json:"model_version_id,omitempty"`
	// Output only. The [display
	// name][google.cloud.aiplatform.v1beta1.Model.display_name] of the Model
	// which is deployed as the DeployedModel that this prediction hits.
	ModelDisplayName string `protobuf:"bytes,4,opt,name=model_display_name,json=modelDisplayName,proto3" json:"model_display_name,omitempty"`
	// Output only. Request-level metadata returned by the model. The metadata
	// type will be dependent upon the model implementation.
	Metadata      *structpb.Value `protobuf:"bytes,6,opt,name=metadata,proto3" json:"metadata,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *PredictResponse) Reset() {
	*x = PredictResponse{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PredictResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PredictResponse) ProtoMessage() {}

func (x *PredictResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PredictResponse.ProtoReflect.Descriptor instead.
func (*PredictResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{1}
}

func (x *PredictResponse) GetPredictions() []*structpb.Value {
	if x != nil {
		return x.Predictions
	}
	return nil
}

func (x *PredictResponse) GetDeployedModelId() string {
	if x != nil {
		return x.DeployedModelId
	}
	return ""
}

func (x *PredictResponse) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *PredictResponse) GetModelVersionId() string {
	if x != nil {
		return x.ModelVersionId
	}
	return ""
}

func (x *PredictResponse) GetModelDisplayName() string {
	if x != nil {
		return x.ModelDisplayName
	}
	return ""
}

func (x *PredictResponse) GetMetadata() *structpb.Value {
	if x != nil {
		return x.Metadata
	}
	return nil
}

// Request message for
// [PredictionService.RawPredict][google.cloud.aiplatform.v1beta1.PredictionService.RawPredict].
type RawPredictRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the Endpoint requested to serve the prediction.
	// Format:
	// `projects/{project}/locations/{location}/endpoints/{endpoint}`
	Endpoint string `protobuf:"bytes,1,opt,name=endpoint,proto3" json:"endpoint,omitempty"`
	// The prediction input. Supports HTTP headers and arbitrary data payload.
	//
	// A [DeployedModel][google.cloud.aiplatform.v1beta1.DeployedModel] may have
	// an upper limit on the number of instances it supports per request. When
	// this limit it is exceeded for an AutoML model, the
	// [RawPredict][google.cloud.aiplatform.v1beta1.PredictionService.RawPredict]
	// method returns an error. When this limit is exceeded for a custom-trained
	// model, the behavior varies depending on the model.
	//
	// You can specify the schema for each instance in the
	// [predict_schemata.instance_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri]
	// field when you create a [Model][google.cloud.aiplatform.v1beta1.Model].
	// This schema applies when you deploy the `Model` as a `DeployedModel` to an
	// [Endpoint][google.cloud.aiplatform.v1beta1.Endpoint] and use the
	// `RawPredict` method.
	HttpBody      *httpbody.HttpBody `protobuf:"bytes,2,opt,name=http_body,json=httpBody,proto3" json:"http_body,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *RawPredictRequest) Reset() {
	*x = RawPredictRequest{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RawPredictRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RawPredictRequest) ProtoMessage() {}

func (x *RawPredictRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RawPredictRequest.ProtoReflect.Descriptor instead.
func (*RawPredictRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{2}
}

func (x *RawPredictRequest) GetEndpoint() string {
	if x != nil {
		return x.Endpoint
	}
	return ""
}

func (x *RawPredictRequest) GetHttpBody() *httpbody.HttpBody {
	if x != nil {
		return x.HttpBody
	}
	return nil
}

// Request message for
// [PredictionService.StreamRawPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamRawPredict].
type StreamRawPredictRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the Endpoint requested to serve the prediction.
	// Format:
	// `projects/{project}/locations/{location}/endpoints/{endpoint}`
	Endpoint string `protobuf:"bytes,1,opt,name=endpoint,proto3" json:"endpoint,omitempty"`
	// The prediction input. Supports HTTP headers and arbitrary data payload.
	HttpBody      *httpbody.HttpBody `protobuf:"bytes,2,opt,name=http_body,json=httpBody,proto3" json:"http_body,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamRawPredictRequest) Reset() {
	*x = StreamRawPredictRequest{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamRawPredictRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamRawPredictRequest) ProtoMessage() {}

func (x *StreamRawPredictRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamRawPredictRequest.ProtoReflect.Descriptor instead.
func (*StreamRawPredictRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{3}
}

func (x *StreamRawPredictRequest) GetEndpoint() string {
	if x != nil {
		return x.Endpoint
	}
	return ""
}

func (x *StreamRawPredictRequest) GetHttpBody() *httpbody.HttpBody {
	if x != nil {
		return x.HttpBody
	}
	return nil
}

// Request message for
// [PredictionService.DirectPredict][google.cloud.aiplatform.v1beta1.PredictionService.DirectPredict].
type DirectPredictRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the Endpoint requested to serve the prediction.
	// Format:
	// `projects/{project}/locations/{location}/endpoints/{endpoint}`
	Endpoint string `protobuf:"bytes,1,opt,name=endpoint,proto3" json:"endpoint,omitempty"`
	// The prediction input.
	Inputs []*Tensor `protobuf:"bytes,2,rep,name=inputs,proto3" json:"inputs,omitempty"`
	// The parameters that govern the prediction.
	Parameters    *Tensor `protobuf:"bytes,3,opt,name=parameters,proto3" json:"parameters,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DirectPredictRequest) Reset() {
	*x = DirectPredictRequest{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DirectPredictRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DirectPredictRequest) ProtoMessage() {}

func (x *DirectPredictRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DirectPredictRequest.ProtoReflect.Descriptor instead.
func (*DirectPredictRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{4}
}

func (x *DirectPredictRequest) GetEndpoint() string {
	if x != nil {
		return x.Endpoint
	}
	return ""
}

func (x *DirectPredictRequest) GetInputs() []*Tensor {
	if x != nil {
		return x.Inputs
	}
	return nil
}

func (x *DirectPredictRequest) GetParameters() *Tensor {
	if x != nil {
		return x.Parameters
	}
	return nil
}

// Response message for
// [PredictionService.DirectPredict][google.cloud.aiplatform.v1beta1.PredictionService.DirectPredict].
type DirectPredictResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The prediction output.
	Outputs []*Tensor `protobuf:"bytes,1,rep,name=outputs,proto3" json:"outputs,omitempty"`
	// The parameters that govern the prediction.
	Parameters    *Tensor `protobuf:"bytes,2,opt,name=parameters,proto3" json:"parameters,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DirectPredictResponse) Reset() {
	*x = DirectPredictResponse{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DirectPredictResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DirectPredictResponse) ProtoMessage() {}

func (x *DirectPredictResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DirectPredictResponse.ProtoReflect.Descriptor instead.
func (*DirectPredictResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{5}
}

func (x *DirectPredictResponse) GetOutputs() []*Tensor {
	if x != nil {
		return x.Outputs
	}
	return nil
}

func (x *DirectPredictResponse) GetParameters() *Tensor {
	if x != nil {
		return x.Parameters
	}
	return nil
}

// Request message for
// [PredictionService.DirectRawPredict][google.cloud.aiplatform.v1beta1.PredictionService.DirectRawPredict].
type DirectRawPredictRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the Endpoint requested to serve the prediction.
	// Format:
	// `projects/{project}/locations/{location}/endpoints/{endpoint}`
	Endpoint string `protobuf:"bytes,1,opt,name=endpoint,proto3" json:"endpoint,omitempty"`
	// Fully qualified name of the API method being invoked to perform
	// predictions.
	//
	// Format:
	// `/namespace.Service/Method/`
	// Example:
	// `/tensorflow.serving.PredictionService/Predict`
	MethodName string `protobuf:"bytes,2,opt,name=method_name,json=methodName,proto3" json:"method_name,omitempty"`
	// The prediction input.
	Input         []byte `protobuf:"bytes,3,opt,name=input,proto3" json:"input,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DirectRawPredictRequest) Reset() {
	*x = DirectRawPredictRequest{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DirectRawPredictRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DirectRawPredictRequest) ProtoMessage() {}

func (x *DirectRawPredictRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DirectRawPredictRequest.ProtoReflect.Descriptor instead.
func (*DirectRawPredictRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{6}
}

func (x *DirectRawPredictRequest) GetEndpoint() string {
	if x != nil {
		return x.Endpoint
	}
	return ""
}

func (x *DirectRawPredictRequest) GetMethodName() string {
	if x != nil {
		return x.MethodName
	}
	return ""
}

func (x *DirectRawPredictRequest) GetInput() []byte {
	if x != nil {
		return x.Input
	}
	return nil
}

// Response message for
// [PredictionService.DirectRawPredict][google.cloud.aiplatform.v1beta1.PredictionService.DirectRawPredict].
type DirectRawPredictResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The prediction output.
	Output        []byte `protobuf:"bytes,1,opt,name=output,proto3" json:"output,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *DirectRawPredictResponse) Reset() {
	*x = DirectRawPredictResponse{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *DirectRawPredictResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*DirectRawPredictResponse) ProtoMessage() {}

func (x *DirectRawPredictResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use DirectRawPredictResponse.ProtoReflect.Descriptor instead.
func (*DirectRawPredictResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{7}
}

func (x *DirectRawPredictResponse) GetOutput() []byte {
	if x != nil {
		return x.Output
	}
	return nil
}

// Request message for
// [PredictionService.StreamDirectPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamDirectPredict].
//
// The first message must contain
// [endpoint][google.cloud.aiplatform.v1beta1.StreamDirectPredictRequest.endpoint]
// field and optionally [input][]. The subsequent messages must contain
// [input][].
type StreamDirectPredictRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the Endpoint requested to serve the prediction.
	// Format:
	// `projects/{project}/locations/{location}/endpoints/{endpoint}`
	Endpoint string `protobuf:"bytes,1,opt,name=endpoint,proto3" json:"endpoint,omitempty"`
	// Optional. The prediction input.
	Inputs []*Tensor `protobuf:"bytes,2,rep,name=inputs,proto3" json:"inputs,omitempty"`
	// Optional. The parameters that govern the prediction.
	Parameters    *Tensor `protobuf:"bytes,3,opt,name=parameters,proto3" json:"parameters,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamDirectPredictRequest) Reset() {
	*x = StreamDirectPredictRequest{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamDirectPredictRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamDirectPredictRequest) ProtoMessage() {}

func (x *StreamDirectPredictRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamDirectPredictRequest.ProtoReflect.Descriptor instead.
func (*StreamDirectPredictRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{8}
}

func (x *StreamDirectPredictRequest) GetEndpoint() string {
	if x != nil {
		return x.Endpoint
	}
	return ""
}

func (x *StreamDirectPredictRequest) GetInputs() []*Tensor {
	if x != nil {
		return x.Inputs
	}
	return nil
}

func (x *StreamDirectPredictRequest) GetParameters() *Tensor {
	if x != nil {
		return x.Parameters
	}
	return nil
}

// Response message for
// [PredictionService.StreamDirectPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamDirectPredict].
type StreamDirectPredictResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The prediction output.
	Outputs []*Tensor `protobuf:"bytes,1,rep,name=outputs,proto3" json:"outputs,omitempty"`
	// The parameters that govern the prediction.
	Parameters    *Tensor `protobuf:"bytes,2,opt,name=parameters,proto3" json:"parameters,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamDirectPredictResponse) Reset() {
	*x = StreamDirectPredictResponse{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamDirectPredictResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamDirectPredictResponse) ProtoMessage() {}

func (x *StreamDirectPredictResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamDirectPredictResponse.ProtoReflect.Descriptor instead.
func (*StreamDirectPredictResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{9}
}

func (x *StreamDirectPredictResponse) GetOutputs() []*Tensor {
	if x != nil {
		return x.Outputs
	}
	return nil
}

func (x *StreamDirectPredictResponse) GetParameters() *Tensor {
	if x != nil {
		return x.Parameters
	}
	return nil
}

// Request message for
// [PredictionService.StreamDirectRawPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamDirectRawPredict].
//
// The first message must contain
// [endpoint][google.cloud.aiplatform.v1beta1.StreamDirectRawPredictRequest.endpoint]
// and
// [method_name][google.cloud.aiplatform.v1beta1.StreamDirectRawPredictRequest.method_name]
// fields and optionally
// [input][google.cloud.aiplatform.v1beta1.StreamDirectRawPredictRequest.input].
// The subsequent messages must contain
// [input][google.cloud.aiplatform.v1beta1.StreamDirectRawPredictRequest.input].
// [method_name][google.cloud.aiplatform.v1beta1.StreamDirectRawPredictRequest.method_name]
// in the subsequent messages have no effect.
type StreamDirectRawPredictRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the Endpoint requested to serve the prediction.
	// Format:
	// `projects/{project}/locations/{location}/endpoints/{endpoint}`
	Endpoint string `protobuf:"bytes,1,opt,name=endpoint,proto3" json:"endpoint,omitempty"`
	// Optional. Fully qualified name of the API method being invoked to perform
	// predictions.
	//
	// Format:
	// `/namespace.Service/Method/`
	// Example:
	// `/tensorflow.serving.PredictionService/Predict`
	MethodName string `protobuf:"bytes,2,opt,name=method_name,json=methodName,proto3" json:"method_name,omitempty"`
	// Optional. The prediction input.
	Input         []byte `protobuf:"bytes,3,opt,name=input,proto3" json:"input,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamDirectRawPredictRequest) Reset() {
	*x = StreamDirectRawPredictRequest{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamDirectRawPredictRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamDirectRawPredictRequest) ProtoMessage() {}

func (x *StreamDirectRawPredictRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamDirectRawPredictRequest.ProtoReflect.Descriptor instead.
func (*StreamDirectRawPredictRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{10}
}

func (x *StreamDirectRawPredictRequest) GetEndpoint() string {
	if x != nil {
		return x.Endpoint
	}
	return ""
}

func (x *StreamDirectRawPredictRequest) GetMethodName() string {
	if x != nil {
		return x.MethodName
	}
	return ""
}

func (x *StreamDirectRawPredictRequest) GetInput() []byte {
	if x != nil {
		return x.Input
	}
	return nil
}

// Response message for
// [PredictionService.StreamDirectRawPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamDirectRawPredict].
type StreamDirectRawPredictResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The prediction output.
	Output        []byte `protobuf:"bytes,1,opt,name=output,proto3" json:"output,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamDirectRawPredictResponse) Reset() {
	*x = StreamDirectRawPredictResponse{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[11]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamDirectRawPredictResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamDirectRawPredictResponse) ProtoMessage() {}

func (x *StreamDirectRawPredictResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[11]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamDirectRawPredictResponse.ProtoReflect.Descriptor instead.
func (*StreamDirectRawPredictResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{11}
}

func (x *StreamDirectRawPredictResponse) GetOutput() []byte {
	if x != nil {
		return x.Output
	}
	return nil
}

// Request message for
// [PredictionService.StreamingPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamingPredict].
//
// The first message must contain
// [endpoint][google.cloud.aiplatform.v1beta1.StreamingPredictRequest.endpoint]
// field and optionally [input][]. The subsequent messages must contain
// [input][].
type StreamingPredictRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the Endpoint requested to serve the prediction.
	// Format:
	// `projects/{project}/locations/{location}/endpoints/{endpoint}`
	Endpoint string `protobuf:"bytes,1,opt,name=endpoint,proto3" json:"endpoint,omitempty"`
	// The prediction input.
	Inputs []*Tensor `protobuf:"bytes,2,rep,name=inputs,proto3" json:"inputs,omitempty"`
	// The parameters that govern the prediction.
	Parameters    *Tensor `protobuf:"bytes,3,opt,name=parameters,proto3" json:"parameters,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamingPredictRequest) Reset() {
	*x = StreamingPredictRequest{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[12]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamingPredictRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamingPredictRequest) ProtoMessage() {}

func (x *StreamingPredictRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[12]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamingPredictRequest.ProtoReflect.Descriptor instead.
func (*StreamingPredictRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{12}
}

func (x *StreamingPredictRequest) GetEndpoint() string {
	if x != nil {
		return x.Endpoint
	}
	return ""
}

func (x *StreamingPredictRequest) GetInputs() []*Tensor {
	if x != nil {
		return x.Inputs
	}
	return nil
}

func (x *StreamingPredictRequest) GetParameters() *Tensor {
	if x != nil {
		return x.Parameters
	}
	return nil
}

// Response message for
// [PredictionService.StreamingPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamingPredict].
type StreamingPredictResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The prediction output.
	Outputs []*Tensor `protobuf:"bytes,1,rep,name=outputs,proto3" json:"outputs,omitempty"`
	// The parameters that govern the prediction.
	Parameters    *Tensor `protobuf:"bytes,2,opt,name=parameters,proto3" json:"parameters,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamingPredictResponse) Reset() {
	*x = StreamingPredictResponse{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[13]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamingPredictResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamingPredictResponse) ProtoMessage() {}

func (x *StreamingPredictResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[13]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamingPredictResponse.ProtoReflect.Descriptor instead.
func (*StreamingPredictResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{13}
}

func (x *StreamingPredictResponse) GetOutputs() []*Tensor {
	if x != nil {
		return x.Outputs
	}
	return nil
}

func (x *StreamingPredictResponse) GetParameters() *Tensor {
	if x != nil {
		return x.Parameters
	}
	return nil
}

// Request message for
// [PredictionService.StreamingRawPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamingRawPredict].
//
// The first message must contain
// [endpoint][google.cloud.aiplatform.v1beta1.StreamingRawPredictRequest.endpoint]
// and
// [method_name][google.cloud.aiplatform.v1beta1.StreamingRawPredictRequest.method_name]
// fields and optionally
// [input][google.cloud.aiplatform.v1beta1.StreamingRawPredictRequest.input].
// The subsequent messages must contain
// [input][google.cloud.aiplatform.v1beta1.StreamingRawPredictRequest.input].
// [method_name][google.cloud.aiplatform.v1beta1.StreamingRawPredictRequest.method_name]
// in the subsequent messages have no effect.
type StreamingRawPredictRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the Endpoint requested to serve the prediction.
	// Format:
	// `projects/{project}/locations/{location}/endpoints/{endpoint}`
	Endpoint string `protobuf:"bytes,1,opt,name=endpoint,proto3" json:"endpoint,omitempty"`
	// Fully qualified name of the API method being invoked to perform
	// predictions.
	//
	// Format:
	// `/namespace.Service/Method/`
	// Example:
	// `/tensorflow.serving.PredictionService/Predict`
	MethodName string `protobuf:"bytes,2,opt,name=method_name,json=methodName,proto3" json:"method_name,omitempty"`
	// The prediction input.
	Input         []byte `protobuf:"bytes,3,opt,name=input,proto3" json:"input,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamingRawPredictRequest) Reset() {
	*x = StreamingRawPredictRequest{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[14]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamingRawPredictRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamingRawPredictRequest) ProtoMessage() {}

func (x *StreamingRawPredictRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[14]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamingRawPredictRequest.ProtoReflect.Descriptor instead.
func (*StreamingRawPredictRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{14}
}

func (x *StreamingRawPredictRequest) GetEndpoint() string {
	if x != nil {
		return x.Endpoint
	}
	return ""
}

func (x *StreamingRawPredictRequest) GetMethodName() string {
	if x != nil {
		return x.MethodName
	}
	return ""
}

func (x *StreamingRawPredictRequest) GetInput() []byte {
	if x != nil {
		return x.Input
	}
	return nil
}

// Response message for
// [PredictionService.StreamingRawPredict][google.cloud.aiplatform.v1beta1.PredictionService.StreamingRawPredict].
type StreamingRawPredictResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The prediction output.
	Output        []byte `protobuf:"bytes,1,opt,name=output,proto3" json:"output,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *StreamingRawPredictResponse) Reset() {
	*x = StreamingRawPredictResponse{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[15]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *StreamingRawPredictResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*StreamingRawPredictResponse) ProtoMessage() {}

func (x *StreamingRawPredictResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[15]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use StreamingRawPredictResponse.ProtoReflect.Descriptor instead.
func (*StreamingRawPredictResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{15}
}

func (x *StreamingRawPredictResponse) GetOutput() []byte {
	if x != nil {
		return x.Output
	}
	return nil
}

// Request message for
// [PredictionService.Explain][google.cloud.aiplatform.v1beta1.PredictionService.Explain].
type ExplainRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the Endpoint requested to serve the explanation.
	// Format:
	// `projects/{project}/locations/{location}/endpoints/{endpoint}`
	Endpoint string `protobuf:"bytes,1,opt,name=endpoint,proto3" json:"endpoint,omitempty"`
	// Required. The instances that are the input to the explanation call.
	// A DeployedModel may have an upper limit on the number of instances it
	// supports per request, and when it is exceeded the explanation call errors
	// in case of AutoML Models, or, in case of customer created Models, the
	// behaviour is as documented by that Model.
	// The schema of any single instance may be specified via Endpoint's
	// DeployedModels'
	// [Model's][google.cloud.aiplatform.v1beta1.DeployedModel.model]
	// [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
	// [instance_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri].
	Instances []*structpb.Value `protobuf:"bytes,2,rep,name=instances,proto3" json:"instances,omitempty"`
	// The parameters that govern the prediction. The schema of the parameters may
	// be specified via Endpoint's DeployedModels' [Model's
	// ][google.cloud.aiplatform.v1beta1.DeployedModel.model]
	// [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
	// [parameters_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.parameters_schema_uri].
	Parameters *structpb.Value `protobuf:"bytes,4,opt,name=parameters,proto3" json:"parameters,omitempty"`
	// If specified, overrides the
	// [explanation_spec][google.cloud.aiplatform.v1beta1.DeployedModel.explanation_spec]
	// of the DeployedModel. Can be used for explaining prediction results with
	// different configurations, such as:
	//   - Explaining top-5 predictions results as opposed to top-1;
	//   - Increasing path count or step count of the attribution methods to reduce
	//     approximate errors;
	//   - Using different baselines for explaining the prediction results.
	ExplanationSpecOverride *ExplanationSpecOverride `protobuf:"bytes,5,opt,name=explanation_spec_override,json=explanationSpecOverride,proto3" json:"explanation_spec_override,omitempty"`
	// Optional. This field is the same as the one above, but supports multiple
	// explanations to occur in parallel. The key can be any string. Each override
	// will be run against the model, then its explanations will be grouped
	// together.
	//
	// Note - these explanations are run **In Addition** to the default
	// Explanation in the deployed model.
	ConcurrentExplanationSpecOverride map[string]*ExplanationSpecOverride `protobuf:"bytes,6,rep,name=concurrent_explanation_spec_override,json=concurrentExplanationSpecOverride,proto3" json:"concurrent_explanation_spec_override,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// If specified, this ExplainRequest will be served by the chosen
	// DeployedModel, overriding
	// [Endpoint.traffic_split][google.cloud.aiplatform.v1beta1.Endpoint.traffic_split].
	DeployedModelId string `protobuf:"bytes,3,opt,name=deployed_model_id,json=deployedModelId,proto3" json:"deployed_model_id,omitempty"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *ExplainRequest) Reset() {
	*x = ExplainRequest{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[16]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ExplainRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ExplainRequest) ProtoMessage() {}

func (x *ExplainRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[16]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ExplainRequest.ProtoReflect.Descriptor instead.
func (*ExplainRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{16}
}

func (x *ExplainRequest) GetEndpoint() string {
	if x != nil {
		return x.Endpoint
	}
	return ""
}

func (x *ExplainRequest) GetInstances() []*structpb.Value {
	if x != nil {
		return x.Instances
	}
	return nil
}

func (x *ExplainRequest) GetParameters() *structpb.Value {
	if x != nil {
		return x.Parameters
	}
	return nil
}

func (x *ExplainRequest) GetExplanationSpecOverride() *ExplanationSpecOverride {
	if x != nil {
		return x.ExplanationSpecOverride
	}
	return nil
}

func (x *ExplainRequest) GetConcurrentExplanationSpecOverride() map[string]*ExplanationSpecOverride {
	if x != nil {
		return x.ConcurrentExplanationSpecOverride
	}
	return nil
}

func (x *ExplainRequest) GetDeployedModelId() string {
	if x != nil {
		return x.DeployedModelId
	}
	return ""
}

// Response message for
// [PredictionService.Explain][google.cloud.aiplatform.v1beta1.PredictionService.Explain].
type ExplainResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The explanations of the Model's
	// [PredictResponse.predictions][google.cloud.aiplatform.v1beta1.PredictResponse.predictions].
	//
	// It has the same number of elements as
	// [instances][google.cloud.aiplatform.v1beta1.ExplainRequest.instances] to be
	// explained.
	Explanations []*Explanation `protobuf:"bytes,1,rep,name=explanations,proto3" json:"explanations,omitempty"`
	// This field stores the results of the explanations run in parallel with
	// The default explanation strategy/method.
	ConcurrentExplanations map[string]*ExplainResponse_ConcurrentExplanation `protobuf:"bytes,4,rep,name=concurrent_explanations,json=concurrentExplanations,proto3" json:"concurrent_explanations,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// ID of the Endpoint's DeployedModel that served this explanation.
	DeployedModelId string `protobuf:"bytes,2,opt,name=deployed_model_id,json=deployedModelId,proto3" json:"deployed_model_id,omitempty"`
	// The predictions that are the output of the predictions call.
	// Same as
	// [PredictResponse.predictions][google.cloud.aiplatform.v1beta1.PredictResponse.predictions].
	Predictions   []*structpb.Value `protobuf:"bytes,3,rep,name=predictions,proto3" json:"predictions,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ExplainResponse) Reset() {
	*x = ExplainResponse{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[17]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ExplainResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ExplainResponse) ProtoMessage() {}

func (x *ExplainResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[17]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ExplainResponse.ProtoReflect.Descriptor instead.
func (*ExplainResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{17}
}

func (x *ExplainResponse) GetExplanations() []*Explanation {
	if x != nil {
		return x.Explanations
	}
	return nil
}

func (x *ExplainResponse) GetConcurrentExplanations() map[string]*ExplainResponse_ConcurrentExplanation {
	if x != nil {
		return x.ConcurrentExplanations
	}
	return nil
}

func (x *ExplainResponse) GetDeployedModelId() string {
	if x != nil {
		return x.DeployedModelId
	}
	return ""
}

func (x *ExplainResponse) GetPredictions() []*structpb.Value {
	if x != nil {
		return x.Predictions
	}
	return nil
}

// Request message for
// [PredictionService.CountTokens][google.cloud.aiplatform.v1beta1.PredictionService.CountTokens].
type CountTokensRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the Endpoint requested to perform token counting.
	// Format:
	// `projects/{project}/locations/{location}/endpoints/{endpoint}`
	Endpoint string `protobuf:"bytes,1,opt,name=endpoint,proto3" json:"endpoint,omitempty"`
	// Optional. The name of the publisher model requested to serve the
	// prediction. Format:
	// `projects/{project}/locations/{location}/publishers/*/models/*`
	Model string `protobuf:"bytes,3,opt,name=model,proto3" json:"model,omitempty"`
	// Optional. The instances that are the input to token counting call.
	// Schema is identical to the prediction schema of the underlying model.
	Instances []*structpb.Value `protobuf:"bytes,2,rep,name=instances,proto3" json:"instances,omitempty"`
	// Optional. Input content.
	Contents []*Content `protobuf:"bytes,4,rep,name=contents,proto3" json:"contents,omitempty"`
	// Optional. The user provided system instructions for the model.
	// Note: only text should be used in parts and content in each part will be in
	// a separate paragraph.
	SystemInstruction *Content `protobuf:"bytes,5,opt,name=system_instruction,json=systemInstruction,proto3,oneof" json:"system_instruction,omitempty"`
	// Optional. A list of `Tools` the model may use to generate the next
	// response.
	//
	// A `Tool` is a piece of code that enables the system to interact with
	// external systems to perform an action, or set of actions, outside of
	// knowledge and scope of the model.
	Tools []*Tool `protobuf:"bytes,6,rep,name=tools,proto3" json:"tools,omitempty"`
	// Optional. Generation config that the model will use to generate the
	// response.
	GenerationConfig *GenerationConfig `protobuf:"bytes,7,opt,name=generation_config,json=generationConfig,proto3,oneof" json:"generation_config,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *CountTokensRequest) Reset() {
	*x = CountTokensRequest{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[18]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CountTokensRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CountTokensRequest) ProtoMessage() {}

func (x *CountTokensRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[18]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CountTokensRequest.ProtoReflect.Descriptor instead.
func (*CountTokensRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{18}
}

func (x *CountTokensRequest) GetEndpoint() string {
	if x != nil {
		return x.Endpoint
	}
	return ""
}

func (x *CountTokensRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *CountTokensRequest) GetInstances() []*structpb.Value {
	if x != nil {
		return x.Instances
	}
	return nil
}

func (x *CountTokensRequest) GetContents() []*Content {
	if x != nil {
		return x.Contents
	}
	return nil
}

func (x *CountTokensRequest) GetSystemInstruction() *Content {
	if x != nil {
		return x.SystemInstruction
	}
	return nil
}

func (x *CountTokensRequest) GetTools() []*Tool {
	if x != nil {
		return x.Tools
	}
	return nil
}

func (x *CountTokensRequest) GetGenerationConfig() *GenerationConfig {
	if x != nil {
		return x.GenerationConfig
	}
	return nil
}

// Response message for
// [PredictionService.CountTokens][google.cloud.aiplatform.v1beta1.PredictionService.CountTokens].
type CountTokensResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The total number of tokens counted across all instances from the request.
	TotalTokens int32 `protobuf:"varint,1,opt,name=total_tokens,json=totalTokens,proto3" json:"total_tokens,omitempty"`
	// The total number of billable characters counted across all instances from
	// the request.
	TotalBillableCharacters int32 `protobuf:"varint,2,opt,name=total_billable_characters,json=totalBillableCharacters,proto3" json:"total_billable_characters,omitempty"`
	// Output only. List of modalities that were processed in the request input.
	PromptTokensDetails []*ModalityTokenCount `protobuf:"bytes,3,rep,name=prompt_tokens_details,json=promptTokensDetails,proto3" json:"prompt_tokens_details,omitempty"`
	unknownFields       protoimpl.UnknownFields
	sizeCache           protoimpl.SizeCache
}

func (x *CountTokensResponse) Reset() {
	*x = CountTokensResponse{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[19]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CountTokensResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CountTokensResponse) ProtoMessage() {}

func (x *CountTokensResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[19]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CountTokensResponse.ProtoReflect.Descriptor instead.
func (*CountTokensResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{19}
}

func (x *CountTokensResponse) GetTotalTokens() int32 {
	if x != nil {
		return x.TotalTokens
	}
	return 0
}

func (x *CountTokensResponse) GetTotalBillableCharacters() int32 {
	if x != nil {
		return x.TotalBillableCharacters
	}
	return 0
}

func (x *CountTokensResponse) GetPromptTokensDetails() []*ModalityTokenCount {
	if x != nil {
		return x.PromptTokensDetails
	}
	return nil
}

// Request message for [PredictionService.GenerateContent].
type GenerateContentRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The fully qualified name of the publisher model or tuned model
	// endpoint to use.
	//
	// Publisher model format:
	// `projects/{project}/locations/{location}/publishers/*/models/*`
	//
	// Tuned model endpoint format:
	// `projects/{project}/locations/{location}/endpoints/{endpoint}`
	Model string `protobuf:"bytes,5,opt,name=model,proto3" json:"model,omitempty"`
	// Required. The content of the current conversation with the model.
	//
	// For single-turn queries, this is a single instance. For multi-turn queries,
	// this is a repeated field that contains conversation history + latest
	// request.
	Contents []*Content `protobuf:"bytes,2,rep,name=contents,proto3" json:"contents,omitempty"`
	// Optional. The user provided system instructions for the model.
	// Note: only text should be used in parts and content in each part will be in
	// a separate paragraph.
	SystemInstruction *Content `protobuf:"bytes,8,opt,name=system_instruction,json=systemInstruction,proto3,oneof" json:"system_instruction,omitempty"`
	// Optional. The name of the cached content used as context to serve the
	// prediction. Note: only used in explicit caching, where users can have
	// control over caching (e.g. what content to cache) and enjoy guaranteed cost
	// savings. Format:
	// `projects/{project}/locations/{location}/cachedContents/{cachedContent}`
	CachedContent string `protobuf:"bytes,9,opt,name=cached_content,json=cachedContent,proto3" json:"cached_content,omitempty"`
	// Optional. A list of `Tools` the model may use to generate the next
	// response.
	//
	// A `Tool` is a piece of code that enables the system to interact with
	// external systems to perform an action, or set of actions, outside of
	// knowledge and scope of the model.
	Tools []*Tool `protobuf:"bytes,6,rep,name=tools,proto3" json:"tools,omitempty"`
	// Optional. Tool config. This config is shared for all tools provided in the
	// request.
	ToolConfig *ToolConfig `protobuf:"bytes,7,opt,name=tool_config,json=toolConfig,proto3" json:"tool_config,omitempty"`
	// Optional. The labels with user-defined metadata for the request. It is used
	// for billing and reporting only.
	//
	// Label keys and values can be no longer than 63 characters
	// (Unicode codepoints) and can only contain lowercase letters, numeric
	// characters, underscores, and dashes. International characters are allowed.
	// Label values are optional. Label keys must start with a letter.
	Labels map[string]string `protobuf:"bytes,10,rep,name=labels,proto3" json:"labels,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// Optional. Per request settings for blocking unsafe content.
	// Enforced on GenerateContentResponse.candidates.
	SafetySettings []*SafetySetting `protobuf:"bytes,3,rep,name=safety_settings,json=safetySettings,proto3" json:"safety_settings,omitempty"`
	// Optional. Generation config.
	GenerationConfig *GenerationConfig `protobuf:"bytes,4,opt,name=generation_config,json=generationConfig,proto3" json:"generation_config,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *GenerateContentRequest) Reset() {
	*x = GenerateContentRequest{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[20]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateContentRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateContentRequest) ProtoMessage() {}

func (x *GenerateContentRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[20]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateContentRequest.ProtoReflect.Descriptor instead.
func (*GenerateContentRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{20}
}

func (x *GenerateContentRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *GenerateContentRequest) GetContents() []*Content {
	if x != nil {
		return x.Contents
	}
	return nil
}

func (x *GenerateContentRequest) GetSystemInstruction() *Content {
	if x != nil {
		return x.SystemInstruction
	}
	return nil
}

func (x *GenerateContentRequest) GetCachedContent() string {
	if x != nil {
		return x.CachedContent
	}
	return ""
}

func (x *GenerateContentRequest) GetTools() []*Tool {
	if x != nil {
		return x.Tools
	}
	return nil
}

func (x *GenerateContentRequest) GetToolConfig() *ToolConfig {
	if x != nil {
		return x.ToolConfig
	}
	return nil
}

func (x *GenerateContentRequest) GetLabels() map[string]string {
	if x != nil {
		return x.Labels
	}
	return nil
}

func (x *GenerateContentRequest) GetSafetySettings() []*SafetySetting {
	if x != nil {
		return x.SafetySettings
	}
	return nil
}

func (x *GenerateContentRequest) GetGenerationConfig() *GenerationConfig {
	if x != nil {
		return x.GenerationConfig
	}
	return nil
}

// Response message for [PredictionService.GenerateContent].
type GenerateContentResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. Generated candidates.
	Candidates []*Candidate `protobuf:"bytes,2,rep,name=candidates,proto3" json:"candidates,omitempty"`
	// Output only. The model version used to generate the response.
	ModelVersion string `protobuf:"bytes,11,opt,name=model_version,json=modelVersion,proto3" json:"model_version,omitempty"`
	// Output only. Timestamp when the request is made to the server.
	CreateTime *timestamppb.Timestamp `protobuf:"bytes,12,opt,name=create_time,json=createTime,proto3" json:"create_time,omitempty"`
	// Output only. response_id is used to identify each response. It is the
	// encoding of the event_id.
	ResponseId string `protobuf:"bytes,13,opt,name=response_id,json=responseId,proto3" json:"response_id,omitempty"`
	// Output only. Content filter results for a prompt sent in the request.
	// Note: Sent only in the first stream chunk.
	// Only happens when no candidates were generated due to content violations.
	PromptFeedback *GenerateContentResponse_PromptFeedback `protobuf:"bytes,3,opt,name=prompt_feedback,json=promptFeedback,proto3" json:"prompt_feedback,omitempty"`
	// Usage metadata about the response(s).
	UsageMetadata *GenerateContentResponse_UsageMetadata `protobuf:"bytes,4,opt,name=usage_metadata,json=usageMetadata,proto3" json:"usage_metadata,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerateContentResponse) Reset() {
	*x = GenerateContentResponse{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[21]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateContentResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateContentResponse) ProtoMessage() {}

func (x *GenerateContentResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[21]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateContentResponse.ProtoReflect.Descriptor instead.
func (*GenerateContentResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{21}
}

func (x *GenerateContentResponse) GetCandidates() []*Candidate {
	if x != nil {
		return x.Candidates
	}
	return nil
}

func (x *GenerateContentResponse) GetModelVersion() string {
	if x != nil {
		return x.ModelVersion
	}
	return ""
}

func (x *GenerateContentResponse) GetCreateTime() *timestamppb.Timestamp {
	if x != nil {
		return x.CreateTime
	}
	return nil
}

func (x *GenerateContentResponse) GetResponseId() string {
	if x != nil {
		return x.ResponseId
	}
	return ""
}

func (x *GenerateContentResponse) GetPromptFeedback() *GenerateContentResponse_PromptFeedback {
	if x != nil {
		return x.PromptFeedback
	}
	return nil
}

func (x *GenerateContentResponse) GetUsageMetadata() *GenerateContentResponse_UsageMetadata {
	if x != nil {
		return x.UsageMetadata
	}
	return nil
}

// Request message for [PredictionService.ChatCompletions]
type ChatCompletionsRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the endpoint requested to serve the prediction.
	// Format:
	// `projects/{project}/locations/{location}/endpoints/{endpoint}`
	Endpoint string `protobuf:"bytes,1,opt,name=endpoint,proto3" json:"endpoint,omitempty"`
	// Optional. The prediction input. Supports HTTP headers and arbitrary data
	// payload.
	HttpBody      *httpbody.HttpBody `protobuf:"bytes,2,opt,name=http_body,json=httpBody,proto3" json:"http_body,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ChatCompletionsRequest) Reset() {
	*x = ChatCompletionsRequest{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[22]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ChatCompletionsRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ChatCompletionsRequest) ProtoMessage() {}

func (x *ChatCompletionsRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[22]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ChatCompletionsRequest.ProtoReflect.Descriptor instead.
func (*ChatCompletionsRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{22}
}

func (x *ChatCompletionsRequest) GetEndpoint() string {
	if x != nil {
		return x.Endpoint
	}
	return ""
}

func (x *ChatCompletionsRequest) GetHttpBody() *httpbody.HttpBody {
	if x != nil {
		return x.HttpBody
	}
	return nil
}

// Response message for [PredictionService.PredictLongRunning]
type PredictLongRunningResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The response of the long running operation.
	//
	// Types that are valid to be assigned to Response:
	//
	//	*PredictLongRunningResponse_GenerateVideoResponse
	Response      isPredictLongRunningResponse_Response `protobuf_oneof:"response"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *PredictLongRunningResponse) Reset() {
	*x = PredictLongRunningResponse{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[23]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PredictLongRunningResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PredictLongRunningResponse) ProtoMessage() {}

func (x *PredictLongRunningResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[23]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PredictLongRunningResponse.ProtoReflect.Descriptor instead.
func (*PredictLongRunningResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{23}
}

func (x *PredictLongRunningResponse) GetResponse() isPredictLongRunningResponse_Response {
	if x != nil {
		return x.Response
	}
	return nil
}

func (x *PredictLongRunningResponse) GetGenerateVideoResponse() *GenerateVideoResponse {
	if x != nil {
		if x, ok := x.Response.(*PredictLongRunningResponse_GenerateVideoResponse); ok {
			return x.GenerateVideoResponse
		}
	}
	return nil
}

type isPredictLongRunningResponse_Response interface {
	isPredictLongRunningResponse_Response()
}

type PredictLongRunningResponse_GenerateVideoResponse struct {
	// The response of the video generation prediction.
	GenerateVideoResponse *GenerateVideoResponse `protobuf:"bytes,1,opt,name=generate_video_response,json=generateVideoResponse,proto3,oneof"`
}

func (*PredictLongRunningResponse_GenerateVideoResponse) isPredictLongRunningResponse_Response() {}

// Metadata for PredictLongRunning long running operations.
type PredictLongRunningMetadata struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *PredictLongRunningMetadata) Reset() {
	*x = PredictLongRunningMetadata{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[24]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *PredictLongRunningMetadata) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*PredictLongRunningMetadata) ProtoMessage() {}

func (x *PredictLongRunningMetadata) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[24]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use PredictLongRunningMetadata.ProtoReflect.Descriptor instead.
func (*PredictLongRunningMetadata) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{24}
}

// Generate video response.
type GenerateVideoResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The cloud storage uris of the generated videos.
	GeneratedSamples []string `protobuf:"bytes,1,rep,name=generated_samples,json=generatedSamples,proto3" json:"generated_samples,omitempty"`
	// Returns if any videos were filtered due to RAI policies.
	RaiMediaFilteredCount *int32 `protobuf:"varint,2,opt,name=rai_media_filtered_count,json=raiMediaFilteredCount,proto3,oneof" json:"rai_media_filtered_count,omitempty"`
	// Returns rai failure reasons if any.
	RaiMediaFilteredReasons []string `protobuf:"bytes,3,rep,name=rai_media_filtered_reasons,json=raiMediaFilteredReasons,proto3" json:"rai_media_filtered_reasons,omitempty"`
	unknownFields           protoimpl.UnknownFields
	sizeCache               protoimpl.SizeCache
}

func (x *GenerateVideoResponse) Reset() {
	*x = GenerateVideoResponse{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[25]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateVideoResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateVideoResponse) ProtoMessage() {}

func (x *GenerateVideoResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[25]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateVideoResponse.ProtoReflect.Descriptor instead.
func (*GenerateVideoResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{25}
}

func (x *GenerateVideoResponse) GetGeneratedSamples() []string {
	if x != nil {
		return x.GeneratedSamples
	}
	return nil
}

func (x *GenerateVideoResponse) GetRaiMediaFilteredCount() int32 {
	if x != nil && x.RaiMediaFilteredCount != nil {
		return *x.RaiMediaFilteredCount
	}
	return 0
}

func (x *GenerateVideoResponse) GetRaiMediaFilteredReasons() []string {
	if x != nil {
		return x.RaiMediaFilteredReasons
	}
	return nil
}

// This message is a wrapper grouping Concurrent Explanations.
type ExplainResponse_ConcurrentExplanation struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The explanations of the Model's
	// [PredictResponse.predictions][google.cloud.aiplatform.v1beta1.PredictResponse.predictions].
	//
	// It has the same number of elements as
	// [instances][google.cloud.aiplatform.v1beta1.ExplainRequest.instances] to
	// be explained.
	Explanations  []*Explanation `protobuf:"bytes,1,rep,name=explanations,proto3" json:"explanations,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ExplainResponse_ConcurrentExplanation) Reset() {
	*x = ExplainResponse_ConcurrentExplanation{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[27]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ExplainResponse_ConcurrentExplanation) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ExplainResponse_ConcurrentExplanation) ProtoMessage() {}

func (x *ExplainResponse_ConcurrentExplanation) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[27]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ExplainResponse_ConcurrentExplanation.ProtoReflect.Descriptor instead.
func (*ExplainResponse_ConcurrentExplanation) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{17, 0}
}

func (x *ExplainResponse_ConcurrentExplanation) GetExplanations() []*Explanation {
	if x != nil {
		return x.Explanations
	}
	return nil
}

// Content filter results for a prompt sent in the request.
type GenerateContentResponse_PromptFeedback struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. Blocked reason.
	BlockReason GenerateContentResponse_PromptFeedback_BlockedReason `protobuf:"varint,1,opt,name=block_reason,json=blockReason,proto3,enum=qclaogui.aiplatform.v1beta1.GenerateContentResponse_PromptFeedback_BlockedReason" json:"block_reason,omitempty"`
	// Output only. Safety ratings.
	SafetyRatings []*SafetyRating `protobuf:"bytes,2,rep,name=safety_ratings,json=safetyRatings,proto3" json:"safety_ratings,omitempty"`
	// Output only. A readable block reason message.
	BlockReasonMessage string `protobuf:"bytes,3,opt,name=block_reason_message,json=blockReasonMessage,proto3" json:"block_reason_message,omitempty"`
	unknownFields      protoimpl.UnknownFields
	sizeCache          protoimpl.SizeCache
}

func (x *GenerateContentResponse_PromptFeedback) Reset() {
	*x = GenerateContentResponse_PromptFeedback{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[30]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateContentResponse_PromptFeedback) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateContentResponse_PromptFeedback) ProtoMessage() {}

func (x *GenerateContentResponse_PromptFeedback) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[30]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateContentResponse_PromptFeedback.ProtoReflect.Descriptor instead.
func (*GenerateContentResponse_PromptFeedback) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{21, 0}
}

func (x *GenerateContentResponse_PromptFeedback) GetBlockReason() GenerateContentResponse_PromptFeedback_BlockedReason {
	if x != nil {
		return x.BlockReason
	}
	return GenerateContentResponse_PromptFeedback_BLOCKED_REASON_UNSPECIFIED
}

func (x *GenerateContentResponse_PromptFeedback) GetSafetyRatings() []*SafetyRating {
	if x != nil {
		return x.SafetyRatings
	}
	return nil
}

func (x *GenerateContentResponse_PromptFeedback) GetBlockReasonMessage() string {
	if x != nil {
		return x.BlockReasonMessage
	}
	return ""
}

// Usage metadata about response(s).
type GenerateContentResponse_UsageMetadata struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Number of tokens in the request. When `cached_content` is set, this is
	// still the total effective prompt size meaning this includes the number of
	// tokens in the cached content.
	PromptTokenCount int32 `protobuf:"varint,1,opt,name=prompt_token_count,json=promptTokenCount,proto3" json:"prompt_token_count,omitempty"`
	// Number of tokens in the response(s).
	CandidatesTokenCount int32 `protobuf:"varint,2,opt,name=candidates_token_count,json=candidatesTokenCount,proto3" json:"candidates_token_count,omitempty"`
	// Output only. Number of tokens present in thoughts output.
	ThoughtsTokenCount int32 `protobuf:"varint,14,opt,name=thoughts_token_count,json=thoughtsTokenCount,proto3" json:"thoughts_token_count,omitempty"`
	// Total token count for prompt and response candidates.
	TotalTokenCount int32 `protobuf:"varint,3,opt,name=total_token_count,json=totalTokenCount,proto3" json:"total_token_count,omitempty"`
	// Output only. Number of tokens in the cached part in the input (the cached
	// content).
	CachedContentTokenCount int32 `protobuf:"varint,5,opt,name=cached_content_token_count,json=cachedContentTokenCount,proto3" json:"cached_content_token_count,omitempty"`
	// Output only. List of modalities that were processed in the request input.
	PromptTokensDetails []*ModalityTokenCount `protobuf:"bytes,9,rep,name=prompt_tokens_details,json=promptTokensDetails,proto3" json:"prompt_tokens_details,omitempty"`
	// Output only. List of modalities of the cached content in the request
	// input.
	CacheTokensDetails []*ModalityTokenCount `protobuf:"bytes,10,rep,name=cache_tokens_details,json=cacheTokensDetails,proto3" json:"cache_tokens_details,omitempty"`
	// Output only. List of modalities that were returned in the response.
	CandidatesTokensDetails []*ModalityTokenCount `protobuf:"bytes,11,rep,name=candidates_tokens_details,json=candidatesTokensDetails,proto3" json:"candidates_tokens_details,omitempty"`
	unknownFields           protoimpl.UnknownFields
	sizeCache               protoimpl.SizeCache
}

func (x *GenerateContentResponse_UsageMetadata) Reset() {
	*x = GenerateContentResponse_UsageMetadata{}
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[31]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateContentResponse_UsageMetadata) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateContentResponse_UsageMetadata) ProtoMessage() {}

func (x *GenerateContentResponse_UsageMetadata) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[31]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateContentResponse_UsageMetadata.ProtoReflect.Descriptor instead.
func (*GenerateContentResponse_UsageMetadata) Descriptor() ([]byte, []int) {
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP(), []int{21, 1}
}

func (x *GenerateContentResponse_UsageMetadata) GetPromptTokenCount() int32 {
	if x != nil {
		return x.PromptTokenCount
	}
	return 0
}

func (x *GenerateContentResponse_UsageMetadata) GetCandidatesTokenCount() int32 {
	if x != nil {
		return x.CandidatesTokenCount
	}
	return 0
}

func (x *GenerateContentResponse_UsageMetadata) GetThoughtsTokenCount() int32 {
	if x != nil {
		return x.ThoughtsTokenCount
	}
	return 0
}

func (x *GenerateContentResponse_UsageMetadata) GetTotalTokenCount() int32 {
	if x != nil {
		return x.TotalTokenCount
	}
	return 0
}

func (x *GenerateContentResponse_UsageMetadata) GetCachedContentTokenCount() int32 {
	if x != nil {
		return x.CachedContentTokenCount
	}
	return 0
}

func (x *GenerateContentResponse_UsageMetadata) GetPromptTokensDetails() []*ModalityTokenCount {
	if x != nil {
		return x.PromptTokensDetails
	}
	return nil
}

func (x *GenerateContentResponse_UsageMetadata) GetCacheTokensDetails() []*ModalityTokenCount {
	if x != nil {
		return x.CacheTokensDetails
	}
	return nil
}

func (x *GenerateContentResponse_UsageMetadata) GetCandidatesTokensDetails() []*ModalityTokenCount {
	if x != nil {
		return x.CandidatesTokensDetails
	}
	return nil
}

var File_qclaogui_aiplatform_v1beta1_prediction_service_proto protoreflect.FileDescriptor

const file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDesc = "" +
	"\n" +
	"4qclaogui/aiplatform/v1beta1/prediction_service.proto\x12\x1bqclaogui.aiplatform.v1beta1\x1a\x1cgoogle/api/annotations.proto\x1a\x17google/api/client.proto\x1a\x1fgoogle/api/field_behavior.proto\x1a\x19google/api/httpbody.proto\x1a\x19google/api/resource.proto\x1a\x1cgoogle/protobuf/struct.proto\x1a\x1fgoogle/protobuf/timestamp.proto\x1a)qclaogui/aiplatform/v1beta1/content.proto\x1a-qclaogui/aiplatform/v1beta1/explanation.proto\x1a&qclaogui/aiplatform/v1beta1/tool.proto\x1a'qclaogui/aiplatform/v1beta1/types.proto\"\xcd\x01\n" +
	"\x0ePredictRequest\x12G\n" +
	"\bendpoint\x18\x01 \x01(\tB+\xe2A\x01\x02\xfaA$\n" +
	"\"aiplatform.googleapis.com/EndpointR\bendpoint\x12:\n" +
	"\tinstances\x18\x02 \x03(\v2\x16.google.protobuf.ValueB\x04\xe2A\x01\x02R\tinstances\x126\n" +
	"\n" +
	"parameters\x18\x03 \x01(\v2\x16.google.protobuf.ValueR\n" +
	"parameters\"\xd5\x02\n" +
	"\x0fPredictResponse\x128\n" +
	"\vpredictions\x18\x01 \x03(\v2\x16.google.protobuf.ValueR\vpredictions\x12*\n" +
	"\x11deployed_model_id\x18\x02 \x01(\tR\x0fdeployedModelId\x12>\n" +
	"\x05model\x18\x03 \x01(\tB(\xe2A\x01\x03\xfaA!\n" +
	"\x1faiplatform.googleapis.com/ModelR\x05model\x12.\n" +
	"\x10model_version_id\x18\x05 \x01(\tB\x04\xe2A\x01\x03R\x0emodelVersionId\x122\n" +
	"\x12model_display_name\x18\x04 \x01(\tB\x04\xe2A\x01\x03R\x10modelDisplayName\x128\n" +
	"\bmetadata\x18\x06 \x01(\v2\x16.google.protobuf.ValueB\x04\xe2A\x01\x03R\bmetadata\"\x8f\x01\n" +
	"\x11RawPredictRequest\x12G\n" +
	"\bendpoint\x18\x01 \x01(\tB+\xe2A\x01\x02\xfaA$\n" +
	"\"aiplatform.googleapis.com/EndpointR\bendpoint\x121\n" +
	"\thttp_body\x18\x02 \x01(\v2\x14.google.api.HttpBodyR\bhttpBody\"\x95\x01\n" +
	"\x17StreamRawPredictRequest\x12G\n" +
	"\bendpoint\x18\x01 \x01(\tB+\xe2A\x01\x02\xfaA$\n" +
	"\"aiplatform.googleapis.com/EndpointR\bendpoint\x121\n" +
	"\thttp_body\x18\x02 \x01(\v2\x14.google.api.HttpBodyR\bhttpBody\"\xe1\x01\n" +
	"\x14DirectPredictRequest\x12G\n" +
	"\bendpoint\x18\x01 \x01(\tB+\xe2A\x01\x02\xfaA$\n" +
	"\"aiplatform.googleapis.com/EndpointR\bendpoint\x12;\n" +
	"\x06inputs\x18\x02 \x03(\v2#.qclaogui.aiplatform.v1beta1.TensorR\x06inputs\x12C\n" +
	"\n" +
	"parameters\x18\x03 \x01(\v2#.qclaogui.aiplatform.v1beta1.TensorR\n" +
	"parameters\"\x9b\x01\n" +
	"\x15DirectPredictResponse\x12=\n" +
	"\aoutputs\x18\x01 \x03(\v2#.qclaogui.aiplatform.v1beta1.TensorR\aoutputs\x12C\n" +
	"\n" +
	"parameters\x18\x02 \x01(\v2#.qclaogui.aiplatform.v1beta1.TensorR\n" +
	"parameters\"\x99\x01\n" +
	"\x17DirectRawPredictRequest\x12G\n" +
	"\bendpoint\x18\x01 \x01(\tB+\xe2A\x01\x02\xfaA$\n" +
	"\"aiplatform.googleapis.com/EndpointR\bendpoint\x12\x1f\n" +
	"\vmethod_name\x18\x02 \x01(\tR\n" +
	"methodName\x12\x14\n" +
	"\x05input\x18\x03 \x01(\fR\x05input\"2\n" +
	"\x18DirectRawPredictResponse\x12\x16\n" +
	"\x06output\x18\x01 \x01(\fR\x06output\"\xf3\x01\n" +
	"\x1aStreamDirectPredictRequest\x12G\n" +
	"\bendpoint\x18\x01 \x01(\tB+\xe2A\x01\x02\xfaA$\n" +
	"\"aiplatform.googleapis.com/EndpointR\bendpoint\x12A\n" +
	"\x06inputs\x18\x02 \x03(\v2#.qclaogui.aiplatform.v1beta1.TensorB\x04\xe2A\x01\x01R\x06inputs\x12I\n" +
	"\n" +
	"parameters\x18\x03 \x01(\v2#.qclaogui.aiplatform.v1beta1.TensorB\x04\xe2A\x01\x01R\n" +
	"parameters\"\xa1\x01\n" +
	"\x1bStreamDirectPredictResponse\x12=\n" +
	"\aoutputs\x18\x01 \x03(\v2#.qclaogui.aiplatform.v1beta1.TensorR\aoutputs\x12C\n" +
	"\n" +
	"parameters\x18\x02 \x01(\v2#.qclaogui.aiplatform.v1beta1.TensorR\n" +
	"parameters\"\xab\x01\n" +
	"\x1dStreamDirectRawPredictRequest\x12G\n" +
	"\bendpoint\x18\x01 \x01(\tB+\xe2A\x01\x02\xfaA$\n" +
	"\"aiplatform.googleapis.com/EndpointR\bendpoint\x12%\n" +
	"\vmethod_name\x18\x02 \x01(\tB\x04\xe2A\x01\x01R\n" +
	"methodName\x12\x1a\n" +
	"\x05input\x18\x03 \x01(\fB\x04\xe2A\x01\x01R\x05input\"8\n" +
	"\x1eStreamDirectRawPredictResponse\x12\x16\n" +
	"\x06output\x18\x01 \x01(\fR\x06output\"\xe4\x01\n" +
	"\x17StreamingPredictRequest\x12G\n" +
	"\bendpoint\x18\x01 \x01(\tB+\xe2A\x01\x02\xfaA$\n" +
	"\"aiplatform.googleapis.com/EndpointR\bendpoint\x12;\n" +
	"\x06inputs\x18\x02 \x03(\v2#.qclaogui.aiplatform.v1beta1.TensorR\x06inputs\x12C\n" +
	"\n" +
	"parameters\x18\x03 \x01(\v2#.qclaogui.aiplatform.v1beta1.TensorR\n" +
	"parameters\"\x9e\x01\n" +
	"\x18StreamingPredictResponse\x12=\n" +
	"\aoutputs\x18\x01 \x03(\v2#.qclaogui.aiplatform.v1beta1.TensorR\aoutputs\x12C\n" +
	"\n" +
	"parameters\x18\x02 \x01(\v2#.qclaogui.aiplatform.v1beta1.TensorR\n" +
	"parameters\"\x9c\x01\n" +
	"\x1aStreamingRawPredictRequest\x12G\n" +
	"\bendpoint\x18\x01 \x01(\tB+\xe2A\x01\x02\xfaA$\n" +
	"\"aiplatform.googleapis.com/EndpointR\bendpoint\x12\x1f\n" +
	"\vmethod_name\x18\x02 \x01(\tR\n" +
	"methodName\x12\x14\n" +
	"\x05input\x18\x03 \x01(\fR\x05input\"5\n" +
	"\x1bStreamingRawPredictResponse\x12\x16\n" +
	"\x06output\x18\x01 \x01(\fR\x06output\"\xa4\x05\n" +
	"\x0eExplainRequest\x12G\n" +
	"\bendpoint\x18\x01 \x01(\tB+\xe2A\x01\x02\xfaA$\n" +
	"\"aiplatform.googleapis.com/EndpointR\bendpoint\x12:\n" +
	"\tinstances\x18\x02 \x03(\v2\x16.google.protobuf.ValueB\x04\xe2A\x01\x02R\tinstances\x126\n" +
	"\n" +
	"parameters\x18\x04 \x01(\v2\x16.google.protobuf.ValueR\n" +
	"parameters\x12p\n" +
	"\x19explanation_spec_override\x18\x05 \x01(\v24.qclaogui.aiplatform.v1beta1.ExplanationSpecOverrideR\x17explanationSpecOverride\x12\xa9\x01\n" +
	"$concurrent_explanation_spec_override\x18\x06 \x03(\v2R.qclaogui.aiplatform.v1beta1.ExplainRequest.ConcurrentExplanationSpecOverrideEntryB\x04\xe2A\x01\x01R!concurrentExplanationSpecOverride\x12*\n" +
	"\x11deployed_model_id\x18\x03 \x01(\tR\x0fdeployedModelId\x1a\x8a\x01\n" +
	"&ConcurrentExplanationSpecOverrideEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12J\n" +
	"\x05value\x18\x02 \x01(\v24.qclaogui.aiplatform.v1beta1.ExplanationSpecOverrideR\x05value:\x028\x01\"\xc0\x04\n" +
	"\x0fExplainResponse\x12L\n" +
	"\fexplanations\x18\x01 \x03(\v2(.qclaogui.aiplatform.v1beta1.ExplanationR\fexplanations\x12\x81\x01\n" +
	"\x17concurrent_explanations\x18\x04 \x03(\v2H.qclaogui.aiplatform.v1beta1.ExplainResponse.ConcurrentExplanationsEntryR\x16concurrentExplanations\x12*\n" +
	"\x11deployed_model_id\x18\x02 \x01(\tR\x0fdeployedModelId\x128\n" +
	"\vpredictions\x18\x03 \x03(\v2\x16.google.protobuf.ValueR\vpredictions\x1ae\n" +
	"\x15ConcurrentExplanation\x12L\n" +
	"\fexplanations\x18\x01 \x03(\v2(.qclaogui.aiplatform.v1beta1.ExplanationR\fexplanations\x1a\x8d\x01\n" +
	"\x1bConcurrentExplanationsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12X\n" +
	"\x05value\x18\x02 \x01(\v2B.qclaogui.aiplatform.v1beta1.ExplainResponse.ConcurrentExplanationR\x05value:\x028\x01\"\xb0\x04\n" +
	"\x12CountTokensRequest\x12G\n" +
	"\bendpoint\x18\x01 \x01(\tB+\xe2A\x01\x02\xfaA$\n" +
	"\"aiplatform.googleapis.com/EndpointR\bendpoint\x12\x1a\n" +
	"\x05model\x18\x03 \x01(\tB\x04\xe2A\x01\x01R\x05model\x12:\n" +
	"\tinstances\x18\x02 \x03(\v2\x16.google.protobuf.ValueB\x04\xe2A\x01\x01R\tinstances\x12F\n" +
	"\bcontents\x18\x04 \x03(\v2$.qclaogui.aiplatform.v1beta1.ContentB\x04\xe2A\x01\x01R\bcontents\x12^\n" +
	"\x12system_instruction\x18\x05 \x01(\v2$.qclaogui.aiplatform.v1beta1.ContentB\x04\xe2A\x01\x01H\x00R\x11systemInstruction\x88\x01\x01\x12=\n" +
	"\x05tools\x18\x06 \x03(\v2!.qclaogui.aiplatform.v1beta1.ToolB\x04\xe2A\x01\x01R\x05tools\x12e\n" +
	"\x11generation_config\x18\a \x01(\v2-.qclaogui.aiplatform.v1beta1.GenerationConfigB\x04\xe2A\x01\x01H\x01R\x10generationConfig\x88\x01\x01B\x15\n" +
	"\x13_system_instructionB\x14\n" +
	"\x12_generation_config\"\xdf\x01\n" +
	"\x13CountTokensResponse\x12!\n" +
	"\ftotal_tokens\x18\x01 \x01(\x05R\vtotalTokens\x12:\n" +
	"\x19total_billable_characters\x18\x02 \x01(\x05R\x17totalBillableCharacters\x12i\n" +
	"\x15prompt_tokens_details\x18\x03 \x03(\v2/.qclaogui.aiplatform.v1beta1.ModalityTokenCountB\x04\xe2A\x01\x03R\x13promptTokensDetails\"\xb2\x06\n" +
	"\x16GenerateContentRequest\x12\x1a\n" +
	"\x05model\x18\x05 \x01(\tB\x04\xe2A\x01\x02R\x05model\x12F\n" +
	"\bcontents\x18\x02 \x03(\v2$.qclaogui.aiplatform.v1beta1.ContentB\x04\xe2A\x01\x02R\bcontents\x12^\n" +
	"\x12system_instruction\x18\b \x01(\v2$.qclaogui.aiplatform.v1beta1.ContentB\x04\xe2A\x01\x01H\x00R\x11systemInstruction\x88\x01\x01\x12W\n" +
	"\x0ecached_content\x18\t \x01(\tB0\xe2A\x01\x01\xfaA)\n" +
	"'aiplatform.googleapis.com/CachedContentR\rcachedContent\x12=\n" +
	"\x05tools\x18\x06 \x03(\v2!.qclaogui.aiplatform.v1beta1.ToolB\x04\xe2A\x01\x01R\x05tools\x12N\n" +
	"\vtool_config\x18\a \x01(\v2'.qclaogui.aiplatform.v1beta1.ToolConfigB\x04\xe2A\x01\x01R\n" +
	"toolConfig\x12]\n" +
	"\x06labels\x18\n" +
	" \x03(\v2?.qclaogui.aiplatform.v1beta1.GenerateContentRequest.LabelsEntryB\x04\xe2A\x01\x01R\x06labels\x12Y\n" +
	"\x0fsafety_settings\x18\x03 \x03(\v2*.qclaogui.aiplatform.v1beta1.SafetySettingB\x04\xe2A\x01\x01R\x0esafetySettings\x12`\n" +
	"\x11generation_config\x18\x04 \x01(\v2-.qclaogui.aiplatform.v1beta1.GenerationConfigB\x04\xe2A\x01\x01R\x10generationConfig\x1a9\n" +
	"\vLabelsEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n" +
	"\x05value\x18\x02 \x01(\tR\x05value:\x028\x01B\x15\n" +
	"\x13_system_instruction\"\xcd\v\n" +
	"\x17GenerateContentResponse\x12L\n" +
	"\n" +
	"candidates\x18\x02 \x03(\v2&.qclaogui.aiplatform.v1beta1.CandidateB\x04\xe2A\x01\x03R\n" +
	"candidates\x12)\n" +
	"\rmodel_version\x18\v \x01(\tB\x04\xe2A\x01\x03R\fmodelVersion\x12A\n" +
	"\vcreate_time\x18\f \x01(\v2\x1a.google.protobuf.TimestampB\x04\xe2A\x01\x03R\n" +
	"createTime\x12%\n" +
	"\vresponse_id\x18\r \x01(\tB\x04\xe2A\x01\x03R\n" +
	"responseId\x12r\n" +
	"\x0fprompt_feedback\x18\x03 \x01(\v2C.qclaogui.aiplatform.v1beta1.GenerateContentResponse.PromptFeedbackB\x04\xe2A\x01\x03R\x0epromptFeedback\x12i\n" +
	"\x0eusage_metadata\x18\x04 \x01(\v2B.qclaogui.aiplatform.v1beta1.GenerateContentResponse.UsageMetadataR\rusageMetadata\x1a\x8b\x03\n" +
	"\x0ePromptFeedback\x12z\n" +
	"\fblock_reason\x18\x01 \x01(\x0e2Q.qclaogui.aiplatform.v1beta1.GenerateContentResponse.PromptFeedback.BlockedReasonB\x04\xe2A\x01\x03R\vblockReason\x12V\n" +
	"\x0esafety_ratings\x18\x02 \x03(\v2).qclaogui.aiplatform.v1beta1.SafetyRatingB\x04\xe2A\x01\x03R\rsafetyRatings\x126\n" +
	"\x14block_reason_message\x18\x03 \x01(\tB\x04\xe2A\x01\x03R\x12blockReasonMessage\"m\n" +
	"\rBlockedReason\x12\x1e\n" +
	"\x1aBLOCKED_REASON_UNSPECIFIED\x10\x00\x12\n" +
	"\n" +
	"\x06SAFETY\x10\x01\x12\t\n" +
	"\x05OTHER\x10\x02\x12\r\n" +
	"\tBLOCKLIST\x10\x03\x12\x16\n" +
	"\x12PROHIBITED_CONTENT\x10\x04\x1a\xe1\x04\n" +
	"\rUsageMetadata\x12,\n" +
	"\x12prompt_token_count\x18\x01 \x01(\x05R\x10promptTokenCount\x124\n" +
	"\x16candidates_token_count\x18\x02 \x01(\x05R\x14candidatesTokenCount\x126\n" +
	"\x14thoughts_token_count\x18\x0e \x01(\x05B\x04\xe2A\x01\x03R\x12thoughtsTokenCount\x12*\n" +
	"\x11total_token_count\x18\x03 \x01(\x05R\x0ftotalTokenCount\x12A\n" +
	"\x1acached_content_token_count\x18\x05 \x01(\x05B\x04\xe2A\x01\x03R\x17cachedContentTokenCount\x12i\n" +
	"\x15prompt_tokens_details\x18\t \x03(\v2/.qclaogui.aiplatform.v1beta1.ModalityTokenCountB\x04\xe2A\x01\x03R\x13promptTokensDetails\x12g\n" +
	"\x14cache_tokens_details\x18\n" +
	" \x03(\v2/.qclaogui.aiplatform.v1beta1.ModalityTokenCountB\x04\xe2A\x01\x03R\x12cacheTokensDetails\x12q\n" +
	"\x19candidates_tokens_details\x18\v \x03(\v2/.qclaogui.aiplatform.v1beta1.ModalityTokenCountB\x04\xe2A\x01\x03R\x17candidatesTokensDetails\"\x9a\x01\n" +
	"\x16ChatCompletionsRequest\x12G\n" +
	"\bendpoint\x18\x01 \x01(\tB+\xe2A\x01\x02\xfaA$\n" +
	"\"aiplatform.googleapis.com/EndpointR\bendpoint\x127\n" +
	"\thttp_body\x18\x02 \x01(\v2\x14.google.api.HttpBodyB\x04\xe2A\x01\x01R\bhttpBody\"\x96\x01\n" +
	"\x1aPredictLongRunningResponse\x12l\n" +
	"\x17generate_video_response\x18\x01 \x01(\v22.qclaogui.aiplatform.v1beta1.GenerateVideoResponseH\x00R\x15generateVideoResponseB\n" +
	"\n" +
	"\bresponse\"\x1c\n" +
	"\x1aPredictLongRunningMetadata\"\xdc\x01\n" +
	"\x15GenerateVideoResponse\x12+\n" +
	"\x11generated_samples\x18\x01 \x03(\tR\x10generatedSamples\x12<\n" +
	"\x18rai_media_filtered_count\x18\x02 \x01(\x05H\x00R\x15raiMediaFilteredCount\x88\x01\x01\x12;\n" +
	"\x1arai_media_filtered_reasons\x18\x03 \x03(\tR\x17raiMediaFilteredReasonsB\x1b\n" +
	"\x19_rai_media_filtered_count2\xd5\x1f\n" +
	"\x11PredictionService\x12\xa0\x02\n" +
	"\aPredict\x12+.qclaogui.aiplatform.v1beta1.PredictRequest\x1a,.qclaogui.aiplatform.v1beta1.PredictResponse\"\xb9\x01\xdaA\x1dendpoint,instances,parameters\x82\xd3\xe4\x93\x02\x92\x01:\x01*ZM:\x01*\"H/v1beta1/{endpoint=projects/*/locations/*/publishers/*/models/*}:predict\">/v1beta1/{endpoint=projects/*/locations/*/endpoints/*}:predict\x12\x89\x02\n" +
	"\n" +
	"RawPredict\x12..qclaogui.aiplatform.v1beta1.RawPredictRequest\x1a\x14.google.api.HttpBody\"\xb4\x01\xdaA\x12endpoint,http_body\x82\xd3\xe4\x93\x02\x98\x01:\x01*ZP:\x01*\"K/v1beta1/{endpoint=projects/*/locations/*/publishers/*/models/*}:rawPredict\"A/v1beta1/{endpoint=projects/*/locations/*/endpoints/*}:rawPredict\x12\xa3\x02\n" +
	"\x10StreamRawPredict\x124.qclaogui.aiplatform.v1beta1.StreamRawPredictRequest\x1a\x14.google.api.HttpBody\"\xc0\x01\xdaA\x12endpoint,http_body\x82\xd3\xe4\x93\x02\xa4\x01:\x01*ZV:\x01*\"Q/v1beta1/{endpoint=projects/*/locations/*/publishers/*/models/*}:streamRawPredict\"G/v1beta1/{endpoint=projects/*/locations/*/endpoints/*}:streamRawPredict0\x01\x12\xc7\x01\n" +
	"\rDirectPredict\x121.qclaogui.aiplatform.v1beta1.DirectPredictRequest\x1a2.qclaogui.aiplatform.v1beta1.DirectPredictResponse\"O\x82\xd3\xe4\x93\x02I:\x01*\"D/v1beta1/{endpoint=projects/*/locations/*/endpoints/*}:directPredict\x12\xd3\x01\n" +
	"\x10DirectRawPredict\x124.qclaogui.aiplatform.v1beta1.DirectRawPredictRequest\x1a5.qclaogui.aiplatform.v1beta1.DirectRawPredictResponse\"R\x82\xd3\xe4\x93\x02L:\x01*\"G/v1beta1/{endpoint=projects/*/locations/*/endpoints/*}:directRawPredict\x12\x8e\x01\n" +
	"\x13StreamDirectPredict\x127.qclaogui.aiplatform.v1beta1.StreamDirectPredictRequest\x1a8.qclaogui.aiplatform.v1beta1.StreamDirectPredictResponse\"\x00(\x010\x01\x12\x97\x01\n" +
	"\x16StreamDirectRawPredict\x12:.qclaogui.aiplatform.v1beta1.StreamDirectRawPredictRequest\x1a;.qclaogui.aiplatform.v1beta1.StreamDirectRawPredictResponse\"\x00(\x010\x01\x12\x85\x01\n" +
	"\x10StreamingPredict\x124.qclaogui.aiplatform.v1beta1.StreamingPredictRequest\x1a5.qclaogui.aiplatform.v1beta1.StreamingPredictResponse\"\x00(\x010\x01\x12\xc1\x02\n" +
	"\x16ServerStreamingPredict\x124.qclaogui.aiplatform.v1beta1.StreamingPredictRequest\x1a5.qclaogui.aiplatform.v1beta1.StreamingPredictResponse\"\xb7\x01\x82\xd3\xe4\x93\x02\xb0\x01:\x01*Z\\:\x01*\"W/v1beta1/{endpoint=projects/*/locations/*/publishers/*/models/*}:serverStreamingPredict\"M/v1beta1/{endpoint=projects/*/locations/*/endpoints/*}:serverStreamingPredict0\x01\x12\x8e\x01\n" +
	"\x13StreamingRawPredict\x127.qclaogui.aiplatform.v1beta1.StreamingRawPredictRequest\x1a8.qclaogui.aiplatform.v1beta1.StreamingRawPredictResponse\"\x00(\x010\x01\x12\xe1\x01\n" +
	"\aExplain\x12+.qclaogui.aiplatform.v1beta1.ExplainRequest\x1a,.qclaogui.aiplatform.v1beta1.ExplainResponse\"{\xdaA/endpoint,instances,parameters,deployed_model_id\x82\xd3\xe4\x93\x02C:\x01*\">/v1beta1/{endpoint=projects/*/locations/*/endpoints/*}:explain\x12\x97\x03\n" +
	"\vCountTokens\x12/.qclaogui.aiplatform.v1beta1.CountTokensRequest\x1a0.qclaogui.aiplatform.v1beta1.CountTokensResponse\"\xa4\x02\xdaA\x12endpoint,instances\x82\xd3\xe4\x93\x02\x88\x02:\x01*ZQ:\x01*\"L/v1beta1/{endpoint=projects/*/locations/*/publishers/*/models/*}:countTokensZ0:\x01*\"+/v1beta1/{endpoint=endpoints/*}:countTokensZ::\x01*\"5/v1beta1/{endpoint=publishers/*/models/*}:countTokens\"B/v1beta1/{endpoint=projects/*/locations/*/endpoints/*}:countTokens\x12\xa3\x03\n" +
	"\x0fGenerateContent\x123.qclaogui.aiplatform.v1beta1.GenerateContentRequest\x1a4.qclaogui.aiplatform.v1beta1.GenerateContentResponse\"\xa4\x02\xdaA\x0emodel,contents\x82\xd3\xe4\x93\x02\x8c\x02:\x01*ZR:\x01*\"M/v1beta1/{model=projects/*/locations/*/publishers/*/models/*}:generateContentZ1:\x01*\",/v1beta1/{model=endpoints/*}:generateContentZ;:\x01*\"6/v1beta1/{model=publishers/*/models/*}:generateContent\"C/v1beta1/{model=projects/*/locations/*/endpoints/*}:generateContent\x12\xc3\x03\n" +
	"\x15StreamGenerateContent\x123.qclaogui.aiplatform.v1beta1.GenerateContentRequest\x1a4.qclaogui.aiplatform.v1beta1.GenerateContentResponse\"\xbc\x02\xdaA\x0emodel,contents\x82\xd3\xe4\x93\x02\xa4\x02:\x01*ZX:\x01*\"S/v1beta1/{model=projects/*/locations/*/publishers/*/models/*}:streamGenerateContentZ7:\x01*\"2/v1beta1/{model=endpoints/*}:streamGenerateContentZA:\x01*\"</v1beta1/{model=publishers/*/models/*}:streamGenerateContent\"I/v1beta1/{model=projects/*/locations/*/endpoints/*}:streamGenerateContent0\x01\x12\xcf\x01\n" +
	"\x0fChatCompletions\x123.qclaogui.aiplatform.v1beta1.ChatCompletionsRequest\x1a\x14.google.api.HttpBody\"o\xdaA\x12endpoint,http_body\x82\xd3\xe4\x93\x02T:\thttp_body\"G/v1beta1/{endpoint=projects/*/locations/*/endpoints/*}/chat/completions0\x01\x1a\x86\x01\xcaA\x19aiplatform.googleapis.com\xd2Aghttps://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/cloud-platform.read-onlyBFZDgithub.com/qclaogui/gaip/genproto/aiplatform/apiv1beta1/aiplatformpbb\x06proto3"

var (
	file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescOnce sync.Once
	file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescData []byte
)

func file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescGZIP() []byte {
	file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescOnce.Do(func() {
		file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDesc), len(file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDesc)))
	})
	return file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDescData
}

var (
	file_qclaogui_aiplatform_v1beta1_prediction_service_proto_enumTypes = make([]protoimpl.EnumInfo, 1)
	file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes  = make([]protoimpl.MessageInfo, 32)
	file_qclaogui_aiplatform_v1beta1_prediction_service_proto_goTypes   = []any{
		(GenerateContentResponse_PromptFeedback_BlockedReason)(0), // 0: qclaogui.aiplatform.v1beta1.GenerateContentResponse.PromptFeedback.BlockedReason
		(*PredictRequest)(nil),                        // 1: qclaogui.aiplatform.v1beta1.PredictRequest
		(*PredictResponse)(nil),                       // 2: qclaogui.aiplatform.v1beta1.PredictResponse
		(*RawPredictRequest)(nil),                     // 3: qclaogui.aiplatform.v1beta1.RawPredictRequest
		(*StreamRawPredictRequest)(nil),               // 4: qclaogui.aiplatform.v1beta1.StreamRawPredictRequest
		(*DirectPredictRequest)(nil),                  // 5: qclaogui.aiplatform.v1beta1.DirectPredictRequest
		(*DirectPredictResponse)(nil),                 // 6: qclaogui.aiplatform.v1beta1.DirectPredictResponse
		(*DirectRawPredictRequest)(nil),               // 7: qclaogui.aiplatform.v1beta1.DirectRawPredictRequest
		(*DirectRawPredictResponse)(nil),              // 8: qclaogui.aiplatform.v1beta1.DirectRawPredictResponse
		(*StreamDirectPredictRequest)(nil),            // 9: qclaogui.aiplatform.v1beta1.StreamDirectPredictRequest
		(*StreamDirectPredictResponse)(nil),           // 10: qclaogui.aiplatform.v1beta1.StreamDirectPredictResponse
		(*StreamDirectRawPredictRequest)(nil),         // 11: qclaogui.aiplatform.v1beta1.StreamDirectRawPredictRequest
		(*StreamDirectRawPredictResponse)(nil),        // 12: qclaogui.aiplatform.v1beta1.StreamDirectRawPredictResponse
		(*StreamingPredictRequest)(nil),               // 13: qclaogui.aiplatform.v1beta1.StreamingPredictRequest
		(*StreamingPredictResponse)(nil),              // 14: qclaogui.aiplatform.v1beta1.StreamingPredictResponse
		(*StreamingRawPredictRequest)(nil),            // 15: qclaogui.aiplatform.v1beta1.StreamingRawPredictRequest
		(*StreamingRawPredictResponse)(nil),           // 16: qclaogui.aiplatform.v1beta1.StreamingRawPredictResponse
		(*ExplainRequest)(nil),                        // 17: qclaogui.aiplatform.v1beta1.ExplainRequest
		(*ExplainResponse)(nil),                       // 18: qclaogui.aiplatform.v1beta1.ExplainResponse
		(*CountTokensRequest)(nil),                    // 19: qclaogui.aiplatform.v1beta1.CountTokensRequest
		(*CountTokensResponse)(nil),                   // 20: qclaogui.aiplatform.v1beta1.CountTokensResponse
		(*GenerateContentRequest)(nil),                // 21: qclaogui.aiplatform.v1beta1.GenerateContentRequest
		(*GenerateContentResponse)(nil),               // 22: qclaogui.aiplatform.v1beta1.GenerateContentResponse
		(*ChatCompletionsRequest)(nil),                // 23: qclaogui.aiplatform.v1beta1.ChatCompletionsRequest
		(*PredictLongRunningResponse)(nil),            // 24: qclaogui.aiplatform.v1beta1.PredictLongRunningResponse
		(*PredictLongRunningMetadata)(nil),            // 25: qclaogui.aiplatform.v1beta1.PredictLongRunningMetadata
		(*GenerateVideoResponse)(nil),                 // 26: qclaogui.aiplatform.v1beta1.GenerateVideoResponse
		nil,                                           // 27: qclaogui.aiplatform.v1beta1.ExplainRequest.ConcurrentExplanationSpecOverrideEntry
		(*ExplainResponse_ConcurrentExplanation)(nil), // 28: qclaogui.aiplatform.v1beta1.ExplainResponse.ConcurrentExplanation
		nil, // 29: qclaogui.aiplatform.v1beta1.ExplainResponse.ConcurrentExplanationsEntry
		nil, // 30: qclaogui.aiplatform.v1beta1.GenerateContentRequest.LabelsEntry
		(*GenerateContentResponse_PromptFeedback)(nil), // 31: qclaogui.aiplatform.v1beta1.GenerateContentResponse.PromptFeedback
		(*GenerateContentResponse_UsageMetadata)(nil),  // 32: qclaogui.aiplatform.v1beta1.GenerateContentResponse.UsageMetadata
		(*structpb.Value)(nil),                         // 33: google.protobuf.Value
		(*httpbody.HttpBody)(nil),                      // 34: google.api.HttpBody
		(*Tensor)(nil),                                 // 35: qclaogui.aiplatform.v1beta1.Tensor
		(*ExplanationSpecOverride)(nil),                // 36: qclaogui.aiplatform.v1beta1.ExplanationSpecOverride
		(*Explanation)(nil),                            // 37: qclaogui.aiplatform.v1beta1.Explanation
		(*Content)(nil),                                // 38: qclaogui.aiplatform.v1beta1.Content
		(*Tool)(nil),                                   // 39: qclaogui.aiplatform.v1beta1.Tool
		(*GenerationConfig)(nil),                       // 40: qclaogui.aiplatform.v1beta1.GenerationConfig
		(*ModalityTokenCount)(nil),                     // 41: qclaogui.aiplatform.v1beta1.ModalityTokenCount
		(*ToolConfig)(nil),                             // 42: qclaogui.aiplatform.v1beta1.ToolConfig
		(*SafetySetting)(nil),                          // 43: qclaogui.aiplatform.v1beta1.SafetySetting
		(*Candidate)(nil),                              // 44: qclaogui.aiplatform.v1beta1.Candidate
		(*timestamppb.Timestamp)(nil),                  // 45: google.protobuf.Timestamp
		(*SafetyRating)(nil),                           // 46: qclaogui.aiplatform.v1beta1.SafetyRating
	}
)

var file_qclaogui_aiplatform_v1beta1_prediction_service_proto_depIdxs = []int32{
	33, // 0: qclaogui.aiplatform.v1beta1.PredictRequest.instances:type_name -> google.protobuf.Value
	33, // 1: qclaogui.aiplatform.v1beta1.PredictRequest.parameters:type_name -> google.protobuf.Value
	33, // 2: qclaogui.aiplatform.v1beta1.PredictResponse.predictions:type_name -> google.protobuf.Value
	33, // 3: qclaogui.aiplatform.v1beta1.PredictResponse.metadata:type_name -> google.protobuf.Value
	34, // 4: qclaogui.aiplatform.v1beta1.RawPredictRequest.http_body:type_name -> google.api.HttpBody
	34, // 5: qclaogui.aiplatform.v1beta1.StreamRawPredictRequest.http_body:type_name -> google.api.HttpBody
	35, // 6: qclaogui.aiplatform.v1beta1.DirectPredictRequest.inputs:type_name -> qclaogui.aiplatform.v1beta1.Tensor
	35, // 7: qclaogui.aiplatform.v1beta1.DirectPredictRequest.parameters:type_name -> qclaogui.aiplatform.v1beta1.Tensor
	35, // 8: qclaogui.aiplatform.v1beta1.DirectPredictResponse.outputs:type_name -> qclaogui.aiplatform.v1beta1.Tensor
	35, // 9: qclaogui.aiplatform.v1beta1.DirectPredictResponse.parameters:type_name -> qclaogui.aiplatform.v1beta1.Tensor
	35, // 10: qclaogui.aiplatform.v1beta1.StreamDirectPredictRequest.inputs:type_name -> qclaogui.aiplatform.v1beta1.Tensor
	35, // 11: qclaogui.aiplatform.v1beta1.StreamDirectPredictRequest.parameters:type_name -> qclaogui.aiplatform.v1beta1.Tensor
	35, // 12: qclaogui.aiplatform.v1beta1.StreamDirectPredictResponse.outputs:type_name -> qclaogui.aiplatform.v1beta1.Tensor
	35, // 13: qclaogui.aiplatform.v1beta1.StreamDirectPredictResponse.parameters:type_name -> qclaogui.aiplatform.v1beta1.Tensor
	35, // 14: qclaogui.aiplatform.v1beta1.StreamingPredictRequest.inputs:type_name -> qclaogui.aiplatform.v1beta1.Tensor
	35, // 15: qclaogui.aiplatform.v1beta1.StreamingPredictRequest.parameters:type_name -> qclaogui.aiplatform.v1beta1.Tensor
	35, // 16: qclaogui.aiplatform.v1beta1.StreamingPredictResponse.outputs:type_name -> qclaogui.aiplatform.v1beta1.Tensor
	35, // 17: qclaogui.aiplatform.v1beta1.StreamingPredictResponse.parameters:type_name -> qclaogui.aiplatform.v1beta1.Tensor
	33, // 18: qclaogui.aiplatform.v1beta1.ExplainRequest.instances:type_name -> google.protobuf.Value
	33, // 19: qclaogui.aiplatform.v1beta1.ExplainRequest.parameters:type_name -> google.protobuf.Value
	36, // 20: qclaogui.aiplatform.v1beta1.ExplainRequest.explanation_spec_override:type_name -> qclaogui.aiplatform.v1beta1.ExplanationSpecOverride
	27, // 21: qclaogui.aiplatform.v1beta1.ExplainRequest.concurrent_explanation_spec_override:type_name -> qclaogui.aiplatform.v1beta1.ExplainRequest.ConcurrentExplanationSpecOverrideEntry
	37, // 22: qclaogui.aiplatform.v1beta1.ExplainResponse.explanations:type_name -> qclaogui.aiplatform.v1beta1.Explanation
	29, // 23: qclaogui.aiplatform.v1beta1.ExplainResponse.concurrent_explanations:type_name -> qclaogui.aiplatform.v1beta1.ExplainResponse.ConcurrentExplanationsEntry
	33, // 24: qclaogui.aiplatform.v1beta1.ExplainResponse.predictions:type_name -> google.protobuf.Value
	33, // 25: qclaogui.aiplatform.v1beta1.CountTokensRequest.instances:type_name -> google.protobuf.Value
	38, // 26: qclaogui.aiplatform.v1beta1.CountTokensRequest.contents:type_name -> qclaogui.aiplatform.v1beta1.Content
	38, // 27: qclaogui.aiplatform.v1beta1.CountTokensRequest.system_instruction:type_name -> qclaogui.aiplatform.v1beta1.Content
	39, // 28: qclaogui.aiplatform.v1beta1.CountTokensRequest.tools:type_name -> qclaogui.aiplatform.v1beta1.Tool
	40, // 29: qclaogui.aiplatform.v1beta1.CountTokensRequest.generation_config:type_name -> qclaogui.aiplatform.v1beta1.GenerationConfig
	41, // 30: qclaogui.aiplatform.v1beta1.CountTokensResponse.prompt_tokens_details:type_name -> qclaogui.aiplatform.v1beta1.ModalityTokenCount
	38, // 31: qclaogui.aiplatform.v1beta1.GenerateContentRequest.contents:type_name -> qclaogui.aiplatform.v1beta1.Content
	38, // 32: qclaogui.aiplatform.v1beta1.GenerateContentRequest.system_instruction:type_name -> qclaogui.aiplatform.v1beta1.Content
	39, // 33: qclaogui.aiplatform.v1beta1.GenerateContentRequest.tools:type_name -> qclaogui.aiplatform.v1beta1.Tool
	42, // 34: qclaogui.aiplatform.v1beta1.GenerateContentRequest.tool_config:type_name -> qclaogui.aiplatform.v1beta1.ToolConfig
	30, // 35: qclaogui.aiplatform.v1beta1.GenerateContentRequest.labels:type_name -> qclaogui.aiplatform.v1beta1.GenerateContentRequest.LabelsEntry
	43, // 36: qclaogui.aiplatform.v1beta1.GenerateContentRequest.safety_settings:type_name -> qclaogui.aiplatform.v1beta1.SafetySetting
	40, // 37: qclaogui.aiplatform.v1beta1.GenerateContentRequest.generation_config:type_name -> qclaogui.aiplatform.v1beta1.GenerationConfig
	44, // 38: qclaogui.aiplatform.v1beta1.GenerateContentResponse.candidates:type_name -> qclaogui.aiplatform.v1beta1.Candidate
	45, // 39: qclaogui.aiplatform.v1beta1.GenerateContentResponse.create_time:type_name -> google.protobuf.Timestamp
	31, // 40: qclaogui.aiplatform.v1beta1.GenerateContentResponse.prompt_feedback:type_name -> qclaogui.aiplatform.v1beta1.GenerateContentResponse.PromptFeedback
	32, // 41: qclaogui.aiplatform.v1beta1.GenerateContentResponse.usage_metadata:type_name -> qclaogui.aiplatform.v1beta1.GenerateContentResponse.UsageMetadata
	34, // 42: qclaogui.aiplatform.v1beta1.ChatCompletionsRequest.http_body:type_name -> google.api.HttpBody
	26, // 43: qclaogui.aiplatform.v1beta1.PredictLongRunningResponse.generate_video_response:type_name -> qclaogui.aiplatform.v1beta1.GenerateVideoResponse
	36, // 44: qclaogui.aiplatform.v1beta1.ExplainRequest.ConcurrentExplanationSpecOverrideEntry.value:type_name -> qclaogui.aiplatform.v1beta1.ExplanationSpecOverride
	37, // 45: qclaogui.aiplatform.v1beta1.ExplainResponse.ConcurrentExplanation.explanations:type_name -> qclaogui.aiplatform.v1beta1.Explanation
	28, // 46: qclaogui.aiplatform.v1beta1.ExplainResponse.ConcurrentExplanationsEntry.value:type_name -> qclaogui.aiplatform.v1beta1.ExplainResponse.ConcurrentExplanation
	0,  // 47: qclaogui.aiplatform.v1beta1.GenerateContentResponse.PromptFeedback.block_reason:type_name -> qclaogui.aiplatform.v1beta1.GenerateContentResponse.PromptFeedback.BlockedReason
	46, // 48: qclaogui.aiplatform.v1beta1.GenerateContentResponse.PromptFeedback.safety_ratings:type_name -> qclaogui.aiplatform.v1beta1.SafetyRating
	41, // 49: qclaogui.aiplatform.v1beta1.GenerateContentResponse.UsageMetadata.prompt_tokens_details:type_name -> qclaogui.aiplatform.v1beta1.ModalityTokenCount
	41, // 50: qclaogui.aiplatform.v1beta1.GenerateContentResponse.UsageMetadata.cache_tokens_details:type_name -> qclaogui.aiplatform.v1beta1.ModalityTokenCount
	41, // 51: qclaogui.aiplatform.v1beta1.GenerateContentResponse.UsageMetadata.candidates_tokens_details:type_name -> qclaogui.aiplatform.v1beta1.ModalityTokenCount
	1,  // 52: qclaogui.aiplatform.v1beta1.PredictionService.Predict:input_type -> qclaogui.aiplatform.v1beta1.PredictRequest
	3,  // 53: qclaogui.aiplatform.v1beta1.PredictionService.RawPredict:input_type -> qclaogui.aiplatform.v1beta1.RawPredictRequest
	4,  // 54: qclaogui.aiplatform.v1beta1.PredictionService.StreamRawPredict:input_type -> qclaogui.aiplatform.v1beta1.StreamRawPredictRequest
	5,  // 55: qclaogui.aiplatform.v1beta1.PredictionService.DirectPredict:input_type -> qclaogui.aiplatform.v1beta1.DirectPredictRequest
	7,  // 56: qclaogui.aiplatform.v1beta1.PredictionService.DirectRawPredict:input_type -> qclaogui.aiplatform.v1beta1.DirectRawPredictRequest
	9,  // 57: qclaogui.aiplatform.v1beta1.PredictionService.StreamDirectPredict:input_type -> qclaogui.aiplatform.v1beta1.StreamDirectPredictRequest
	11, // 58: qclaogui.aiplatform.v1beta1.PredictionService.StreamDirectRawPredict:input_type -> qclaogui.aiplatform.v1beta1.StreamDirectRawPredictRequest
	13, // 59: qclaogui.aiplatform.v1beta1.PredictionService.StreamingPredict:input_type -> qclaogui.aiplatform.v1beta1.StreamingPredictRequest
	13, // 60: qclaogui.aiplatform.v1beta1.PredictionService.ServerStreamingPredict:input_type -> qclaogui.aiplatform.v1beta1.StreamingPredictRequest
	15, // 61: qclaogui.aiplatform.v1beta1.PredictionService.StreamingRawPredict:input_type -> qclaogui.aiplatform.v1beta1.StreamingRawPredictRequest
	17, // 62: qclaogui.aiplatform.v1beta1.PredictionService.Explain:input_type -> qclaogui.aiplatform.v1beta1.ExplainRequest
	19, // 63: qclaogui.aiplatform.v1beta1.PredictionService.CountTokens:input_type -> qclaogui.aiplatform.v1beta1.CountTokensRequest
	21, // 64: qclaogui.aiplatform.v1beta1.PredictionService.GenerateContent:input_type -> qclaogui.aiplatform.v1beta1.GenerateContentRequest
	21, // 65: qclaogui.aiplatform.v1beta1.PredictionService.StreamGenerateContent:input_type -> qclaogui.aiplatform.v1beta1.GenerateContentRequest
	23, // 66: qclaogui.aiplatform.v1beta1.PredictionService.ChatCompletions:input_type -> qclaogui.aiplatform.v1beta1.ChatCompletionsRequest
	2,  // 67: qclaogui.aiplatform.v1beta1.PredictionService.Predict:output_type -> qclaogui.aiplatform.v1beta1.PredictResponse
	34, // 68: qclaogui.aiplatform.v1beta1.PredictionService.RawPredict:output_type -> google.api.HttpBody
	34, // 69: qclaogui.aiplatform.v1beta1.PredictionService.StreamRawPredict:output_type -> google.api.HttpBody
	6,  // 70: qclaogui.aiplatform.v1beta1.PredictionService.DirectPredict:output_type -> qclaogui.aiplatform.v1beta1.DirectPredictResponse
	8,  // 71: qclaogui.aiplatform.v1beta1.PredictionService.DirectRawPredict:output_type -> qclaogui.aiplatform.v1beta1.DirectRawPredictResponse
	10, // 72: qclaogui.aiplatform.v1beta1.PredictionService.StreamDirectPredict:output_type -> qclaogui.aiplatform.v1beta1.StreamDirectPredictResponse
	12, // 73: qclaogui.aiplatform.v1beta1.PredictionService.StreamDirectRawPredict:output_type -> qclaogui.aiplatform.v1beta1.StreamDirectRawPredictResponse
	14, // 74: qclaogui.aiplatform.v1beta1.PredictionService.StreamingPredict:output_type -> qclaogui.aiplatform.v1beta1.StreamingPredictResponse
	14, // 75: qclaogui.aiplatform.v1beta1.PredictionService.ServerStreamingPredict:output_type -> qclaogui.aiplatform.v1beta1.StreamingPredictResponse
	16, // 76: qclaogui.aiplatform.v1beta1.PredictionService.StreamingRawPredict:output_type -> qclaogui.aiplatform.v1beta1.StreamingRawPredictResponse
	18, // 77: qclaogui.aiplatform.v1beta1.PredictionService.Explain:output_type -> qclaogui.aiplatform.v1beta1.ExplainResponse
	20, // 78: qclaogui.aiplatform.v1beta1.PredictionService.CountTokens:output_type -> qclaogui.aiplatform.v1beta1.CountTokensResponse
	22, // 79: qclaogui.aiplatform.v1beta1.PredictionService.GenerateContent:output_type -> qclaogui.aiplatform.v1beta1.GenerateContentResponse
	22, // 80: qclaogui.aiplatform.v1beta1.PredictionService.StreamGenerateContent:output_type -> qclaogui.aiplatform.v1beta1.GenerateContentResponse
	34, // 81: qclaogui.aiplatform.v1beta1.PredictionService.ChatCompletions:output_type -> google.api.HttpBody
	67, // [67:82] is the sub-list for method output_type
	52, // [52:67] is the sub-list for method input_type
	52, // [52:52] is the sub-list for extension type_name
	52, // [52:52] is the sub-list for extension extendee
	0,  // [0:52] is the sub-list for field type_name
}

func init() { file_qclaogui_aiplatform_v1beta1_prediction_service_proto_init() }
func file_qclaogui_aiplatform_v1beta1_prediction_service_proto_init() {
	if File_qclaogui_aiplatform_v1beta1_prediction_service_proto != nil {
		return
	}
	file_qclaogui_aiplatform_v1beta1_content_proto_init()
	file_qclaogui_aiplatform_v1beta1_explanation_proto_init()
	file_qclaogui_aiplatform_v1beta1_tool_proto_init()
	file_qclaogui_aiplatform_v1beta1_types_proto_init()
	file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[18].OneofWrappers = []any{}
	file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[20].OneofWrappers = []any{}
	file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[23].OneofWrappers = []any{
		(*PredictLongRunningResponse_GenerateVideoResponse)(nil),
	}
	file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes[25].OneofWrappers = []any{}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDesc), len(file_qclaogui_aiplatform_v1beta1_prediction_service_proto_rawDesc)),
			NumEnums:      1,
			NumMessages:   32,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_qclaogui_aiplatform_v1beta1_prediction_service_proto_goTypes,
		DependencyIndexes: file_qclaogui_aiplatform_v1beta1_prediction_service_proto_depIdxs,
		EnumInfos:         file_qclaogui_aiplatform_v1beta1_prediction_service_proto_enumTypes,
		MessageInfos:      file_qclaogui_aiplatform_v1beta1_prediction_service_proto_msgTypes,
	}.Build()
	File_qclaogui_aiplatform_v1beta1_prediction_service_proto = out.File
	file_qclaogui_aiplatform_v1beta1_prediction_service_proto_goTypes = nil
	file_qclaogui_aiplatform_v1beta1_prediction_service_proto_depIdxs = nil
}
