// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.6
// 	protoc        v6.31.1
// source: qclaogui/generativelanguage/v1beta/discuss_service.proto

package generativelanguagepb

import (
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"

	_ "google.golang.org/genproto/googleapis/api/annotations"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Request to generate a message response from the model.
type GenerateMessageRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the model to use.
	//
	// Format: `name=models/{model}`.
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Required. The structured textual input given to the model as a prompt.
	//
	// Given a
	// prompt, the model will return what it predicts is the next message in the
	// discussion.
	Prompt *MessagePrompt `protobuf:"bytes,2,opt,name=prompt,proto3" json:"prompt,omitempty"`
	// Optional. Controls the randomness of the output.
	//
	// Values can range over `[0.0,1.0]`,
	// inclusive. A value closer to `1.0` will produce responses that are more
	// varied, while a value closer to `0.0` will typically result in
	// less surprising responses from the model.
	Temperature *float32 `protobuf:"fixed32,3,opt,name=temperature,proto3,oneof" json:"temperature,omitempty"`
	// Optional. The number of generated response messages to return.
	//
	// This value must be between
	// `[1, 8]`, inclusive. If unset, this will default to `1`.
	CandidateCount *int32 `protobuf:"varint,4,opt,name=candidate_count,json=candidateCount,proto3,oneof" json:"candidate_count,omitempty"`
	// Optional. The maximum cumulative probability of tokens to consider when
	// sampling.
	//
	// The model uses combined Top-k and nucleus sampling.
	//
	// Nucleus sampling considers the smallest set of tokens whose probability
	// sum is at least `top_p`.
	TopP *float32 `protobuf:"fixed32,5,opt,name=top_p,json=topP,proto3,oneof" json:"top_p,omitempty"`
	// Optional. The maximum number of tokens to consider when sampling.
	//
	// The model uses combined Top-k and nucleus sampling.
	//
	// Top-k sampling considers the set of `top_k` most probable tokens.
	TopK          *int32 `protobuf:"varint,6,opt,name=top_k,json=topK,proto3,oneof" json:"top_k,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerateMessageRequest) Reset() {
	*x = GenerateMessageRequest{}
	mi := &file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateMessageRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateMessageRequest) ProtoMessage() {}

func (x *GenerateMessageRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateMessageRequest.ProtoReflect.Descriptor instead.
func (*GenerateMessageRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDescGZIP(), []int{0}
}

func (x *GenerateMessageRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *GenerateMessageRequest) GetPrompt() *MessagePrompt {
	if x != nil {
		return x.Prompt
	}
	return nil
}

func (x *GenerateMessageRequest) GetTemperature() float32 {
	if x != nil && x.Temperature != nil {
		return *x.Temperature
	}
	return 0
}

func (x *GenerateMessageRequest) GetCandidateCount() int32 {
	if x != nil && x.CandidateCount != nil {
		return *x.CandidateCount
	}
	return 0
}

func (x *GenerateMessageRequest) GetTopP() float32 {
	if x != nil && x.TopP != nil {
		return *x.TopP
	}
	return 0
}

func (x *GenerateMessageRequest) GetTopK() int32 {
	if x != nil && x.TopK != nil {
		return *x.TopK
	}
	return 0
}

// The response from the model.
//
// This includes candidate messages and
// conversation history in the form of chronologically-ordered messages.
type GenerateMessageResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Candidate response messages from the model.
	Candidates []*Message `protobuf:"bytes,1,rep,name=candidates,proto3" json:"candidates,omitempty"`
	// The conversation history used by the model.
	Messages []*Message `protobuf:"bytes,2,rep,name=messages,proto3" json:"messages,omitempty"`
	// A set of content filtering metadata for the prompt and response
	// text.
	//
	// This indicates which `SafetyCategory`(s) blocked a
	// candidate from this response, the lowest `HarmProbability`
	// that triggered a block, and the HarmThreshold setting for that category.
	Filters       []*ContentFilter `protobuf:"bytes,3,rep,name=filters,proto3" json:"filters,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerateMessageResponse) Reset() {
	*x = GenerateMessageResponse{}
	mi := &file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateMessageResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateMessageResponse) ProtoMessage() {}

func (x *GenerateMessageResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateMessageResponse.ProtoReflect.Descriptor instead.
func (*GenerateMessageResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDescGZIP(), []int{1}
}

func (x *GenerateMessageResponse) GetCandidates() []*Message {
	if x != nil {
		return x.Candidates
	}
	return nil
}

func (x *GenerateMessageResponse) GetMessages() []*Message {
	if x != nil {
		return x.Messages
	}
	return nil
}

func (x *GenerateMessageResponse) GetFilters() []*ContentFilter {
	if x != nil {
		return x.Filters
	}
	return nil
}

// The base unit of structured text.
//
// A `Message` includes an `author` and the `content` of
// the `Message`.
//
// The `author` is used to tag messages when they are fed to the
// model as text.
type Message struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. The author of this Message.
	//
	// This serves as a key for tagging
	// the content of this Message when it is fed to the model as text.
	//
	// The author can be any alphanumeric string.
	Author string `protobuf:"bytes,1,opt,name=author,proto3" json:"author,omitempty"`
	// Required. The text content of the structured `Message`.
	Content string `protobuf:"bytes,2,opt,name=content,proto3" json:"content,omitempty"`
	// Output only. Citation information for model-generated `content` in this
	// `Message`.
	//
	// If this `Message` was generated as output from the model, this field may be
	// populated with attribution information for any text included in the
	// `content`. This field is used only on output.
	CitationMetadata *CitationMetadata `protobuf:"bytes,3,opt,name=citation_metadata,json=citationMetadata,proto3,oneof" json:"citation_metadata,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *Message) Reset() {
	*x = Message{}
	mi := &file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Message) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Message) ProtoMessage() {}

func (x *Message) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Message.ProtoReflect.Descriptor instead.
func (*Message) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDescGZIP(), []int{2}
}

func (x *Message) GetAuthor() string {
	if x != nil {
		return x.Author
	}
	return ""
}

func (x *Message) GetContent() string {
	if x != nil {
		return x.Content
	}
	return ""
}

func (x *Message) GetCitationMetadata() *CitationMetadata {
	if x != nil {
		return x.CitationMetadata
	}
	return nil
}

// All of the structured input text passed to the model as a prompt.
//
// A `MessagePrompt` contains a structured set of fields that provide context
// for the conversation, examples of user input/model output message pairs that
// prime the model to respond in different ways, and the conversation history
// or list of messages representing the alternating turns of the conversation
// between the user and the model.
type MessagePrompt struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. Text that should be provided to the model first to ground the
	// response.
	//
	// If not empty, this `context` will be given to the model first before the
	// `examples` and `messages`. When using a `context` be sure to provide it
	// with every request to maintain continuity.
	//
	// This field can be a description of your prompt to the model to help provide
	// context and guide the responses. Examples: "Translate the phrase from
	// English to French." or "Given a statement, classify the sentiment as happy,
	// sad or neutral."
	//
	// Anything included in this field will take precedence over message history
	// if the total input size exceeds the model's `input_token_limit` and the
	// input request is truncated.
	Context string `protobuf:"bytes,1,opt,name=context,proto3" json:"context,omitempty"`
	// Optional. Examples of what the model should generate.
	//
	// This includes both user input and the response that the model should
	// emulate.
	//
	// These `examples` are treated identically to conversation messages except
	// that they take precedence over the history in `messages`:
	// If the total input size exceeds the model's `input_token_limit` the input
	// will be truncated. Items will be dropped from `messages` before `examples`.
	Examples []*Example `protobuf:"bytes,2,rep,name=examples,proto3" json:"examples,omitempty"`
	// Required. A snapshot of the recent conversation history sorted
	// chronologically.
	//
	// Turns alternate between two authors.
	//
	// If the total input size exceeds the model's `input_token_limit` the input
	// will be truncated: The oldest items will be dropped from `messages`.
	Messages      []*Message `protobuf:"bytes,3,rep,name=messages,proto3" json:"messages,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *MessagePrompt) Reset() {
	*x = MessagePrompt{}
	mi := &file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *MessagePrompt) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*MessagePrompt) ProtoMessage() {}

func (x *MessagePrompt) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use MessagePrompt.ProtoReflect.Descriptor instead.
func (*MessagePrompt) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDescGZIP(), []int{3}
}

func (x *MessagePrompt) GetContext() string {
	if x != nil {
		return x.Context
	}
	return ""
}

func (x *MessagePrompt) GetExamples() []*Example {
	if x != nil {
		return x.Examples
	}
	return nil
}

func (x *MessagePrompt) GetMessages() []*Message {
	if x != nil {
		return x.Messages
	}
	return nil
}

// An input/output example used to instruct the Model.
//
// It demonstrates how the model should respond or format its response.
type Example struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. An example of an input `Message` from the user.
	Input *Message `protobuf:"bytes,1,opt,name=input,proto3" json:"input,omitempty"`
	// Required. An example of what the model should output given the input.
	Output        *Message `protobuf:"bytes,2,opt,name=output,proto3" json:"output,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Example) Reset() {
	*x = Example{}
	mi := &file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Example) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Example) ProtoMessage() {}

func (x *Example) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Example.ProtoReflect.Descriptor instead.
func (*Example) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDescGZIP(), []int{4}
}

func (x *Example) GetInput() *Message {
	if x != nil {
		return x.Input
	}
	return nil
}

func (x *Example) GetOutput() *Message {
	if x != nil {
		return x.Output
	}
	return nil
}

// Counts the number of tokens in the `prompt` sent to a model.
//
// Models may tokenize text differently, so each model may return a different
// `token_count`.
type CountMessageTokensRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The model's resource name. This serves as an ID for the Model to
	// use.
	//
	// This name should match a model name returned by the `ListModels` method.
	//
	// Format: `models/{model}`
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Required. The prompt, whose token count is to be returned.
	Prompt        *MessagePrompt `protobuf:"bytes,2,opt,name=prompt,proto3" json:"prompt,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CountMessageTokensRequest) Reset() {
	*x = CountMessageTokensRequest{}
	mi := &file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CountMessageTokensRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CountMessageTokensRequest) ProtoMessage() {}

func (x *CountMessageTokensRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CountMessageTokensRequest.ProtoReflect.Descriptor instead.
func (*CountMessageTokensRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDescGZIP(), []int{5}
}

func (x *CountMessageTokensRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *CountMessageTokensRequest) GetPrompt() *MessagePrompt {
	if x != nil {
		return x.Prompt
	}
	return nil
}

// A response from `CountMessageTokens`.
//
// It returns the model's `token_count` for the `prompt`.
type CountMessageTokensResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The number of tokens that the `model` tokenizes the `prompt` into.
	//
	// Always non-negative.
	TokenCount    int32 `protobuf:"varint,1,opt,name=token_count,json=tokenCount,proto3" json:"token_count,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CountMessageTokensResponse) Reset() {
	*x = CountMessageTokensResponse{}
	mi := &file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CountMessageTokensResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CountMessageTokensResponse) ProtoMessage() {}

func (x *CountMessageTokensResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CountMessageTokensResponse.ProtoReflect.Descriptor instead.
func (*CountMessageTokensResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDescGZIP(), []int{6}
}

func (x *CountMessageTokensResponse) GetTokenCount() int32 {
	if x != nil {
		return x.TokenCount
	}
	return 0
}

var File_qclaogui_generativelanguage_v1beta_discuss_service_proto protoreflect.FileDescriptor

const file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDesc = "" +
	"\n" +
	"8qclaogui/generativelanguage/v1beta/discuss_service.proto\x12\"qclaogui.generativelanguage.v1beta\x1a\x1cgoogle/api/annotations.proto\x1a\x17google/api/client.proto\x1a\x1fgoogle/api/field_behavior.proto\x1a\x19google/api/resource.proto\x1a1qclaogui/generativelanguage/v1beta/citation.proto\x1a/qclaogui/generativelanguage/v1beta/safety.proto\"\x88\x03\n" +
	"\x16GenerateMessageRequest\x12D\n" +
	"\x05model\x18\x01 \x01(\tB.\xe2A\x01\x02\xfaA'\n" +
	"%generativelanguage.qclaogui.com/ModelR\x05model\x12O\n" +
	"\x06prompt\x18\x02 \x01(\v21.qclaogui.generativelanguage.v1beta.MessagePromptB\x04\xe2A\x01\x02R\x06prompt\x12+\n" +
	"\vtemperature\x18\x03 \x01(\x02B\x04\xe2A\x01\x01H\x00R\vtemperature\x88\x01\x01\x122\n" +
	"\x0fcandidate_count\x18\x04 \x01(\x05B\x04\xe2A\x01\x01H\x01R\x0ecandidateCount\x88\x01\x01\x12\x1e\n" +
	"\x05top_p\x18\x05 \x01(\x02B\x04\xe2A\x01\x01H\x02R\x04topP\x88\x01\x01\x12\x1e\n" +
	"\x05top_k\x18\x06 \x01(\x05B\x04\xe2A\x01\x01H\x03R\x04topK\x88\x01\x01B\x0e\n" +
	"\f_temperatureB\x12\n" +
	"\x10_candidate_countB\b\n" +
	"\x06_top_pB\b\n" +
	"\x06_top_k\"\xfc\x01\n" +
	"\x17GenerateMessageResponse\x12K\n" +
	"\n" +
	"candidates\x18\x01 \x03(\v2+.qclaogui.generativelanguage.v1beta.MessageR\n" +
	"candidates\x12G\n" +
	"\bmessages\x18\x02 \x03(\v2+.qclaogui.generativelanguage.v1beta.MessageR\bmessages\x12K\n" +
	"\afilters\x18\x03 \x03(\v21.qclaogui.generativelanguage.v1beta.ContentFilterR\afilters\"\xcb\x01\n" +
	"\aMessage\x12\x1c\n" +
	"\x06author\x18\x01 \x01(\tB\x04\xe2A\x01\x01R\x06author\x12\x1e\n" +
	"\acontent\x18\x02 \x01(\tB\x04\xe2A\x01\x02R\acontent\x12l\n" +
	"\x11citation_metadata\x18\x03 \x01(\v24.qclaogui.generativelanguage.v1beta.CitationMetadataB\x04\xe2A\x01\x03H\x00R\x10citationMetadata\x88\x01\x01B\x14\n" +
	"\x12_citation_metadata\"\xcd\x01\n" +
	"\rMessagePrompt\x12\x1e\n" +
	"\acontext\x18\x01 \x01(\tB\x04\xe2A\x01\x01R\acontext\x12M\n" +
	"\bexamples\x18\x02 \x03(\v2+.qclaogui.generativelanguage.v1beta.ExampleB\x04\xe2A\x01\x01R\bexamples\x12M\n" +
	"\bmessages\x18\x03 \x03(\v2+.qclaogui.generativelanguage.v1beta.MessageB\x04\xe2A\x01\x02R\bmessages\"\x9d\x01\n" +
	"\aExample\x12G\n" +
	"\x05input\x18\x01 \x01(\v2+.qclaogui.generativelanguage.v1beta.MessageB\x04\xe2A\x01\x02R\x05input\x12I\n" +
	"\x06output\x18\x02 \x01(\v2+.qclaogui.generativelanguage.v1beta.MessageB\x04\xe2A\x01\x02R\x06output\"\xb2\x01\n" +
	"\x19CountMessageTokensRequest\x12D\n" +
	"\x05model\x18\x01 \x01(\tB.\xe2A\x01\x02\xfaA'\n" +
	"%generativelanguage.qclaogui.com/ModelR\x05model\x12O\n" +
	"\x06prompt\x18\x02 \x01(\v21.qclaogui.generativelanguage.v1beta.MessagePromptB\x04\xe2A\x01\x02R\x06prompt\"=\n" +
	"\x1aCountMessageTokensResponse\x12\x1f\n" +
	"\vtoken_count\x18\x01 \x01(\x05R\n" +
	"tokenCount2\x8a\x04\n" +
	"\x0eDiscussService\x12\xf6\x01\n" +
	"\x0fGenerateMessage\x12:.qclaogui.generativelanguage.v1beta.GenerateMessageRequest\x1a;.qclaogui.generativelanguage.v1beta.GenerateMessageResponse\"j\xdaA4model,prompt,temperature,candidate_count,top_p,top_k\x82\xd3\xe4\x93\x02-:\x01*\"(/v1beta/{model=models/*}:generateMessage\x12\xda\x01\n" +
	"\x12CountMessageTokens\x12=.qclaogui.generativelanguage.v1beta.CountMessageTokensRequest\x1a>.qclaogui.generativelanguage.v1beta.CountMessageTokensResponse\"E\xdaA\fmodel,prompt\x82\xd3\xe4\x93\x020:\x01*\"+/v1beta/{model=models/*}:countMessageTokens\x1a\"\xcaA\x1fgenerativelanguage.qclaogui.comBUZSgithub.com/qclaogui/gaip/genproto/generativelanguage/apiv1beta/generativelanguagepbb\x06proto3"

var (
	file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDescOnce sync.Once
	file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDescData []byte
)

func file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDescGZIP() []byte {
	file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDescOnce.Do(func() {
		file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDesc), len(file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDesc)))
	})
	return file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDescData
}

var (
	file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes = make([]protoimpl.MessageInfo, 7)
	file_qclaogui_generativelanguage_v1beta_discuss_service_proto_goTypes  = []any{
		(*GenerateMessageRequest)(nil),     // 0: qclaogui.generativelanguage.v1beta.GenerateMessageRequest
		(*GenerateMessageResponse)(nil),    // 1: qclaogui.generativelanguage.v1beta.GenerateMessageResponse
		(*Message)(nil),                    // 2: qclaogui.generativelanguage.v1beta.Message
		(*MessagePrompt)(nil),              // 3: qclaogui.generativelanguage.v1beta.MessagePrompt
		(*Example)(nil),                    // 4: qclaogui.generativelanguage.v1beta.Example
		(*CountMessageTokensRequest)(nil),  // 5: qclaogui.generativelanguage.v1beta.CountMessageTokensRequest
		(*CountMessageTokensResponse)(nil), // 6: qclaogui.generativelanguage.v1beta.CountMessageTokensResponse
		(*ContentFilter)(nil),              // 7: qclaogui.generativelanguage.v1beta.ContentFilter
		(*CitationMetadata)(nil),           // 8: qclaogui.generativelanguage.v1beta.CitationMetadata
	}
)

var file_qclaogui_generativelanguage_v1beta_discuss_service_proto_depIdxs = []int32{
	3,  // 0: qclaogui.generativelanguage.v1beta.GenerateMessageRequest.prompt:type_name -> qclaogui.generativelanguage.v1beta.MessagePrompt
	2,  // 1: qclaogui.generativelanguage.v1beta.GenerateMessageResponse.candidates:type_name -> qclaogui.generativelanguage.v1beta.Message
	2,  // 2: qclaogui.generativelanguage.v1beta.GenerateMessageResponse.messages:type_name -> qclaogui.generativelanguage.v1beta.Message
	7,  // 3: qclaogui.generativelanguage.v1beta.GenerateMessageResponse.filters:type_name -> qclaogui.generativelanguage.v1beta.ContentFilter
	8,  // 4: qclaogui.generativelanguage.v1beta.Message.citation_metadata:type_name -> qclaogui.generativelanguage.v1beta.CitationMetadata
	4,  // 5: qclaogui.generativelanguage.v1beta.MessagePrompt.examples:type_name -> qclaogui.generativelanguage.v1beta.Example
	2,  // 6: qclaogui.generativelanguage.v1beta.MessagePrompt.messages:type_name -> qclaogui.generativelanguage.v1beta.Message
	2,  // 7: qclaogui.generativelanguage.v1beta.Example.input:type_name -> qclaogui.generativelanguage.v1beta.Message
	2,  // 8: qclaogui.generativelanguage.v1beta.Example.output:type_name -> qclaogui.generativelanguage.v1beta.Message
	3,  // 9: qclaogui.generativelanguage.v1beta.CountMessageTokensRequest.prompt:type_name -> qclaogui.generativelanguage.v1beta.MessagePrompt
	0,  // 10: qclaogui.generativelanguage.v1beta.DiscussService.GenerateMessage:input_type -> qclaogui.generativelanguage.v1beta.GenerateMessageRequest
	5,  // 11: qclaogui.generativelanguage.v1beta.DiscussService.CountMessageTokens:input_type -> qclaogui.generativelanguage.v1beta.CountMessageTokensRequest
	1,  // 12: qclaogui.generativelanguage.v1beta.DiscussService.GenerateMessage:output_type -> qclaogui.generativelanguage.v1beta.GenerateMessageResponse
	6,  // 13: qclaogui.generativelanguage.v1beta.DiscussService.CountMessageTokens:output_type -> qclaogui.generativelanguage.v1beta.CountMessageTokensResponse
	12, // [12:14] is the sub-list for method output_type
	10, // [10:12] is the sub-list for method input_type
	10, // [10:10] is the sub-list for extension type_name
	10, // [10:10] is the sub-list for extension extendee
	0,  // [0:10] is the sub-list for field type_name
}

func init() { file_qclaogui_generativelanguage_v1beta_discuss_service_proto_init() }
func file_qclaogui_generativelanguage_v1beta_discuss_service_proto_init() {
	if File_qclaogui_generativelanguage_v1beta_discuss_service_proto != nil {
		return
	}
	file_qclaogui_generativelanguage_v1beta_citation_proto_init()
	file_qclaogui_generativelanguage_v1beta_safety_proto_init()
	file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[0].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes[2].OneofWrappers = []any{}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDesc), len(file_qclaogui_generativelanguage_v1beta_discuss_service_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   7,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_qclaogui_generativelanguage_v1beta_discuss_service_proto_goTypes,
		DependencyIndexes: file_qclaogui_generativelanguage_v1beta_discuss_service_proto_depIdxs,
		MessageInfos:      file_qclaogui_generativelanguage_v1beta_discuss_service_proto_msgTypes,
	}.Build()
	File_qclaogui_generativelanguage_v1beta_discuss_service_proto = out.File
	file_qclaogui_generativelanguage_v1beta_discuss_service_proto_goTypes = nil
	file_qclaogui_generativelanguage_v1beta_discuss_service_proto_depIdxs = nil
}
