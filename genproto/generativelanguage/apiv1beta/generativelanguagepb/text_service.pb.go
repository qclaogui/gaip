// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.6
// 	protoc        v6.31.0
// source: qclaogui/generativelanguage/v1beta/text_service.proto

package generativelanguagepb

import (
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"

	_ "google.golang.org/genproto/googleapis/api/annotations"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Request to generate a text completion response from the model.
type GenerateTextRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the `Model` or `TunedModel` to use for generating the
	// completion.
	// Examples:
	//
	//	models/text-bison-001
	//	tunedModels/sentence-translator-u3b7m
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Required. The free-form input text given to the model as a prompt.
	//
	// Given a prompt, the model will generate a TextCompletion response it
	// predicts as the completion of the input text.
	Prompt *TextPrompt `protobuf:"bytes,2,opt,name=prompt,proto3" json:"prompt,omitempty"`
	// Optional. Controls the randomness of the output.
	// Note: The default value varies by model, see the `Model.temperature`
	// attribute of the `Model` returned the `getModel` function.
	//
	// Values can range from [0.0,1.0],
	// inclusive. A value closer to 1.0 will produce responses that are more
	// varied and creative, while a value closer to 0.0 will typically result in
	// more straightforward responses from the model.
	Temperature *float32 `protobuf:"fixed32,3,opt,name=temperature,proto3,oneof" json:"temperature,omitempty"`
	// Optional. Number of generated responses to return.
	//
	// This value must be between [1, 8], inclusive. If unset, this will default
	// to 1.
	CandidateCount *int32 `protobuf:"varint,4,opt,name=candidate_count,json=candidateCount,proto3,oneof" json:"candidate_count,omitempty"`
	// Optional. The maximum number of tokens to include in a candidate.
	//
	// If unset, this will default to output_token_limit specified in the `Model`
	// specification.
	MaxOutputTokens *int32 `protobuf:"varint,5,opt,name=max_output_tokens,json=maxOutputTokens,proto3,oneof" json:"max_output_tokens,omitempty"`
	// Optional. The maximum cumulative probability of tokens to consider when
	// sampling.
	//
	// The model uses combined Top-k and nucleus sampling.
	//
	// Tokens are sorted based on their assigned probabilities so that only the
	// most likely tokens are considered. Top-k sampling directly limits the
	// maximum number of tokens to consider, while Nucleus sampling limits number
	// of tokens based on the cumulative probability.
	//
	// Note: The default value varies by model, see the `Model.top_p`
	// attribute of the `Model` returned the `getModel` function.
	TopP *float32 `protobuf:"fixed32,6,opt,name=top_p,json=topP,proto3,oneof" json:"top_p,omitempty"`
	// Optional. The maximum number of tokens to consider when sampling.
	//
	// The model uses combined Top-k and nucleus sampling.
	//
	// Top-k sampling considers the set of `top_k` most probable tokens.
	// Defaults to 40.
	//
	// Note: The default value varies by model, see the `Model.top_k`
	// attribute of the `Model` returned the `getModel` function.
	TopK *int32 `protobuf:"varint,7,opt,name=top_k,json=topK,proto3,oneof" json:"top_k,omitempty"`
	// Optional. A list of unique `SafetySetting` instances for blocking unsafe
	// content.
	//
	// that will be enforced on the `GenerateTextRequest.prompt` and
	// `GenerateTextResponse.candidates`. There should not be more than one
	// setting for each `SafetyCategory` type. The API will block any prompts and
	// responses that fail to meet the thresholds set by these settings. This list
	// overrides the default settings for each `SafetyCategory` specified in the
	// safety_settings. If there is no `SafetySetting` for a given
	// `SafetyCategory` provided in the list, the API will use the default safety
	// setting for that category. Harm categories HARM_CATEGORY_DEROGATORY,
	// HARM_CATEGORY_TOXICITY, HARM_CATEGORY_VIOLENCE, HARM_CATEGORY_SEXUAL,
	// HARM_CATEGORY_MEDICAL, HARM_CATEGORY_DANGEROUS are supported in text
	// service.
	SafetySettings []*SafetySetting `protobuf:"bytes,8,rep,name=safety_settings,json=safetySettings,proto3" json:"safety_settings,omitempty"`
	// The set of character sequences (up to 5) that will stop output generation.
	// If specified, the API will stop at the first appearance of a stop
	// sequence. The stop sequence will not be included as part of the response.
	StopSequences []string `protobuf:"bytes,9,rep,name=stop_sequences,json=stopSequences,proto3" json:"stop_sequences,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerateTextRequest) Reset() {
	*x = GenerateTextRequest{}
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateTextRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateTextRequest) ProtoMessage() {}

func (x *GenerateTextRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateTextRequest.ProtoReflect.Descriptor instead.
func (*GenerateTextRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescGZIP(), []int{0}
}

func (x *GenerateTextRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *GenerateTextRequest) GetPrompt() *TextPrompt {
	if x != nil {
		return x.Prompt
	}
	return nil
}

func (x *GenerateTextRequest) GetTemperature() float32 {
	if x != nil && x.Temperature != nil {
		return *x.Temperature
	}
	return 0
}

func (x *GenerateTextRequest) GetCandidateCount() int32 {
	if x != nil && x.CandidateCount != nil {
		return *x.CandidateCount
	}
	return 0
}

func (x *GenerateTextRequest) GetMaxOutputTokens() int32 {
	if x != nil && x.MaxOutputTokens != nil {
		return *x.MaxOutputTokens
	}
	return 0
}

func (x *GenerateTextRequest) GetTopP() float32 {
	if x != nil && x.TopP != nil {
		return *x.TopP
	}
	return 0
}

func (x *GenerateTextRequest) GetTopK() int32 {
	if x != nil && x.TopK != nil {
		return *x.TopK
	}
	return 0
}

func (x *GenerateTextRequest) GetSafetySettings() []*SafetySetting {
	if x != nil {
		return x.SafetySettings
	}
	return nil
}

func (x *GenerateTextRequest) GetStopSequences() []string {
	if x != nil {
		return x.StopSequences
	}
	return nil
}

// The response from the model, including candidate completions.
type GenerateTextResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Candidate responses from the model.
	Candidates []*TextCompletion `protobuf:"bytes,1,rep,name=candidates,proto3" json:"candidates,omitempty"`
	// A set of content filtering metadata for the prompt and response
	// text.
	//
	// This indicates which `SafetyCategory`(s) blocked a
	// candidate from this response, the lowest `HarmProbability`
	// that triggered a block, and the HarmThreshold setting for that category.
	// This indicates the smallest change to the `SafetySettings` that would be
	// necessary to unblock at least 1 response.
	//
	// The blocking is configured by the `SafetySettings` in the request (or the
	// default `SafetySettings` of the API).
	Filters []*ContentFilter `protobuf:"bytes,3,rep,name=filters,proto3" json:"filters,omitempty"`
	// Returns any safety feedback related to content filtering.
	SafetyFeedback []*SafetyFeedback `protobuf:"bytes,4,rep,name=safety_feedback,json=safetyFeedback,proto3" json:"safety_feedback,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *GenerateTextResponse) Reset() {
	*x = GenerateTextResponse{}
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateTextResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateTextResponse) ProtoMessage() {}

func (x *GenerateTextResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateTextResponse.ProtoReflect.Descriptor instead.
func (*GenerateTextResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescGZIP(), []int{1}
}

func (x *GenerateTextResponse) GetCandidates() []*TextCompletion {
	if x != nil {
		return x.Candidates
	}
	return nil
}

func (x *GenerateTextResponse) GetFilters() []*ContentFilter {
	if x != nil {
		return x.Filters
	}
	return nil
}

func (x *GenerateTextResponse) GetSafetyFeedback() []*SafetyFeedback {
	if x != nil {
		return x.SafetyFeedback
	}
	return nil
}

// Text given to the model as a prompt.
//
// The Model will use this TextPrompt to Generate a text completion.
type TextPrompt struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The prompt text.
	Text          string `protobuf:"bytes,1,opt,name=text,proto3" json:"text,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextPrompt) Reset() {
	*x = TextPrompt{}
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextPrompt) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextPrompt) ProtoMessage() {}

func (x *TextPrompt) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TextPrompt.ProtoReflect.Descriptor instead.
func (*TextPrompt) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescGZIP(), []int{2}
}

func (x *TextPrompt) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

// Output text returned from a model.
type TextCompletion struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. The generated text returned from the model.
	Output string `protobuf:"bytes,1,opt,name=output,proto3" json:"output,omitempty"`
	// Ratings for the safety of a response.
	//
	// There is at most one rating per category.
	SafetyRatings []*SafetyRating `protobuf:"bytes,2,rep,name=safety_ratings,json=safetyRatings,proto3" json:"safety_ratings,omitempty"`
	// Output only. Citation information for model-generated `output` in this
	// `TextCompletion`.
	//
	// This field may be populated with attribution information for any text
	// included in the `output`.
	CitationMetadata *CitationMetadata `protobuf:"bytes,3,opt,name=citation_metadata,json=citationMetadata,proto3,oneof" json:"citation_metadata,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *TextCompletion) Reset() {
	*x = TextCompletion{}
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextCompletion) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextCompletion) ProtoMessage() {}

func (x *TextCompletion) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TextCompletion.ProtoReflect.Descriptor instead.
func (*TextCompletion) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescGZIP(), []int{3}
}

func (x *TextCompletion) GetOutput() string {
	if x != nil {
		return x.Output
	}
	return ""
}

func (x *TextCompletion) GetSafetyRatings() []*SafetyRating {
	if x != nil {
		return x.SafetyRatings
	}
	return nil
}

func (x *TextCompletion) GetCitationMetadata() *CitationMetadata {
	if x != nil {
		return x.CitationMetadata
	}
	return nil
}

// Request to get a text embedding from the model.
type EmbedTextRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The model name to use with the format model=models/{model}.
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Optional. The free-form input text that the model will turn into an
	// embedding.
	Text          string `protobuf:"bytes,2,opt,name=text,proto3" json:"text,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *EmbedTextRequest) Reset() {
	*x = EmbedTextRequest{}
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *EmbedTextRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*EmbedTextRequest) ProtoMessage() {}

func (x *EmbedTextRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use EmbedTextRequest.ProtoReflect.Descriptor instead.
func (*EmbedTextRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescGZIP(), []int{4}
}

func (x *EmbedTextRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *EmbedTextRequest) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

// The response to a EmbedTextRequest.
type EmbedTextResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. The embedding generated from the input text.
	Embedding     *Embedding `protobuf:"bytes,1,opt,name=embedding,proto3,oneof" json:"embedding,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *EmbedTextResponse) Reset() {
	*x = EmbedTextResponse{}
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *EmbedTextResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*EmbedTextResponse) ProtoMessage() {}

func (x *EmbedTextResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use EmbedTextResponse.ProtoReflect.Descriptor instead.
func (*EmbedTextResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescGZIP(), []int{5}
}

func (x *EmbedTextResponse) GetEmbedding() *Embedding {
	if x != nil {
		return x.Embedding
	}
	return nil
}

// Batch request to get a text embedding from the model.
type BatchEmbedTextRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the `Model` to use for generating the embedding.
	// Examples:
	//
	//	models/embedding-gecko-001
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Optional. The free-form input texts that the model will turn into an
	// embedding. The current limit is 100 texts, over which an error will be
	// thrown.
	Texts []string `protobuf:"bytes,2,rep,name=texts,proto3" json:"texts,omitempty"`
	// Optional. Embed requests for the batch. Only one of `texts` or `requests`
	// can be set.
	Requests      []*EmbedTextRequest `protobuf:"bytes,3,rep,name=requests,proto3" json:"requests,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *BatchEmbedTextRequest) Reset() {
	*x = BatchEmbedTextRequest{}
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BatchEmbedTextRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BatchEmbedTextRequest) ProtoMessage() {}

func (x *BatchEmbedTextRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BatchEmbedTextRequest.ProtoReflect.Descriptor instead.
func (*BatchEmbedTextRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescGZIP(), []int{6}
}

func (x *BatchEmbedTextRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *BatchEmbedTextRequest) GetTexts() []string {
	if x != nil {
		return x.Texts
	}
	return nil
}

func (x *BatchEmbedTextRequest) GetRequests() []*EmbedTextRequest {
	if x != nil {
		return x.Requests
	}
	return nil
}

// The response to a EmbedTextRequest.
type BatchEmbedTextResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. The embeddings generated from the input text.
	Embeddings    []*Embedding `protobuf:"bytes,1,rep,name=embeddings,proto3" json:"embeddings,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *BatchEmbedTextResponse) Reset() {
	*x = BatchEmbedTextResponse{}
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BatchEmbedTextResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BatchEmbedTextResponse) ProtoMessage() {}

func (x *BatchEmbedTextResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BatchEmbedTextResponse.ProtoReflect.Descriptor instead.
func (*BatchEmbedTextResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescGZIP(), []int{7}
}

func (x *BatchEmbedTextResponse) GetEmbeddings() []*Embedding {
	if x != nil {
		return x.Embeddings
	}
	return nil
}

// A list of floats representing the embedding.
type Embedding struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The embedding values.
	Value         []float32 `protobuf:"fixed32,1,rep,packed,name=value,proto3" json:"value,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Embedding) Reset() {
	*x = Embedding{}
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Embedding) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Embedding) ProtoMessage() {}

func (x *Embedding) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Embedding.ProtoReflect.Descriptor instead.
func (*Embedding) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescGZIP(), []int{8}
}

func (x *Embedding) GetValue() []float32 {
	if x != nil {
		return x.Value
	}
	return nil
}

// Counts the number of tokens in the `prompt` sent to a model.
//
// Models may tokenize text differently, so each model may return a different
// `token_count`.
type CountTextTokensRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The model's resource name. This serves as an ID for the Model to
	// use.
	//
	// This name should match a model name returned by the `ListModels` method.
	//
	// Format: `models/{model}`
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Required. The free-form input text given to the model as a prompt.
	Prompt        *TextPrompt `protobuf:"bytes,2,opt,name=prompt,proto3" json:"prompt,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CountTextTokensRequest) Reset() {
	*x = CountTextTokensRequest{}
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CountTextTokensRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CountTextTokensRequest) ProtoMessage() {}

func (x *CountTextTokensRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CountTextTokensRequest.ProtoReflect.Descriptor instead.
func (*CountTextTokensRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescGZIP(), []int{9}
}

func (x *CountTextTokensRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *CountTextTokensRequest) GetPrompt() *TextPrompt {
	if x != nil {
		return x.Prompt
	}
	return nil
}

// A response from `CountTextTokens`.
//
// It returns the model's `token_count` for the `prompt`.
type CountTextTokensResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The number of tokens that the `model` tokenizes the `prompt` into.
	//
	// Always non-negative.
	TokenCount    int32 `protobuf:"varint,1,opt,name=token_count,json=tokenCount,proto3" json:"token_count,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CountTextTokensResponse) Reset() {
	*x = CountTextTokensResponse{}
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CountTextTokensResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CountTextTokensResponse) ProtoMessage() {}

func (x *CountTextTokensResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CountTextTokensResponse.ProtoReflect.Descriptor instead.
func (*CountTextTokensResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescGZIP(), []int{10}
}

func (x *CountTextTokensResponse) GetTokenCount() int32 {
	if x != nil {
		return x.TokenCount
	}
	return 0
}

var File_qclaogui_generativelanguage_v1beta_text_service_proto protoreflect.FileDescriptor

const file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDesc = "" +
	"\n" +
	"5qclaogui/generativelanguage/v1beta/text_service.proto\x12\"qclaogui.generativelanguage.v1beta\x1a\x1cgoogle/api/annotations.proto\x1a\x17google/api/client.proto\x1a\x1fgoogle/api/field_behavior.proto\x1a\x19google/api/resource.proto\x1a1qclaogui/generativelanguage/v1beta/citation.proto\x1a/qclaogui/generativelanguage/v1beta/safety.proto\"\xae\x04\n" +
	"\x13GenerateTextRequest\x12\x1a\n" +
	"\x05model\x18\x01 \x01(\tB\x04\xe2A\x01\x02R\x05model\x12L\n" +
	"\x06prompt\x18\x02 \x01(\v2..qclaogui.generativelanguage.v1beta.TextPromptB\x04\xe2A\x01\x02R\x06prompt\x12+\n" +
	"\vtemperature\x18\x03 \x01(\x02B\x04\xe2A\x01\x01H\x00R\vtemperature\x88\x01\x01\x122\n" +
	"\x0fcandidate_count\x18\x04 \x01(\x05B\x04\xe2A\x01\x01H\x01R\x0ecandidateCount\x88\x01\x01\x125\n" +
	"\x11max_output_tokens\x18\x05 \x01(\x05B\x04\xe2A\x01\x01H\x02R\x0fmaxOutputTokens\x88\x01\x01\x12\x1e\n" +
	"\x05top_p\x18\x06 \x01(\x02B\x04\xe2A\x01\x01H\x03R\x04topP\x88\x01\x01\x12\x1e\n" +
	"\x05top_k\x18\a \x01(\x05B\x04\xe2A\x01\x01H\x04R\x04topK\x88\x01\x01\x12`\n" +
	"\x0fsafety_settings\x18\b \x03(\v21.qclaogui.generativelanguage.v1beta.SafetySettingB\x04\xe2A\x01\x01R\x0esafetySettings\x12%\n" +
	"\x0estop_sequences\x18\t \x03(\tR\rstopSequencesB\x0e\n" +
	"\f_temperatureB\x12\n" +
	"\x10_candidate_countB\x14\n" +
	"\x12_max_output_tokensB\b\n" +
	"\x06_top_pB\b\n" +
	"\x06_top_k\"\x94\x02\n" +
	"\x14GenerateTextResponse\x12R\n" +
	"\n" +
	"candidates\x18\x01 \x03(\v22.qclaogui.generativelanguage.v1beta.TextCompletionR\n" +
	"candidates\x12K\n" +
	"\afilters\x18\x03 \x03(\v21.qclaogui.generativelanguage.v1beta.ContentFilterR\afilters\x12[\n" +
	"\x0fsafety_feedback\x18\x04 \x03(\v22.qclaogui.generativelanguage.v1beta.SafetyFeedbackR\x0esafetyFeedback\"&\n" +
	"\n" +
	"TextPrompt\x12\x18\n" +
	"\x04text\x18\x01 \x01(\tB\x04\xe2A\x01\x02R\x04text\"\x8b\x02\n" +
	"\x0eTextCompletion\x12\x1c\n" +
	"\x06output\x18\x01 \x01(\tB\x04\xe2A\x01\x03R\x06output\x12W\n" +
	"\x0esafety_ratings\x18\x02 \x03(\v20.qclaogui.generativelanguage.v1beta.SafetyRatingR\rsafetyRatings\x12l\n" +
	"\x11citation_metadata\x18\x03 \x01(\v24.qclaogui.generativelanguage.v1beta.CitationMetadataB\x04\xe2A\x01\x03H\x00R\x10citationMetadata\x88\x01\x01B\x14\n" +
	"\x12_citation_metadata\"r\n" +
	"\x10EmbedTextRequest\x12D\n" +
	"\x05model\x18\x01 \x01(\tB.\xe2A\x01\x02\xfaA'\n" +
	"%generativelanguage.qclaogui.com/ModelR\x05model\x12\x18\n" +
	"\x04text\x18\x02 \x01(\tB\x04\xe2A\x01\x01R\x04text\"y\n" +
	"\x11EmbedTextResponse\x12V\n" +
	"\tembedding\x18\x01 \x01(\v2-.qclaogui.generativelanguage.v1beta.EmbeddingB\x04\xe2A\x01\x03H\x00R\tembedding\x88\x01\x01B\f\n" +
	"\n" +
	"_embedding\"\xd1\x01\n" +
	"\x15BatchEmbedTextRequest\x12D\n" +
	"\x05model\x18\x01 \x01(\tB.\xe2A\x01\x02\xfaA'\n" +
	"%generativelanguage.qclaogui.com/ModelR\x05model\x12\x1a\n" +
	"\x05texts\x18\x02 \x03(\tB\x04\xe2A\x01\x01R\x05texts\x12V\n" +
	"\brequests\x18\x03 \x03(\v24.qclaogui.generativelanguage.v1beta.EmbedTextRequestB\x04\xe2A\x01\x01R\brequests\"m\n" +
	"\x16BatchEmbedTextResponse\x12S\n" +
	"\n" +
	"embeddings\x18\x01 \x03(\v2-.qclaogui.generativelanguage.v1beta.EmbeddingB\x04\xe2A\x01\x03R\n" +
	"embeddings\"!\n" +
	"\tEmbedding\x12\x14\n" +
	"\x05value\x18\x01 \x03(\x02R\x05value\"\xac\x01\n" +
	"\x16CountTextTokensRequest\x12D\n" +
	"\x05model\x18\x01 \x01(\tB.\xe2A\x01\x02\xfaA'\n" +
	"%generativelanguage.qclaogui.com/ModelR\x05model\x12L\n" +
	"\x06prompt\x18\x02 \x01(\v2..qclaogui.generativelanguage.v1beta.TextPromptB\x04\xe2A\x01\x02R\x06prompt\":\n" +
	"\x17CountTextTokensResponse\x12\x1f\n" +
	"\vtoken_count\x18\x01 \x01(\x05R\n" +
	"tokenCount2\xb6\a\n" +
	"\vTextService\x12\xae\x02\n" +
	"\fGenerateText\x127.qclaogui.generativelanguage.v1beta.GenerateTextRequest\x1a8.qclaogui.generativelanguage.v1beta.GenerateTextResponse\"\xaa\x01\xdaAFmodel,prompt,temperature,candidate_count,max_output_tokens,top_p,top_k\x82\xd3\xe4\x93\x02[:\x01*Z/:\x01*\"*/v1beta/{model=tunedModels/*}:generateText\"%/v1beta/{model=models/*}:generateText\x12\xb4\x01\n" +
	"\tEmbedText\x124.qclaogui.generativelanguage.v1beta.EmbedTextRequest\x1a5.qclaogui.generativelanguage.v1beta.EmbedTextResponse\":\xdaA\n" +
	"model,text\x82\xd3\xe4\x93\x02':\x01*\"\"/v1beta/{model=models/*}:embedText\x12\xc9\x01\n" +
	"\x0eBatchEmbedText\x129.qclaogui.generativelanguage.v1beta.BatchEmbedTextRequest\x1a:.qclaogui.generativelanguage.v1beta.BatchEmbedTextResponse\"@\xdaA\vmodel,texts\x82\xd3\xe4\x93\x02,:\x01*\"'/v1beta/{model=models/*}:batchEmbedText\x12\xce\x01\n" +
	"\x0fCountTextTokens\x12:.qclaogui.generativelanguage.v1beta.CountTextTokensRequest\x1a;.qclaogui.generativelanguage.v1beta.CountTextTokensResponse\"B\xdaA\fmodel,prompt\x82\xd3\xe4\x93\x02-:\x01*\"(/v1beta/{model=models/*}:countTextTokens\x1a\"\xcaA\x1fgenerativelanguage.qclaogui.comBUZSgithub.com/qclaogui/gaip/genproto/generativelanguage/apiv1beta/generativelanguagepbb\x06proto3"

var (
	file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescOnce sync.Once
	file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescData []byte
)

func file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescGZIP() []byte {
	file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescOnce.Do(func() {
		file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDesc), len(file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDesc)))
	})
	return file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDescData
}

var (
	file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes = make([]protoimpl.MessageInfo, 11)
	file_qclaogui_generativelanguage_v1beta_text_service_proto_goTypes  = []any{
		(*GenerateTextRequest)(nil),     // 0: qclaogui.generativelanguage.v1beta.GenerateTextRequest
		(*GenerateTextResponse)(nil),    // 1: qclaogui.generativelanguage.v1beta.GenerateTextResponse
		(*TextPrompt)(nil),              // 2: qclaogui.generativelanguage.v1beta.TextPrompt
		(*TextCompletion)(nil),          // 3: qclaogui.generativelanguage.v1beta.TextCompletion
		(*EmbedTextRequest)(nil),        // 4: qclaogui.generativelanguage.v1beta.EmbedTextRequest
		(*EmbedTextResponse)(nil),       // 5: qclaogui.generativelanguage.v1beta.EmbedTextResponse
		(*BatchEmbedTextRequest)(nil),   // 6: qclaogui.generativelanguage.v1beta.BatchEmbedTextRequest
		(*BatchEmbedTextResponse)(nil),  // 7: qclaogui.generativelanguage.v1beta.BatchEmbedTextResponse
		(*Embedding)(nil),               // 8: qclaogui.generativelanguage.v1beta.Embedding
		(*CountTextTokensRequest)(nil),  // 9: qclaogui.generativelanguage.v1beta.CountTextTokensRequest
		(*CountTextTokensResponse)(nil), // 10: qclaogui.generativelanguage.v1beta.CountTextTokensResponse
		(*SafetySetting)(nil),           // 11: qclaogui.generativelanguage.v1beta.SafetySetting
		(*ContentFilter)(nil),           // 12: qclaogui.generativelanguage.v1beta.ContentFilter
		(*SafetyFeedback)(nil),          // 13: qclaogui.generativelanguage.v1beta.SafetyFeedback
		(*SafetyRating)(nil),            // 14: qclaogui.generativelanguage.v1beta.SafetyRating
		(*CitationMetadata)(nil),        // 15: qclaogui.generativelanguage.v1beta.CitationMetadata
	}
)

var file_qclaogui_generativelanguage_v1beta_text_service_proto_depIdxs = []int32{
	2,  // 0: qclaogui.generativelanguage.v1beta.GenerateTextRequest.prompt:type_name -> qclaogui.generativelanguage.v1beta.TextPrompt
	11, // 1: qclaogui.generativelanguage.v1beta.GenerateTextRequest.safety_settings:type_name -> qclaogui.generativelanguage.v1beta.SafetySetting
	3,  // 2: qclaogui.generativelanguage.v1beta.GenerateTextResponse.candidates:type_name -> qclaogui.generativelanguage.v1beta.TextCompletion
	12, // 3: qclaogui.generativelanguage.v1beta.GenerateTextResponse.filters:type_name -> qclaogui.generativelanguage.v1beta.ContentFilter
	13, // 4: qclaogui.generativelanguage.v1beta.GenerateTextResponse.safety_feedback:type_name -> qclaogui.generativelanguage.v1beta.SafetyFeedback
	14, // 5: qclaogui.generativelanguage.v1beta.TextCompletion.safety_ratings:type_name -> qclaogui.generativelanguage.v1beta.SafetyRating
	15, // 6: qclaogui.generativelanguage.v1beta.TextCompletion.citation_metadata:type_name -> qclaogui.generativelanguage.v1beta.CitationMetadata
	8,  // 7: qclaogui.generativelanguage.v1beta.EmbedTextResponse.embedding:type_name -> qclaogui.generativelanguage.v1beta.Embedding
	4,  // 8: qclaogui.generativelanguage.v1beta.BatchEmbedTextRequest.requests:type_name -> qclaogui.generativelanguage.v1beta.EmbedTextRequest
	8,  // 9: qclaogui.generativelanguage.v1beta.BatchEmbedTextResponse.embeddings:type_name -> qclaogui.generativelanguage.v1beta.Embedding
	2,  // 10: qclaogui.generativelanguage.v1beta.CountTextTokensRequest.prompt:type_name -> qclaogui.generativelanguage.v1beta.TextPrompt
	0,  // 11: qclaogui.generativelanguage.v1beta.TextService.GenerateText:input_type -> qclaogui.generativelanguage.v1beta.GenerateTextRequest
	4,  // 12: qclaogui.generativelanguage.v1beta.TextService.EmbedText:input_type -> qclaogui.generativelanguage.v1beta.EmbedTextRequest
	6,  // 13: qclaogui.generativelanguage.v1beta.TextService.BatchEmbedText:input_type -> qclaogui.generativelanguage.v1beta.BatchEmbedTextRequest
	9,  // 14: qclaogui.generativelanguage.v1beta.TextService.CountTextTokens:input_type -> qclaogui.generativelanguage.v1beta.CountTextTokensRequest
	1,  // 15: qclaogui.generativelanguage.v1beta.TextService.GenerateText:output_type -> qclaogui.generativelanguage.v1beta.GenerateTextResponse
	5,  // 16: qclaogui.generativelanguage.v1beta.TextService.EmbedText:output_type -> qclaogui.generativelanguage.v1beta.EmbedTextResponse
	7,  // 17: qclaogui.generativelanguage.v1beta.TextService.BatchEmbedText:output_type -> qclaogui.generativelanguage.v1beta.BatchEmbedTextResponse
	10, // 18: qclaogui.generativelanguage.v1beta.TextService.CountTextTokens:output_type -> qclaogui.generativelanguage.v1beta.CountTextTokensResponse
	15, // [15:19] is the sub-list for method output_type
	11, // [11:15] is the sub-list for method input_type
	11, // [11:11] is the sub-list for extension type_name
	11, // [11:11] is the sub-list for extension extendee
	0,  // [0:11] is the sub-list for field type_name
}

func init() { file_qclaogui_generativelanguage_v1beta_text_service_proto_init() }
func file_qclaogui_generativelanguage_v1beta_text_service_proto_init() {
	if File_qclaogui_generativelanguage_v1beta_text_service_proto != nil {
		return
	}
	file_qclaogui_generativelanguage_v1beta_citation_proto_init()
	file_qclaogui_generativelanguage_v1beta_safety_proto_init()
	file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[0].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[3].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes[5].OneofWrappers = []any{}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDesc), len(file_qclaogui_generativelanguage_v1beta_text_service_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   11,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_qclaogui_generativelanguage_v1beta_text_service_proto_goTypes,
		DependencyIndexes: file_qclaogui_generativelanguage_v1beta_text_service_proto_depIdxs,
		MessageInfos:      file_qclaogui_generativelanguage_v1beta_text_service_proto_msgTypes,
	}.Build()
	File_qclaogui_generativelanguage_v1beta_text_service_proto = out.File
	file_qclaogui_generativelanguage_v1beta_text_service_proto_goTypes = nil
	file_qclaogui_generativelanguage_v1beta_text_service_proto_depIdxs = nil
}
