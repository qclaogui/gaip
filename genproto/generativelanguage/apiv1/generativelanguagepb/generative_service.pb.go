// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.6
// 	protoc        v6.30.1
// source: qclaogui/generativelanguage/v1/generative_service.proto

package generativelanguagepb

import (
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"

	_ "google.golang.org/genproto/googleapis/api/annotations"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Type of task for which the embedding will be used.
type TaskType int32

const (
	// Unset value, which will default to one of the other enum values.
	TaskType_TASK_TYPE_UNSPECIFIED TaskType = 0
	// Specifies the given text is a query in a search/retrieval setting.
	TaskType_RETRIEVAL_QUERY TaskType = 1
	// Specifies the given text is a document from the corpus being searched.
	TaskType_RETRIEVAL_DOCUMENT TaskType = 2
	// Specifies the given text will be used for STS.
	TaskType_SEMANTIC_SIMILARITY TaskType = 3
	// Specifies that the given text will be classified.
	TaskType_CLASSIFICATION TaskType = 4
	// Specifies that the embeddings will be used for clustering.
	TaskType_CLUSTERING TaskType = 5
	// Specifies that the given text will be used for question answering.
	TaskType_QUESTION_ANSWERING TaskType = 6
	// Specifies that the given text will be used for fact verification.
	TaskType_FACT_VERIFICATION TaskType = 7
)

// Enum value maps for TaskType.
var (
	TaskType_name = map[int32]string{
		0: "TASK_TYPE_UNSPECIFIED",
		1: "RETRIEVAL_QUERY",
		2: "RETRIEVAL_DOCUMENT",
		3: "SEMANTIC_SIMILARITY",
		4: "CLASSIFICATION",
		5: "CLUSTERING",
		6: "QUESTION_ANSWERING",
		7: "FACT_VERIFICATION",
	}
	TaskType_value = map[string]int32{
		"TASK_TYPE_UNSPECIFIED": 0,
		"RETRIEVAL_QUERY":       1,
		"RETRIEVAL_DOCUMENT":    2,
		"SEMANTIC_SIMILARITY":   3,
		"CLASSIFICATION":        4,
		"CLUSTERING":            5,
		"QUESTION_ANSWERING":    6,
		"FACT_VERIFICATION":     7,
	}
)

func (x TaskType) Enum() *TaskType {
	p := new(TaskType)
	*p = x
	return p
}

func (x TaskType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (TaskType) Descriptor() protoreflect.EnumDescriptor {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_enumTypes[0].Descriptor()
}

func (TaskType) Type() protoreflect.EnumType {
	return &file_qclaogui_generativelanguage_v1_generative_service_proto_enumTypes[0]
}

func (x TaskType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use TaskType.Descriptor instead.
func (TaskType) EnumDescriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{0}
}

// Specifies the reason why the prompt was blocked.
type GenerateContentResponse_PromptFeedback_BlockReason int32

const (
	// Default value. This value is unused.
	GenerateContentResponse_PromptFeedback_BLOCK_REASON_UNSPECIFIED GenerateContentResponse_PromptFeedback_BlockReason = 0
	// Prompt was blocked due to safety reasons. Inspect `safety_ratings`
	// to understand which safety category blocked it.
	GenerateContentResponse_PromptFeedback_SAFETY GenerateContentResponse_PromptFeedback_BlockReason = 1
	// Prompt was blocked due to unknown reasons.
	GenerateContentResponse_PromptFeedback_OTHER GenerateContentResponse_PromptFeedback_BlockReason = 2
	// Prompt was blocked due to the terms which are included from the
	// terminology blocklist.
	GenerateContentResponse_PromptFeedback_BLOCKLIST GenerateContentResponse_PromptFeedback_BlockReason = 3
	// Prompt was blocked due to prohibited content.
	GenerateContentResponse_PromptFeedback_PROHIBITED_CONTENT GenerateContentResponse_PromptFeedback_BlockReason = 4
)

// Enum value maps for GenerateContentResponse_PromptFeedback_BlockReason.
var (
	GenerateContentResponse_PromptFeedback_BlockReason_name = map[int32]string{
		0: "BLOCK_REASON_UNSPECIFIED",
		1: "SAFETY",
		2: "OTHER",
		3: "BLOCKLIST",
		4: "PROHIBITED_CONTENT",
	}
	GenerateContentResponse_PromptFeedback_BlockReason_value = map[string]int32{
		"BLOCK_REASON_UNSPECIFIED": 0,
		"SAFETY":                   1,
		"OTHER":                    2,
		"BLOCKLIST":                3,
		"PROHIBITED_CONTENT":       4,
	}
)

func (x GenerateContentResponse_PromptFeedback_BlockReason) Enum() *GenerateContentResponse_PromptFeedback_BlockReason {
	p := new(GenerateContentResponse_PromptFeedback_BlockReason)
	*p = x
	return p
}

func (x GenerateContentResponse_PromptFeedback_BlockReason) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (GenerateContentResponse_PromptFeedback_BlockReason) Descriptor() protoreflect.EnumDescriptor {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_enumTypes[1].Descriptor()
}

func (GenerateContentResponse_PromptFeedback_BlockReason) Type() protoreflect.EnumType {
	return &file_qclaogui_generativelanguage_v1_generative_service_proto_enumTypes[1]
}

func (x GenerateContentResponse_PromptFeedback_BlockReason) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use GenerateContentResponse_PromptFeedback_BlockReason.Descriptor instead.
func (GenerateContentResponse_PromptFeedback_BlockReason) EnumDescriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{2, 0, 0}
}

// Defines the reason why the model stopped generating tokens.
type Candidate_FinishReason int32

const (
	// Default value. This value is unused.
	Candidate_FINISH_REASON_UNSPECIFIED Candidate_FinishReason = 0
	// Natural stop point of the model or provided stop sequence.
	Candidate_STOP Candidate_FinishReason = 1
	// The maximum number of tokens as specified in the request was reached.
	Candidate_MAX_TOKENS Candidate_FinishReason = 2
	// The response candidate content was flagged for safety reasons.
	Candidate_SAFETY Candidate_FinishReason = 3
	// The response candidate content was flagged for recitation reasons.
	Candidate_RECITATION Candidate_FinishReason = 4
	// The response candidate content was flagged for using an unsupported
	// language.
	Candidate_LANGUAGE Candidate_FinishReason = 6
	// Unknown reason.
	Candidate_OTHER Candidate_FinishReason = 5
	// Token generation stopped because the content contains forbidden terms.
	Candidate_BLOCKLIST Candidate_FinishReason = 7
	// Token generation stopped for potentially containing prohibited content.
	Candidate_PROHIBITED_CONTENT Candidate_FinishReason = 8
	// Token generation stopped because the content potentially contains
	// Sensitive Personally Identifiable Information (SPII).
	Candidate_SPII Candidate_FinishReason = 9
	// The function call generated by the model is invalid.
	Candidate_MALFORMED_FUNCTION_CALL Candidate_FinishReason = 10
)

// Enum value maps for Candidate_FinishReason.
var (
	Candidate_FinishReason_name = map[int32]string{
		0:  "FINISH_REASON_UNSPECIFIED",
		1:  "STOP",
		2:  "MAX_TOKENS",
		3:  "SAFETY",
		4:  "RECITATION",
		6:  "LANGUAGE",
		5:  "OTHER",
		7:  "BLOCKLIST",
		8:  "PROHIBITED_CONTENT",
		9:  "SPII",
		10: "MALFORMED_FUNCTION_CALL",
	}
	Candidate_FinishReason_value = map[string]int32{
		"FINISH_REASON_UNSPECIFIED": 0,
		"STOP":                      1,
		"MAX_TOKENS":                2,
		"SAFETY":                    3,
		"RECITATION":                4,
		"LANGUAGE":                  6,
		"OTHER":                     5,
		"BLOCKLIST":                 7,
		"PROHIBITED_CONTENT":        8,
		"SPII":                      9,
		"MALFORMED_FUNCTION_CALL":   10,
	}
)

func (x Candidate_FinishReason) Enum() *Candidate_FinishReason {
	p := new(Candidate_FinishReason)
	*p = x
	return p
}

func (x Candidate_FinishReason) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (Candidate_FinishReason) Descriptor() protoreflect.EnumDescriptor {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_enumTypes[2].Descriptor()
}

func (Candidate_FinishReason) Type() protoreflect.EnumType {
	return &file_qclaogui_generativelanguage_v1_generative_service_proto_enumTypes[2]
}

func (x Candidate_FinishReason) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use Candidate_FinishReason.Descriptor instead.
func (Candidate_FinishReason) EnumDescriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{3, 0}
}

// Request to generate a completion from the model.
type GenerateContentRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the `Model` to use for generating the completion.
	//
	// Format: `name=models/{model}`.
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Required. The content of the current conversation with the model.
	//
	// For single-turn queries, this is a single instance. For multi-turn queries
	// like [chat](https://ai.google.dev/gemini-api/docs/text-generation#chat),
	// this is a repeated field that contains the conversation history and the
	// latest request.
	Contents []*Content `protobuf:"bytes,2,rep,name=contents,proto3" json:"contents,omitempty"`
	// Optional. A list of unique `SafetySetting` instances for blocking unsafe
	// content.
	//
	// This will be enforced on the `GenerateContentRequest.contents` and
	// `GenerateContentResponse.candidates`. There should not be more than one
	// setting for each `SafetyCategory` type. The API will block any contents and
	// responses that fail to meet the thresholds set by these settings. This list
	// overrides the default settings for each `SafetyCategory` specified in the
	// safety_settings. If there is no `SafetySetting` for a given
	// `SafetyCategory` provided in the list, the API will use the default safety
	// setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
	// HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
	// HARM_CATEGORY_HARASSMENT are supported. Refer to the
	// [guide](https://ai.google.dev/gemini-api/docs/safety-settings)
	// for detailed information on available safety settings. Also refer to the
	// [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance) to
	// learn how to incorporate safety considerations in your AI applications.
	SafetySettings []*SafetySetting `protobuf:"bytes,3,rep,name=safety_settings,json=safetySettings,proto3" json:"safety_settings,omitempty"`
	// Optional. Configuration options for model generation and outputs.
	GenerationConfig *GenerationConfig `protobuf:"bytes,4,opt,name=generation_config,json=generationConfig,proto3,oneof" json:"generation_config,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *GenerateContentRequest) Reset() {
	*x = GenerateContentRequest{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateContentRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateContentRequest) ProtoMessage() {}

func (x *GenerateContentRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateContentRequest.ProtoReflect.Descriptor instead.
func (*GenerateContentRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{0}
}

func (x *GenerateContentRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *GenerateContentRequest) GetContents() []*Content {
	if x != nil {
		return x.Contents
	}
	return nil
}

func (x *GenerateContentRequest) GetSafetySettings() []*SafetySetting {
	if x != nil {
		return x.SafetySettings
	}
	return nil
}

func (x *GenerateContentRequest) GetGenerationConfig() *GenerationConfig {
	if x != nil {
		return x.GenerationConfig
	}
	return nil
}

// Configuration options for model generation and outputs. Not all parameters
// are configurable for every model.
type GenerationConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. Number of generated responses to return.
	//
	// Currently, this value can only be set to 1. If unset, this will default
	// to 1.
	CandidateCount *int32 `protobuf:"varint,1,opt,name=candidate_count,json=candidateCount,proto3,oneof" json:"candidate_count,omitempty"`
	// Optional. The set of character sequences (up to 5) that will stop output
	// generation. If specified, the API will stop at the first appearance of a
	// `stop_sequence`. The stop sequence will not be included as part of the
	// response.
	StopSequences []string `protobuf:"bytes,2,rep,name=stop_sequences,json=stopSequences,proto3" json:"stop_sequences,omitempty"`
	// Optional. The maximum number of tokens to include in a response candidate.
	//
	// Note: The default value varies by model, see the `Model.output_token_limit`
	// attribute of the `Model` returned from the `getModel` function.
	MaxOutputTokens *int32 `protobuf:"varint,4,opt,name=max_output_tokens,json=maxOutputTokens,proto3,oneof" json:"max_output_tokens,omitempty"`
	// Optional. Controls the randomness of the output.
	//
	// Note: The default value varies by model, see the `Model.temperature`
	// attribute of the `Model` returned from the `getModel` function.
	//
	// Values can range from [0.0, 2.0].
	Temperature *float32 `protobuf:"fixed32,5,opt,name=temperature,proto3,oneof" json:"temperature,omitempty"`
	// Optional. The maximum cumulative probability of tokens to consider when
	// sampling.
	//
	// The model uses combined Top-k and Top-p (nucleus) sampling.
	//
	// Tokens are sorted based on their assigned probabilities so that only the
	// most likely tokens are considered. Top-k sampling directly limits the
	// maximum number of tokens to consider, while Nucleus sampling limits the
	// number of tokens based on the cumulative probability.
	//
	// Note: The default value varies by `Model` and is specified by
	// the`Model.top_p` attribute returned from the `getModel` function. An empty
	// `top_k` attribute indicates that the model doesn't apply top-k sampling
	// and doesn't allow setting `top_k` on requests.
	TopP *float32 `protobuf:"fixed32,6,opt,name=top_p,json=topP,proto3,oneof" json:"top_p,omitempty"`
	// Optional. The maximum number of tokens to consider when sampling.
	//
	// Gemini models use Top-p (nucleus) sampling or a combination of Top-k and
	// nucleus sampling. Top-k sampling considers the set of `top_k` most probable
	// tokens. Models running with nucleus sampling don't allow top_k setting.
	//
	// Note: The default value varies by `Model` and is specified by
	// the`Model.top_p` attribute returned from the `getModel` function. An empty
	// `top_k` attribute indicates that the model doesn't apply top-k sampling
	// and doesn't allow setting `top_k` on requests.
	TopK *int32 `protobuf:"varint,7,opt,name=top_k,json=topK,proto3,oneof" json:"top_k,omitempty"`
	// Optional. Presence penalty applied to the next token's logprobs if the
	// token has already been seen in the response.
	//
	// This penalty is binary on/off and not dependant on the number of times the
	// token is used (after the first). Use
	// [frequency_penalty][qclaogui.generativelanguage.v1.GenerationConfig.frequency_penalty]
	// for a penalty that increases with each use.
	//
	// A positive penalty will discourage the use of tokens that have already
	// been used in the response, increasing the vocabulary.
	//
	// A negative penalty will encourage the use of tokens that have already been
	// used in the response, decreasing the vocabulary.
	PresencePenalty *float32 `protobuf:"fixed32,15,opt,name=presence_penalty,json=presencePenalty,proto3,oneof" json:"presence_penalty,omitempty"`
	// Optional. Frequency penalty applied to the next token's logprobs,
	// multiplied by the number of times each token has been seen in the respponse
	// so far.
	//
	// A positive penalty will discourage the use of tokens that have already
	// been used, proportional to the number of times the token has been used:
	// The more a token is used, the more dificult it is for the model to use
	// that token again increasing the vocabulary of responses.
	//
	// Caution: A _negative_ penalty will encourage the model to reuse tokens
	// proportional to the number of times the token has been used. Small
	// negative values will reduce the vocabulary of a response. Larger negative
	// values will cause the model to start repeating a common token  until it
	// hits the
	// [max_output_tokens][qclaogui.generativelanguage.v1.GenerationConfig.max_output_tokens]
	// limit: "...the the the the the...".
	FrequencyPenalty *float32 `protobuf:"fixed32,16,opt,name=frequency_penalty,json=frequencyPenalty,proto3,oneof" json:"frequency_penalty,omitempty"`
	// Optional. If true, export the logprobs results in response.
	ResponseLogprobs *bool `protobuf:"varint,17,opt,name=response_logprobs,json=responseLogprobs,proto3,oneof" json:"response_logprobs,omitempty"`
	// Optional. Only valid if
	// [response_logprobs=True][qclaogui.generativelanguage.v1.GenerationConfig.response_logprobs].
	// This sets the number of top logprobs to return at each decoding step in the
	// [Candidate.logprobs_result][qclaogui.generativelanguage.v1.Candidate.logprobs_result].
	Logprobs      *int32 `protobuf:"varint,18,opt,name=logprobs,proto3,oneof" json:"logprobs,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerationConfig) Reset() {
	*x = GenerationConfig{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerationConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerationConfig) ProtoMessage() {}

func (x *GenerationConfig) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerationConfig.ProtoReflect.Descriptor instead.
func (*GenerationConfig) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{1}
}

func (x *GenerationConfig) GetCandidateCount() int32 {
	if x != nil && x.CandidateCount != nil {
		return *x.CandidateCount
	}
	return 0
}

func (x *GenerationConfig) GetStopSequences() []string {
	if x != nil {
		return x.StopSequences
	}
	return nil
}

func (x *GenerationConfig) GetMaxOutputTokens() int32 {
	if x != nil && x.MaxOutputTokens != nil {
		return *x.MaxOutputTokens
	}
	return 0
}

func (x *GenerationConfig) GetTemperature() float32 {
	if x != nil && x.Temperature != nil {
		return *x.Temperature
	}
	return 0
}

func (x *GenerationConfig) GetTopP() float32 {
	if x != nil && x.TopP != nil {
		return *x.TopP
	}
	return 0
}

func (x *GenerationConfig) GetTopK() int32 {
	if x != nil && x.TopK != nil {
		return *x.TopK
	}
	return 0
}

func (x *GenerationConfig) GetPresencePenalty() float32 {
	if x != nil && x.PresencePenalty != nil {
		return *x.PresencePenalty
	}
	return 0
}

func (x *GenerationConfig) GetFrequencyPenalty() float32 {
	if x != nil && x.FrequencyPenalty != nil {
		return *x.FrequencyPenalty
	}
	return 0
}

func (x *GenerationConfig) GetResponseLogprobs() bool {
	if x != nil && x.ResponseLogprobs != nil {
		return *x.ResponseLogprobs
	}
	return false
}

func (x *GenerationConfig) GetLogprobs() int32 {
	if x != nil && x.Logprobs != nil {
		return *x.Logprobs
	}
	return 0
}

// Response from the model supporting multiple candidate responses.
//
// Safety ratings and content filtering are reported for both
// prompt in `GenerateContentResponse.prompt_feedback` and for each candidate
// in `finish_reason` and in `safety_ratings`. The API:
//   - Returns either all requested candidates or none of them
//   - Returns no candidates at all only if there was something wrong with the
//     prompt (check `prompt_feedback`)
//   - Reports feedback on each candidate in `finish_reason` and
//     `safety_ratings`.
type GenerateContentResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Candidate responses from the model.
	Candidates []*Candidate `protobuf:"bytes,1,rep,name=candidates,proto3" json:"candidates,omitempty"`
	// Returns the prompt's feedback related to the content filters.
	PromptFeedback *GenerateContentResponse_PromptFeedback `protobuf:"bytes,2,opt,name=prompt_feedback,json=promptFeedback,proto3" json:"prompt_feedback,omitempty"`
	// Output only. Metadata on the generation requests' token usage.
	UsageMetadata *GenerateContentResponse_UsageMetadata `protobuf:"bytes,3,opt,name=usage_metadata,json=usageMetadata,proto3" json:"usage_metadata,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerateContentResponse) Reset() {
	*x = GenerateContentResponse{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateContentResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateContentResponse) ProtoMessage() {}

func (x *GenerateContentResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateContentResponse.ProtoReflect.Descriptor instead.
func (*GenerateContentResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{2}
}

func (x *GenerateContentResponse) GetCandidates() []*Candidate {
	if x != nil {
		return x.Candidates
	}
	return nil
}

func (x *GenerateContentResponse) GetPromptFeedback() *GenerateContentResponse_PromptFeedback {
	if x != nil {
		return x.PromptFeedback
	}
	return nil
}

func (x *GenerateContentResponse) GetUsageMetadata() *GenerateContentResponse_UsageMetadata {
	if x != nil {
		return x.UsageMetadata
	}
	return nil
}

// A response candidate generated from the model.
type Candidate struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. Index of the candidate in the list of response candidates.
	Index *int32 `protobuf:"varint,3,opt,name=index,proto3,oneof" json:"index,omitempty"`
	// Output only. Generated content returned from the model.
	Content *Content `protobuf:"bytes,1,opt,name=content,proto3" json:"content,omitempty"`
	// Optional. Output only. The reason why the model stopped generating tokens.
	//
	// If empty, the model has not stopped generating tokens.
	FinishReason Candidate_FinishReason `protobuf:"varint,2,opt,name=finish_reason,json=finishReason,proto3,enum=qclaogui.generativelanguage.v1.Candidate_FinishReason" json:"finish_reason,omitempty"`
	// List of ratings for the safety of a response candidate.
	//
	// There is at most one rating per category.
	SafetyRatings []*SafetyRating `protobuf:"bytes,5,rep,name=safety_ratings,json=safetyRatings,proto3" json:"safety_ratings,omitempty"`
	// Output only. Citation information for model-generated candidate.
	//
	// This field may be populated with recitation information for any text
	// included in the `content`. These are passages that are "recited" from
	// copyrighted material in the foundational LLM's training data.
	CitationMetadata *CitationMetadata `protobuf:"bytes,6,opt,name=citation_metadata,json=citationMetadata,proto3" json:"citation_metadata,omitempty"`
	// Output only. Token count for this candidate.
	TokenCount int32 `protobuf:"varint,7,opt,name=token_count,json=tokenCount,proto3" json:"token_count,omitempty"`
	// Output only.
	AvgLogprobs float64 `protobuf:"fixed64,10,opt,name=avg_logprobs,json=avgLogprobs,proto3" json:"avg_logprobs,omitempty"`
	// Output only. Log-likelihood scores for the response tokens and top tokens
	LogprobsResult *LogprobsResult `protobuf:"bytes,11,opt,name=logprobs_result,json=logprobsResult,proto3" json:"logprobs_result,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *Candidate) Reset() {
	*x = Candidate{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Candidate) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Candidate) ProtoMessage() {}

func (x *Candidate) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Candidate.ProtoReflect.Descriptor instead.
func (*Candidate) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{3}
}

func (x *Candidate) GetIndex() int32 {
	if x != nil && x.Index != nil {
		return *x.Index
	}
	return 0
}

func (x *Candidate) GetContent() *Content {
	if x != nil {
		return x.Content
	}
	return nil
}

func (x *Candidate) GetFinishReason() Candidate_FinishReason {
	if x != nil {
		return x.FinishReason
	}
	return Candidate_FINISH_REASON_UNSPECIFIED
}

func (x *Candidate) GetSafetyRatings() []*SafetyRating {
	if x != nil {
		return x.SafetyRatings
	}
	return nil
}

func (x *Candidate) GetCitationMetadata() *CitationMetadata {
	if x != nil {
		return x.CitationMetadata
	}
	return nil
}

func (x *Candidate) GetTokenCount() int32 {
	if x != nil {
		return x.TokenCount
	}
	return 0
}

func (x *Candidate) GetAvgLogprobs() float64 {
	if x != nil {
		return x.AvgLogprobs
	}
	return 0
}

func (x *Candidate) GetLogprobsResult() *LogprobsResult {
	if x != nil {
		return x.LogprobsResult
	}
	return nil
}

// Logprobs Result
type LogprobsResult struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Length = total number of decoding steps.
	TopCandidates []*LogprobsResult_TopCandidates `protobuf:"bytes,1,rep,name=top_candidates,json=topCandidates,proto3" json:"top_candidates,omitempty"`
	// Length = total number of decoding steps.
	// The chosen candidates may or may not be in top_candidates.
	ChosenCandidates []*LogprobsResult_Candidate `protobuf:"bytes,2,rep,name=chosen_candidates,json=chosenCandidates,proto3" json:"chosen_candidates,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *LogprobsResult) Reset() {
	*x = LogprobsResult{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LogprobsResult) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LogprobsResult) ProtoMessage() {}

func (x *LogprobsResult) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LogprobsResult.ProtoReflect.Descriptor instead.
func (*LogprobsResult) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{4}
}

func (x *LogprobsResult) GetTopCandidates() []*LogprobsResult_TopCandidates {
	if x != nil {
		return x.TopCandidates
	}
	return nil
}

func (x *LogprobsResult) GetChosenCandidates() []*LogprobsResult_Candidate {
	if x != nil {
		return x.ChosenCandidates
	}
	return nil
}

// Request containing the `Content` for the model to embed.
type EmbedContentRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The model's resource name. This serves as an ID for the Model to
	// use.
	//
	// This name should match a model name returned by the `ListModels` method.
	//
	// Format: `models/{model}`
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Required. The content to embed. Only the `parts.text` fields will be
	// counted.
	Content *Content `protobuf:"bytes,2,opt,name=content,proto3" json:"content,omitempty"`
	// Optional. Optional task type for which the embeddings will be used. Can
	// only be set for `models/embedding-001`.
	TaskType *TaskType `protobuf:"varint,3,opt,name=task_type,json=taskType,proto3,enum=qclaogui.generativelanguage.v1.TaskType,oneof" json:"task_type,omitempty"`
	// Optional. An optional title for the text. Only applicable when TaskType is
	// `RETRIEVAL_DOCUMENT`.
	//
	// Note: Specifying a `title` for `RETRIEVAL_DOCUMENT` provides better quality
	// embeddings for retrieval.
	Title *string `protobuf:"bytes,4,opt,name=title,proto3,oneof" json:"title,omitempty"`
	// Optional. Optional reduced dimension for the output embedding. If set,
	// excessive values in the output embedding are truncated from the end.
	// Supported by newer models since 2024 only. You cannot set this value if
	// using the earlier model (`models/embedding-001`).
	OutputDimensionality *int32 `protobuf:"varint,5,opt,name=output_dimensionality,json=outputDimensionality,proto3,oneof" json:"output_dimensionality,omitempty"`
	unknownFields        protoimpl.UnknownFields
	sizeCache            protoimpl.SizeCache
}

func (x *EmbedContentRequest) Reset() {
	*x = EmbedContentRequest{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *EmbedContentRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*EmbedContentRequest) ProtoMessage() {}

func (x *EmbedContentRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use EmbedContentRequest.ProtoReflect.Descriptor instead.
func (*EmbedContentRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{5}
}

func (x *EmbedContentRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *EmbedContentRequest) GetContent() *Content {
	if x != nil {
		return x.Content
	}
	return nil
}

func (x *EmbedContentRequest) GetTaskType() TaskType {
	if x != nil && x.TaskType != nil {
		return *x.TaskType
	}
	return TaskType_TASK_TYPE_UNSPECIFIED
}

func (x *EmbedContentRequest) GetTitle() string {
	if x != nil && x.Title != nil {
		return *x.Title
	}
	return ""
}

func (x *EmbedContentRequest) GetOutputDimensionality() int32 {
	if x != nil && x.OutputDimensionality != nil {
		return *x.OutputDimensionality
	}
	return 0
}

// A list of floats representing an embedding.
type ContentEmbedding struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The embedding values.
	Values        []float32 `protobuf:"fixed32,1,rep,packed,name=values,proto3" json:"values,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ContentEmbedding) Reset() {
	*x = ContentEmbedding{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ContentEmbedding) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ContentEmbedding) ProtoMessage() {}

func (x *ContentEmbedding) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ContentEmbedding.ProtoReflect.Descriptor instead.
func (*ContentEmbedding) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{6}
}

func (x *ContentEmbedding) GetValues() []float32 {
	if x != nil {
		return x.Values
	}
	return nil
}

// The response to an `EmbedContentRequest`.
type EmbedContentResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. The embedding generated from the input content.
	Embedding     *ContentEmbedding `protobuf:"bytes,1,opt,name=embedding,proto3" json:"embedding,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *EmbedContentResponse) Reset() {
	*x = EmbedContentResponse{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *EmbedContentResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*EmbedContentResponse) ProtoMessage() {}

func (x *EmbedContentResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use EmbedContentResponse.ProtoReflect.Descriptor instead.
func (*EmbedContentResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{7}
}

func (x *EmbedContentResponse) GetEmbedding() *ContentEmbedding {
	if x != nil {
		return x.Embedding
	}
	return nil
}

// Batch request to get embeddings from the model for a list of prompts.
type BatchEmbedContentsRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The model's resource name. This serves as an ID for the Model to
	// use.
	//
	// This name should match a model name returned by the `ListModels` method.
	//
	// Format: `models/{model}`
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Required. Embed requests for the batch. The model in each of these requests
	// must match the model specified `BatchEmbedContentsRequest.model`.
	Requests      []*EmbedContentRequest `protobuf:"bytes,2,rep,name=requests,proto3" json:"requests,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *BatchEmbedContentsRequest) Reset() {
	*x = BatchEmbedContentsRequest{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BatchEmbedContentsRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BatchEmbedContentsRequest) ProtoMessage() {}

func (x *BatchEmbedContentsRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BatchEmbedContentsRequest.ProtoReflect.Descriptor instead.
func (*BatchEmbedContentsRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{8}
}

func (x *BatchEmbedContentsRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *BatchEmbedContentsRequest) GetRequests() []*EmbedContentRequest {
	if x != nil {
		return x.Requests
	}
	return nil
}

// The response to a `BatchEmbedContentsRequest`.
type BatchEmbedContentsResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. The embeddings for each request, in the same order as provided
	// in the batch request.
	Embeddings    []*ContentEmbedding `protobuf:"bytes,1,rep,name=embeddings,proto3" json:"embeddings,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *BatchEmbedContentsResponse) Reset() {
	*x = BatchEmbedContentsResponse{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BatchEmbedContentsResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BatchEmbedContentsResponse) ProtoMessage() {}

func (x *BatchEmbedContentsResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BatchEmbedContentsResponse.ProtoReflect.Descriptor instead.
func (*BatchEmbedContentsResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{9}
}

func (x *BatchEmbedContentsResponse) GetEmbeddings() []*ContentEmbedding {
	if x != nil {
		return x.Embeddings
	}
	return nil
}

// Counts the number of tokens in the `prompt` sent to a model.
//
// Models may tokenize text differently, so each model may return a different
// `token_count`.
type CountTokensRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The model's resource name. This serves as an ID for the Model to
	// use.
	//
	// This name should match a model name returned by the `ListModels` method.
	//
	// Format: `models/{model}`
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Optional. The input given to the model as a prompt. This field is ignored
	// when `generate_content_request` is set.
	Contents []*Content `protobuf:"bytes,2,rep,name=contents,proto3" json:"contents,omitempty"`
	// Optional. The overall input given to the `Model`. This includes the prompt
	// as well as other model steering information like [system
	// instructions](https://ai.google.dev/gemini-api/docs/system-instructions),
	// and/or function declarations for [function
	// calling](https://ai.google.dev/gemini-api/docs/function-calling).
	// `Model`s/`Content`s and `generate_content_request`s are mutually
	// exclusive. You can either send `Model` + `Content`s or a
	// `generate_content_request`, but never both.
	GenerateContentRequest *GenerateContentRequest `protobuf:"bytes,3,opt,name=generate_content_request,json=generateContentRequest,proto3" json:"generate_content_request,omitempty"`
	unknownFields          protoimpl.UnknownFields
	sizeCache              protoimpl.SizeCache
}

func (x *CountTokensRequest) Reset() {
	*x = CountTokensRequest{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CountTokensRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CountTokensRequest) ProtoMessage() {}

func (x *CountTokensRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CountTokensRequest.ProtoReflect.Descriptor instead.
func (*CountTokensRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{10}
}

func (x *CountTokensRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *CountTokensRequest) GetContents() []*Content {
	if x != nil {
		return x.Contents
	}
	return nil
}

func (x *CountTokensRequest) GetGenerateContentRequest() *GenerateContentRequest {
	if x != nil {
		return x.GenerateContentRequest
	}
	return nil
}

// A response from `CountTokens`.
//
// It returns the model's `token_count` for the `prompt`.
type CountTokensResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The number of tokens that the `Model` tokenizes the `prompt` into. Always
	// non-negative.
	TotalTokens   int32 `protobuf:"varint,1,opt,name=total_tokens,json=totalTokens,proto3" json:"total_tokens,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CountTokensResponse) Reset() {
	*x = CountTokensResponse{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[11]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CountTokensResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CountTokensResponse) ProtoMessage() {}

func (x *CountTokensResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[11]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CountTokensResponse.ProtoReflect.Descriptor instead.
func (*CountTokensResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{11}
}

func (x *CountTokensResponse) GetTotalTokens() int32 {
	if x != nil {
		return x.TotalTokens
	}
	return 0
}

// A set of the feedback metadata the prompt specified in
// `GenerateContentRequest.content`.
type GenerateContentResponse_PromptFeedback struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. If set, the prompt was blocked and no candidates are returned.
	// Rephrase the prompt.
	BlockReason GenerateContentResponse_PromptFeedback_BlockReason `protobuf:"varint,1,opt,name=block_reason,json=blockReason,proto3,enum=qclaogui.generativelanguage.v1.GenerateContentResponse_PromptFeedback_BlockReason" json:"block_reason,omitempty"`
	// Ratings for safety of the prompt.
	// There is at most one rating per category.
	SafetyRatings []*SafetyRating `protobuf:"bytes,2,rep,name=safety_ratings,json=safetyRatings,proto3" json:"safety_ratings,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerateContentResponse_PromptFeedback) Reset() {
	*x = GenerateContentResponse_PromptFeedback{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[12]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateContentResponse_PromptFeedback) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateContentResponse_PromptFeedback) ProtoMessage() {}

func (x *GenerateContentResponse_PromptFeedback) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[12]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateContentResponse_PromptFeedback.ProtoReflect.Descriptor instead.
func (*GenerateContentResponse_PromptFeedback) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{2, 0}
}

func (x *GenerateContentResponse_PromptFeedback) GetBlockReason() GenerateContentResponse_PromptFeedback_BlockReason {
	if x != nil {
		return x.BlockReason
	}
	return GenerateContentResponse_PromptFeedback_BLOCK_REASON_UNSPECIFIED
}

func (x *GenerateContentResponse_PromptFeedback) GetSafetyRatings() []*SafetyRating {
	if x != nil {
		return x.SafetyRatings
	}
	return nil
}

// Metadata on the generation request's token usage.
type GenerateContentResponse_UsageMetadata struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Number of tokens in the prompt. When `cached_content` is set, this is
	// still the total effective prompt size meaning this includes the number of
	// tokens in the cached content.
	PromptTokenCount int32 `protobuf:"varint,1,opt,name=prompt_token_count,json=promptTokenCount,proto3" json:"prompt_token_count,omitempty"`
	// Total number of tokens across all the generated response candidates.
	CandidatesTokenCount int32 `protobuf:"varint,2,opt,name=candidates_token_count,json=candidatesTokenCount,proto3" json:"candidates_token_count,omitempty"`
	// Total token count for the generation request (prompt + response
	// candidates).
	TotalTokenCount int32 `protobuf:"varint,3,opt,name=total_token_count,json=totalTokenCount,proto3" json:"total_token_count,omitempty"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *GenerateContentResponse_UsageMetadata) Reset() {
	*x = GenerateContentResponse_UsageMetadata{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[13]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateContentResponse_UsageMetadata) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateContentResponse_UsageMetadata) ProtoMessage() {}

func (x *GenerateContentResponse_UsageMetadata) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[13]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateContentResponse_UsageMetadata.ProtoReflect.Descriptor instead.
func (*GenerateContentResponse_UsageMetadata) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{2, 1}
}

func (x *GenerateContentResponse_UsageMetadata) GetPromptTokenCount() int32 {
	if x != nil {
		return x.PromptTokenCount
	}
	return 0
}

func (x *GenerateContentResponse_UsageMetadata) GetCandidatesTokenCount() int32 {
	if x != nil {
		return x.CandidatesTokenCount
	}
	return 0
}

func (x *GenerateContentResponse_UsageMetadata) GetTotalTokenCount() int32 {
	if x != nil {
		return x.TotalTokenCount
	}
	return 0
}

// Candidate for the logprobs token and score.
type LogprobsResult_Candidate struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The candidate’s token string value.
	Token *string `protobuf:"bytes,1,opt,name=token,proto3,oneof" json:"token,omitempty"`
	// The candidate’s token id value.
	TokenId *int32 `protobuf:"varint,3,opt,name=token_id,json=tokenId,proto3,oneof" json:"token_id,omitempty"`
	// The candidate's log probability.
	LogProbability *float32 `protobuf:"fixed32,2,opt,name=log_probability,json=logProbability,proto3,oneof" json:"log_probability,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *LogprobsResult_Candidate) Reset() {
	*x = LogprobsResult_Candidate{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[14]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LogprobsResult_Candidate) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LogprobsResult_Candidate) ProtoMessage() {}

func (x *LogprobsResult_Candidate) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[14]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LogprobsResult_Candidate.ProtoReflect.Descriptor instead.
func (*LogprobsResult_Candidate) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{4, 0}
}

func (x *LogprobsResult_Candidate) GetToken() string {
	if x != nil && x.Token != nil {
		return *x.Token
	}
	return ""
}

func (x *LogprobsResult_Candidate) GetTokenId() int32 {
	if x != nil && x.TokenId != nil {
		return *x.TokenId
	}
	return 0
}

func (x *LogprobsResult_Candidate) GetLogProbability() float32 {
	if x != nil && x.LogProbability != nil {
		return *x.LogProbability
	}
	return 0
}

// Candidates with top log probabilities at each decoding step.
type LogprobsResult_TopCandidates struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Sorted by log probability in descending order.
	Candidates    []*LogprobsResult_Candidate `protobuf:"bytes,1,rep,name=candidates,proto3" json:"candidates,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *LogprobsResult_TopCandidates) Reset() {
	*x = LogprobsResult_TopCandidates{}
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[15]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LogprobsResult_TopCandidates) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LogprobsResult_TopCandidates) ProtoMessage() {}

func (x *LogprobsResult_TopCandidates) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[15]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LogprobsResult_TopCandidates.ProtoReflect.Descriptor instead.
func (*LogprobsResult_TopCandidates) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP(), []int{4, 1}
}

func (x *LogprobsResult_TopCandidates) GetCandidates() []*LogprobsResult_Candidate {
	if x != nil {
		return x.Candidates
	}
	return nil
}

var File_qclaogui_generativelanguage_v1_generative_service_proto protoreflect.FileDescriptor

const file_qclaogui_generativelanguage_v1_generative_service_proto_rawDesc = "" +
	"\n" +
	"7qclaogui/generativelanguage/v1/generative_service.proto\x12\x1eqclaogui.generativelanguage.v1\x1a\x1cgoogle/api/annotations.proto\x1a\x17google/api/client.proto\x1a\x1fgoogle/api/field_behavior.proto\x1a\x19google/api/resource.proto\x1a-qclaogui/generativelanguage/v1/citation.proto\x1a,qclaogui/generativelanguage/v1/content.proto\x1a+qclaogui/generativelanguage/v1/safety.proto\"\x87\x03\n" +
	"\x16GenerateContentRequest\x12D\n" +
	"\x05model\x18\x01 \x01(\tB.\xe2A\x01\x02\xfaA'\n" +
	"%generativelanguage.qclaogui.com/ModelR\x05model\x12I\n" +
	"\bcontents\x18\x02 \x03(\v2'.qclaogui.generativelanguage.v1.ContentB\x04\xe2A\x01\x02R\bcontents\x12\\\n" +
	"\x0fsafety_settings\x18\x03 \x03(\v2-.qclaogui.generativelanguage.v1.SafetySettingB\x04\xe2A\x01\x01R\x0esafetySettings\x12h\n" +
	"\x11generation_config\x18\x04 \x01(\v20.qclaogui.generativelanguage.v1.GenerationConfigB\x04\xe2A\x01\x01H\x00R\x10generationConfig\x88\x01\x01B\x14\n" +
	"\x12_generation_config\"\x80\x05\n" +
	"\x10GenerationConfig\x122\n" +
	"\x0fcandidate_count\x18\x01 \x01(\x05B\x04\xe2A\x01\x01H\x00R\x0ecandidateCount\x88\x01\x01\x12+\n" +
	"\x0estop_sequences\x18\x02 \x03(\tB\x04\xe2A\x01\x01R\rstopSequences\x125\n" +
	"\x11max_output_tokens\x18\x04 \x01(\x05B\x04\xe2A\x01\x01H\x01R\x0fmaxOutputTokens\x88\x01\x01\x12+\n" +
	"\vtemperature\x18\x05 \x01(\x02B\x04\xe2A\x01\x01H\x02R\vtemperature\x88\x01\x01\x12\x1e\n" +
	"\x05top_p\x18\x06 \x01(\x02B\x04\xe2A\x01\x01H\x03R\x04topP\x88\x01\x01\x12\x1e\n" +
	"\x05top_k\x18\a \x01(\x05B\x04\xe2A\x01\x01H\x04R\x04topK\x88\x01\x01\x124\n" +
	"\x10presence_penalty\x18\x0f \x01(\x02B\x04\xe2A\x01\x01H\x05R\x0fpresencePenalty\x88\x01\x01\x126\n" +
	"\x11frequency_penalty\x18\x10 \x01(\x02B\x04\xe2A\x01\x01H\x06R\x10frequencyPenalty\x88\x01\x01\x126\n" +
	"\x11response_logprobs\x18\x11 \x01(\bB\x04\xe2A\x01\x01H\aR\x10responseLogprobs\x88\x01\x01\x12%\n" +
	"\blogprobs\x18\x12 \x01(\x05B\x04\xe2A\x01\x01H\bR\blogprobs\x88\x01\x01B\x12\n" +
	"\x10_candidate_countB\x14\n" +
	"\x12_max_output_tokensB\x0e\n" +
	"\f_temperatureB\b\n" +
	"\x06_top_pB\b\n" +
	"\x06_top_kB\x13\n" +
	"\x11_presence_penaltyB\x14\n" +
	"\x12_frequency_penaltyB\x14\n" +
	"\x12_response_logprobsB\v\n" +
	"\t_logprobs\"\xbb\x06\n" +
	"\x17GenerateContentResponse\x12I\n" +
	"\n" +
	"candidates\x18\x01 \x03(\v2).qclaogui.generativelanguage.v1.CandidateR\n" +
	"candidates\x12o\n" +
	"\x0fprompt_feedback\x18\x02 \x01(\v2F.qclaogui.generativelanguage.v1.GenerateContentResponse.PromptFeedbackR\x0epromptFeedback\x12r\n" +
	"\x0eusage_metadata\x18\x03 \x01(\v2E.qclaogui.generativelanguage.v1.GenerateContentResponse.UsageMetadataB\x04\xe2A\x01\x03R\rusageMetadata\x1a\xcd\x02\n" +
	"\x0ePromptFeedback\x12{\n" +
	"\fblock_reason\x18\x01 \x01(\x0e2R.qclaogui.generativelanguage.v1.GenerateContentResponse.PromptFeedback.BlockReasonB\x04\xe2A\x01\x01R\vblockReason\x12S\n" +
	"\x0esafety_ratings\x18\x02 \x03(\v2,.qclaogui.generativelanguage.v1.SafetyRatingR\rsafetyRatings\"i\n" +
	"\vBlockReason\x12\x1c\n" +
	"\x18BLOCK_REASON_UNSPECIFIED\x10\x00\x12\n" +
	"\n" +
	"\x06SAFETY\x10\x01\x12\t\n" +
	"\x05OTHER\x10\x02\x12\r\n" +
	"\tBLOCKLIST\x10\x03\x12\x16\n" +
	"\x12PROHIBITED_CONTENT\x10\x04\x1a\x9f\x01\n" +
	"\rUsageMetadata\x12,\n" +
	"\x12prompt_token_count\x18\x01 \x01(\x05R\x10promptTokenCount\x124\n" +
	"\x16candidates_token_count\x18\x02 \x01(\x05R\x14candidatesTokenCount\x12*\n" +
	"\x11total_token_count\x18\x03 \x01(\x05R\x0ftotalTokenCount\"\x99\x06\n" +
	"\tCandidate\x12\x1f\n" +
	"\x05index\x18\x03 \x01(\x05B\x04\xe2A\x01\x03H\x00R\x05index\x88\x01\x01\x12G\n" +
	"\acontent\x18\x01 \x01(\v2'.qclaogui.generativelanguage.v1.ContentB\x04\xe2A\x01\x03R\acontent\x12b\n" +
	"\rfinish_reason\x18\x02 \x01(\x0e26.qclaogui.generativelanguage.v1.Candidate.FinishReasonB\x05\xe2A\x02\x01\x03R\ffinishReason\x12S\n" +
	"\x0esafety_ratings\x18\x05 \x03(\v2,.qclaogui.generativelanguage.v1.SafetyRatingR\rsafetyRatings\x12c\n" +
	"\x11citation_metadata\x18\x06 \x01(\v20.qclaogui.generativelanguage.v1.CitationMetadataB\x04\xe2A\x01\x03R\x10citationMetadata\x12%\n" +
	"\vtoken_count\x18\a \x01(\x05B\x04\xe2A\x01\x03R\n" +
	"tokenCount\x12'\n" +
	"\favg_logprobs\x18\n" +
	" \x01(\x01B\x04\xe2A\x01\x03R\vavgLogprobs\x12]\n" +
	"\x0flogprobs_result\x18\v \x01(\v2..qclaogui.generativelanguage.v1.LogprobsResultB\x04\xe2A\x01\x03R\x0elogprobsResult\"\xca\x01\n" +
	"\fFinishReason\x12\x1d\n" +
	"\x19FINISH_REASON_UNSPECIFIED\x10\x00\x12\b\n" +
	"\x04STOP\x10\x01\x12\x0e\n" +
	"\n" +
	"MAX_TOKENS\x10\x02\x12\n" +
	"\n" +
	"\x06SAFETY\x10\x03\x12\x0e\n" +
	"\n" +
	"RECITATION\x10\x04\x12\f\n" +
	"\bLANGUAGE\x10\x06\x12\t\n" +
	"\x05OTHER\x10\x05\x12\r\n" +
	"\tBLOCKLIST\x10\a\x12\x16\n" +
	"\x12PROHIBITED_CONTENT\x10\b\x12\b\n" +
	"\x04SPII\x10\t\x12\x1b\n" +
	"\x17MALFORMED_FUNCTION_CALL\x10\n" +
	"B\b\n" +
	"\x06_index\"\xe9\x03\n" +
	"\x0eLogprobsResult\x12c\n" +
	"\x0etop_candidates\x18\x01 \x03(\v2<.qclaogui.generativelanguage.v1.LogprobsResult.TopCandidatesR\rtopCandidates\x12e\n" +
	"\x11chosen_candidates\x18\x02 \x03(\v28.qclaogui.generativelanguage.v1.LogprobsResult.CandidateR\x10chosenCandidates\x1a\x9f\x01\n" +
	"\tCandidate\x12\x19\n" +
	"\x05token\x18\x01 \x01(\tH\x00R\x05token\x88\x01\x01\x12\x1e\n" +
	"\btoken_id\x18\x03 \x01(\x05H\x01R\atokenId\x88\x01\x01\x12,\n" +
	"\x0flog_probability\x18\x02 \x01(\x02H\x02R\x0elogProbability\x88\x01\x01B\b\n" +
	"\x06_tokenB\v\n" +
	"\t_token_idB\x12\n" +
	"\x10_log_probability\x1ai\n" +
	"\rTopCandidates\x12X\n" +
	"\n" +
	"candidates\x18\x01 \x03(\v28.qclaogui.generativelanguage.v1.LogprobsResult.CandidateR\n" +
	"candidates\"\x89\x03\n" +
	"\x13EmbedContentRequest\x12D\n" +
	"\x05model\x18\x01 \x01(\tB.\xe2A\x01\x02\xfaA'\n" +
	"%generativelanguage.qclaogui.com/ModelR\x05model\x12G\n" +
	"\acontent\x18\x02 \x01(\v2'.qclaogui.generativelanguage.v1.ContentB\x04\xe2A\x01\x02R\acontent\x12P\n" +
	"\ttask_type\x18\x03 \x01(\x0e2(.qclaogui.generativelanguage.v1.TaskTypeB\x04\xe2A\x01\x01H\x00R\btaskType\x88\x01\x01\x12\x1f\n" +
	"\x05title\x18\x04 \x01(\tB\x04\xe2A\x01\x01H\x01R\x05title\x88\x01\x01\x12>\n" +
	"\x15output_dimensionality\x18\x05 \x01(\x05B\x04\xe2A\x01\x01H\x02R\x14outputDimensionality\x88\x01\x01B\f\n" +
	"\n" +
	"_task_typeB\b\n" +
	"\x06_titleB\x18\n" +
	"\x16_output_dimensionality\"*\n" +
	"\x10ContentEmbedding\x12\x16\n" +
	"\x06values\x18\x01 \x03(\x02R\x06values\"l\n" +
	"\x14EmbedContentResponse\x12T\n" +
	"\tembedding\x18\x01 \x01(\v20.qclaogui.generativelanguage.v1.ContentEmbeddingB\x04\xe2A\x01\x03R\tembedding\"\xb8\x01\n" +
	"\x19BatchEmbedContentsRequest\x12D\n" +
	"\x05model\x18\x01 \x01(\tB.\xe2A\x01\x02\xfaA'\n" +
	"%generativelanguage.qclaogui.com/ModelR\x05model\x12U\n" +
	"\brequests\x18\x02 \x03(\v23.qclaogui.generativelanguage.v1.EmbedContentRequestB\x04\xe2A\x01\x02R\brequests\"t\n" +
	"\x1aBatchEmbedContentsResponse\x12V\n" +
	"\n" +
	"embeddings\x18\x01 \x03(\v20.qclaogui.generativelanguage.v1.ContentEmbeddingB\x04\xe2A\x01\x03R\n" +
	"embeddings\"\x9d\x02\n" +
	"\x12CountTokensRequest\x12D\n" +
	"\x05model\x18\x01 \x01(\tB.\xe2A\x01\x02\xfaA'\n" +
	"%generativelanguage.qclaogui.com/ModelR\x05model\x12I\n" +
	"\bcontents\x18\x02 \x03(\v2'.qclaogui.generativelanguage.v1.ContentB\x04\xe2A\x01\x01R\bcontents\x12v\n" +
	"\x18generate_content_request\x18\x03 \x01(\v26.qclaogui.generativelanguage.v1.GenerateContentRequestB\x04\xe2A\x01\x01R\x16generateContentRequest\"8\n" +
	"\x13CountTokensResponse\x12!\n" +
	"\ftotal_tokens\x18\x01 \x01(\x05R\vtotalTokens*\xbe\x01\n" +
	"\bTaskType\x12\x19\n" +
	"\x15TASK_TYPE_UNSPECIFIED\x10\x00\x12\x13\n" +
	"\x0fRETRIEVAL_QUERY\x10\x01\x12\x16\n" +
	"\x12RETRIEVAL_DOCUMENT\x10\x02\x12\x17\n" +
	"\x13SEMANTIC_SIMILARITY\x10\x03\x12\x12\n" +
	"\x0eCLASSIFICATION\x10\x04\x12\x0e\n" +
	"\n" +
	"CLUSTERING\x10\x05\x12\x16\n" +
	"\x12QUESTION_ANSWERING\x10\x06\x12\x15\n" +
	"\x11FACT_VERIFICATION\x10\a2\xc7\b\n" +
	"\x11GenerativeService\x12\xf4\x01\n" +
	"\x0fGenerateContent\x126.qclaogui.generativelanguage.v1.GenerateContentRequest\x1a7.qclaogui.generativelanguage.v1.GenerateContentResponse\"p\xdaA\x0emodel,contents\x82\xd3\xe4\x93\x02Y:\x01*Z.:\x01*\")/v1/{model=tunedModels/*}:generateContent\"$/v1/{model=models/*}:generateContent\x12\xd2\x01\n" +
	"\x15StreamGenerateContent\x126.qclaogui.generativelanguage.v1.GenerateContentRequest\x1a7.qclaogui.generativelanguage.v1.GenerateContentResponse\"F\xdaA\x0emodel,contents\x82\xd3\xe4\x93\x02/:\x01*\"*/v1/{model=models/*}:streamGenerateContent0\x01\x12\xb7\x01\n" +
	"\fEmbedContent\x123.qclaogui.generativelanguage.v1.EmbedContentRequest\x1a4.qclaogui.generativelanguage.v1.EmbedContentResponse\"<\xdaA\rmodel,content\x82\xd3\xe4\x93\x02&:\x01*\"!/v1/{model=models/*}:embedContent\x12\xd0\x01\n" +
	"\x12BatchEmbedContents\x129.qclaogui.generativelanguage.v1.BatchEmbedContentsRequest\x1a:.qclaogui.generativelanguage.v1.BatchEmbedContentsResponse\"C\xdaA\x0emodel,requests\x82\xd3\xe4\x93\x02,:\x01*\"'/v1/{model=models/*}:batchEmbedContents\x12\xb4\x01\n" +
	"\vCountTokens\x122.qclaogui.generativelanguage.v1.CountTokensRequest\x1a3.qclaogui.generativelanguage.v1.CountTokensResponse\"<\xdaA\x0emodel,contents\x82\xd3\xe4\x93\x02%:\x01*\" /v1/{model=models/*}:countTokens\x1a\"\xcaA\x1fgenerativelanguage.qclaogui.comBQZOgithub.com/qclaogui/gaip/genproto/generativelanguage/apiv1/generativelanguagepbb\x06proto3"

var (
	file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescOnce sync.Once
	file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescData []byte
)

func file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescGZIP() []byte {
	file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescOnce.Do(func() {
		file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_qclaogui_generativelanguage_v1_generative_service_proto_rawDesc), len(file_qclaogui_generativelanguage_v1_generative_service_proto_rawDesc)))
	})
	return file_qclaogui_generativelanguage_v1_generative_service_proto_rawDescData
}

var (
	file_qclaogui_generativelanguage_v1_generative_service_proto_enumTypes = make([]protoimpl.EnumInfo, 3)
	file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes  = make([]protoimpl.MessageInfo, 16)
	file_qclaogui_generativelanguage_v1_generative_service_proto_goTypes   = []any{
		(TaskType)(0), // 0: qclaogui.generativelanguage.v1.TaskType
		(GenerateContentResponse_PromptFeedback_BlockReason)(0), // 1: qclaogui.generativelanguage.v1.GenerateContentResponse.PromptFeedback.BlockReason
		(Candidate_FinishReason)(0),                             // 2: qclaogui.generativelanguage.v1.Candidate.FinishReason
		(*GenerateContentRequest)(nil),                          // 3: qclaogui.generativelanguage.v1.GenerateContentRequest
		(*GenerationConfig)(nil),                                // 4: qclaogui.generativelanguage.v1.GenerationConfig
		(*GenerateContentResponse)(nil),                         // 5: qclaogui.generativelanguage.v1.GenerateContentResponse
		(*Candidate)(nil),                                       // 6: qclaogui.generativelanguage.v1.Candidate
		(*LogprobsResult)(nil),                                  // 7: qclaogui.generativelanguage.v1.LogprobsResult
		(*EmbedContentRequest)(nil),                             // 8: qclaogui.generativelanguage.v1.EmbedContentRequest
		(*ContentEmbedding)(nil),                                // 9: qclaogui.generativelanguage.v1.ContentEmbedding
		(*EmbedContentResponse)(nil),                            // 10: qclaogui.generativelanguage.v1.EmbedContentResponse
		(*BatchEmbedContentsRequest)(nil),                       // 11: qclaogui.generativelanguage.v1.BatchEmbedContentsRequest
		(*BatchEmbedContentsResponse)(nil),                      // 12: qclaogui.generativelanguage.v1.BatchEmbedContentsResponse
		(*CountTokensRequest)(nil),                              // 13: qclaogui.generativelanguage.v1.CountTokensRequest
		(*CountTokensResponse)(nil),                             // 14: qclaogui.generativelanguage.v1.CountTokensResponse
		(*GenerateContentResponse_PromptFeedback)(nil),          // 15: qclaogui.generativelanguage.v1.GenerateContentResponse.PromptFeedback
		(*GenerateContentResponse_UsageMetadata)(nil),           // 16: qclaogui.generativelanguage.v1.GenerateContentResponse.UsageMetadata
		(*LogprobsResult_Candidate)(nil),                        // 17: qclaogui.generativelanguage.v1.LogprobsResult.Candidate
		(*LogprobsResult_TopCandidates)(nil),                    // 18: qclaogui.generativelanguage.v1.LogprobsResult.TopCandidates
		(*Content)(nil),                                         // 19: qclaogui.generativelanguage.v1.Content
		(*SafetySetting)(nil),                                   // 20: qclaogui.generativelanguage.v1.SafetySetting
		(*SafetyRating)(nil),                                    // 21: qclaogui.generativelanguage.v1.SafetyRating
		(*CitationMetadata)(nil),                                // 22: qclaogui.generativelanguage.v1.CitationMetadata
	}
)

var file_qclaogui_generativelanguage_v1_generative_service_proto_depIdxs = []int32{
	19, // 0: qclaogui.generativelanguage.v1.GenerateContentRequest.contents:type_name -> qclaogui.generativelanguage.v1.Content
	20, // 1: qclaogui.generativelanguage.v1.GenerateContentRequest.safety_settings:type_name -> qclaogui.generativelanguage.v1.SafetySetting
	4,  // 2: qclaogui.generativelanguage.v1.GenerateContentRequest.generation_config:type_name -> qclaogui.generativelanguage.v1.GenerationConfig
	6,  // 3: qclaogui.generativelanguage.v1.GenerateContentResponse.candidates:type_name -> qclaogui.generativelanguage.v1.Candidate
	15, // 4: qclaogui.generativelanguage.v1.GenerateContentResponse.prompt_feedback:type_name -> qclaogui.generativelanguage.v1.GenerateContentResponse.PromptFeedback
	16, // 5: qclaogui.generativelanguage.v1.GenerateContentResponse.usage_metadata:type_name -> qclaogui.generativelanguage.v1.GenerateContentResponse.UsageMetadata
	19, // 6: qclaogui.generativelanguage.v1.Candidate.content:type_name -> qclaogui.generativelanguage.v1.Content
	2,  // 7: qclaogui.generativelanguage.v1.Candidate.finish_reason:type_name -> qclaogui.generativelanguage.v1.Candidate.FinishReason
	21, // 8: qclaogui.generativelanguage.v1.Candidate.safety_ratings:type_name -> qclaogui.generativelanguage.v1.SafetyRating
	22, // 9: qclaogui.generativelanguage.v1.Candidate.citation_metadata:type_name -> qclaogui.generativelanguage.v1.CitationMetadata
	7,  // 10: qclaogui.generativelanguage.v1.Candidate.logprobs_result:type_name -> qclaogui.generativelanguage.v1.LogprobsResult
	18, // 11: qclaogui.generativelanguage.v1.LogprobsResult.top_candidates:type_name -> qclaogui.generativelanguage.v1.LogprobsResult.TopCandidates
	17, // 12: qclaogui.generativelanguage.v1.LogprobsResult.chosen_candidates:type_name -> qclaogui.generativelanguage.v1.LogprobsResult.Candidate
	19, // 13: qclaogui.generativelanguage.v1.EmbedContentRequest.content:type_name -> qclaogui.generativelanguage.v1.Content
	0,  // 14: qclaogui.generativelanguage.v1.EmbedContentRequest.task_type:type_name -> qclaogui.generativelanguage.v1.TaskType
	9,  // 15: qclaogui.generativelanguage.v1.EmbedContentResponse.embedding:type_name -> qclaogui.generativelanguage.v1.ContentEmbedding
	8,  // 16: qclaogui.generativelanguage.v1.BatchEmbedContentsRequest.requests:type_name -> qclaogui.generativelanguage.v1.EmbedContentRequest
	9,  // 17: qclaogui.generativelanguage.v1.BatchEmbedContentsResponse.embeddings:type_name -> qclaogui.generativelanguage.v1.ContentEmbedding
	19, // 18: qclaogui.generativelanguage.v1.CountTokensRequest.contents:type_name -> qclaogui.generativelanguage.v1.Content
	3,  // 19: qclaogui.generativelanguage.v1.CountTokensRequest.generate_content_request:type_name -> qclaogui.generativelanguage.v1.GenerateContentRequest
	1,  // 20: qclaogui.generativelanguage.v1.GenerateContentResponse.PromptFeedback.block_reason:type_name -> qclaogui.generativelanguage.v1.GenerateContentResponse.PromptFeedback.BlockReason
	21, // 21: qclaogui.generativelanguage.v1.GenerateContentResponse.PromptFeedback.safety_ratings:type_name -> qclaogui.generativelanguage.v1.SafetyRating
	17, // 22: qclaogui.generativelanguage.v1.LogprobsResult.TopCandidates.candidates:type_name -> qclaogui.generativelanguage.v1.LogprobsResult.Candidate
	3,  // 23: qclaogui.generativelanguage.v1.GenerativeService.GenerateContent:input_type -> qclaogui.generativelanguage.v1.GenerateContentRequest
	3,  // 24: qclaogui.generativelanguage.v1.GenerativeService.StreamGenerateContent:input_type -> qclaogui.generativelanguage.v1.GenerateContentRequest
	8,  // 25: qclaogui.generativelanguage.v1.GenerativeService.EmbedContent:input_type -> qclaogui.generativelanguage.v1.EmbedContentRequest
	11, // 26: qclaogui.generativelanguage.v1.GenerativeService.BatchEmbedContents:input_type -> qclaogui.generativelanguage.v1.BatchEmbedContentsRequest
	13, // 27: qclaogui.generativelanguage.v1.GenerativeService.CountTokens:input_type -> qclaogui.generativelanguage.v1.CountTokensRequest
	5,  // 28: qclaogui.generativelanguage.v1.GenerativeService.GenerateContent:output_type -> qclaogui.generativelanguage.v1.GenerateContentResponse
	5,  // 29: qclaogui.generativelanguage.v1.GenerativeService.StreamGenerateContent:output_type -> qclaogui.generativelanguage.v1.GenerateContentResponse
	10, // 30: qclaogui.generativelanguage.v1.GenerativeService.EmbedContent:output_type -> qclaogui.generativelanguage.v1.EmbedContentResponse
	12, // 31: qclaogui.generativelanguage.v1.GenerativeService.BatchEmbedContents:output_type -> qclaogui.generativelanguage.v1.BatchEmbedContentsResponse
	14, // 32: qclaogui.generativelanguage.v1.GenerativeService.CountTokens:output_type -> qclaogui.generativelanguage.v1.CountTokensResponse
	28, // [28:33] is the sub-list for method output_type
	23, // [23:28] is the sub-list for method input_type
	23, // [23:23] is the sub-list for extension type_name
	23, // [23:23] is the sub-list for extension extendee
	0,  // [0:23] is the sub-list for field type_name
}

func init() { file_qclaogui_generativelanguage_v1_generative_service_proto_init() }
func file_qclaogui_generativelanguage_v1_generative_service_proto_init() {
	if File_qclaogui_generativelanguage_v1_generative_service_proto != nil {
		return
	}
	file_qclaogui_generativelanguage_v1_citation_proto_init()
	file_qclaogui_generativelanguage_v1_content_proto_init()
	file_qclaogui_generativelanguage_v1_safety_proto_init()
	file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[0].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[1].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[3].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[5].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes[14].OneofWrappers = []any{}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_qclaogui_generativelanguage_v1_generative_service_proto_rawDesc), len(file_qclaogui_generativelanguage_v1_generative_service_proto_rawDesc)),
			NumEnums:      3,
			NumMessages:   16,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_qclaogui_generativelanguage_v1_generative_service_proto_goTypes,
		DependencyIndexes: file_qclaogui_generativelanguage_v1_generative_service_proto_depIdxs,
		EnumInfos:         file_qclaogui_generativelanguage_v1_generative_service_proto_enumTypes,
		MessageInfos:      file_qclaogui_generativelanguage_v1_generative_service_proto_msgTypes,
	}.Build()
	File_qclaogui_generativelanguage_v1_generative_service_proto = out.File
	file_qclaogui_generativelanguage_v1_generative_service_proto_goTypes = nil
	file_qclaogui_generativelanguage_v1_generative_service_proto_depIdxs = nil
}
