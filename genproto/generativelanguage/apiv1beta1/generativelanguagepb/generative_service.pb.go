// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.6
// 	protoc        v6.30.1
// source: qclaogui/generativelanguage/v1beta1/generative_service.proto

package generativelanguagepb

import (
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"

	_ "google.golang.org/genproto/googleapis/api/annotations"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Type of task for which the embedding will be used.
type TaskType int32

const (
	// Unset value, which will default to one of the other enum values.
	TaskType_TASK_TYPE_UNSPECIFIED TaskType = 0
	// Specifies the given text is a query in a search/retrieval setting.
	TaskType_RETRIEVAL_QUERY TaskType = 1
	// Specifies the given text is a document from the corpus being searched.
	TaskType_RETRIEVAL_DOCUMENT TaskType = 2
	// Specifies the given text will be used for STS.
	TaskType_SEMANTIC_SIMILARITY TaskType = 3
	// Specifies that the given text will be classified.
	TaskType_CLASSIFICATION TaskType = 4
	// Specifies that the embeddings will be used for clustering.
	TaskType_CLUSTERING TaskType = 5
	// Specifies that the given text will be used for question answering.
	TaskType_QUESTION_ANSWERING TaskType = 6
	// Specifies that the given text will be used for fact verification.
	TaskType_FACT_VERIFICATION TaskType = 7
)

// Enum value maps for TaskType.
var (
	TaskType_name = map[int32]string{
		0: "TASK_TYPE_UNSPECIFIED",
		1: "RETRIEVAL_QUERY",
		2: "RETRIEVAL_DOCUMENT",
		3: "SEMANTIC_SIMILARITY",
		4: "CLASSIFICATION",
		5: "CLUSTERING",
		6: "QUESTION_ANSWERING",
		7: "FACT_VERIFICATION",
	}
	TaskType_value = map[string]int32{
		"TASK_TYPE_UNSPECIFIED": 0,
		"RETRIEVAL_QUERY":       1,
		"RETRIEVAL_DOCUMENT":    2,
		"SEMANTIC_SIMILARITY":   3,
		"CLASSIFICATION":        4,
		"CLUSTERING":            5,
		"QUESTION_ANSWERING":    6,
		"FACT_VERIFICATION":     7,
	}
)

func (x TaskType) Enum() *TaskType {
	p := new(TaskType)
	*p = x
	return p
}

func (x TaskType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (TaskType) Descriptor() protoreflect.EnumDescriptor {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_enumTypes[0].Descriptor()
}

func (TaskType) Type() protoreflect.EnumType {
	return &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_enumTypes[0]
}

func (x TaskType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use TaskType.Descriptor instead.
func (TaskType) EnumDescriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{0}
}

// Specifies the reason why the prompt was blocked.
type GenerateContentResponse_PromptFeedback_BlockReason int32

const (
	// Default value. This value is unused.
	GenerateContentResponse_PromptFeedback_BLOCK_REASON_UNSPECIFIED GenerateContentResponse_PromptFeedback_BlockReason = 0
	// Prompt was blocked due to safety reasons. Inspect `safety_ratings`
	// to understand which safety category blocked it.
	GenerateContentResponse_PromptFeedback_SAFETY GenerateContentResponse_PromptFeedback_BlockReason = 1
	// Prompt was blocked due to unknown reasons.
	GenerateContentResponse_PromptFeedback_OTHER GenerateContentResponse_PromptFeedback_BlockReason = 2
	// Prompt was blocked due to the terms which are included from the
	// terminology blocklist.
	GenerateContentResponse_PromptFeedback_BLOCKLIST GenerateContentResponse_PromptFeedback_BlockReason = 3
	// Prompt was blocked due to prohibited content.
	GenerateContentResponse_PromptFeedback_PROHIBITED_CONTENT GenerateContentResponse_PromptFeedback_BlockReason = 4
)

// Enum value maps for GenerateContentResponse_PromptFeedback_BlockReason.
var (
	GenerateContentResponse_PromptFeedback_BlockReason_name = map[int32]string{
		0: "BLOCK_REASON_UNSPECIFIED",
		1: "SAFETY",
		2: "OTHER",
		3: "BLOCKLIST",
		4: "PROHIBITED_CONTENT",
	}
	GenerateContentResponse_PromptFeedback_BlockReason_value = map[string]int32{
		"BLOCK_REASON_UNSPECIFIED": 0,
		"SAFETY":                   1,
		"OTHER":                    2,
		"BLOCKLIST":                3,
		"PROHIBITED_CONTENT":       4,
	}
)

func (x GenerateContentResponse_PromptFeedback_BlockReason) Enum() *GenerateContentResponse_PromptFeedback_BlockReason {
	p := new(GenerateContentResponse_PromptFeedback_BlockReason)
	*p = x
	return p
}

func (x GenerateContentResponse_PromptFeedback_BlockReason) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (GenerateContentResponse_PromptFeedback_BlockReason) Descriptor() protoreflect.EnumDescriptor {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_enumTypes[1].Descriptor()
}

func (GenerateContentResponse_PromptFeedback_BlockReason) Type() protoreflect.EnumType {
	return &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_enumTypes[1]
}

func (x GenerateContentResponse_PromptFeedback_BlockReason) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use GenerateContentResponse_PromptFeedback_BlockReason.Descriptor instead.
func (GenerateContentResponse_PromptFeedback_BlockReason) EnumDescriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{3, 0, 0}
}

// Defines the reason why the model stopped generating tokens.
type Candidate_FinishReason int32

const (
	// Default value. This value is unused.
	Candidate_FINISH_REASON_UNSPECIFIED Candidate_FinishReason = 0
	// Natural stop point of the model or provided stop sequence.
	Candidate_STOP Candidate_FinishReason = 1
	// The maximum number of tokens as specified in the request was reached.
	Candidate_MAX_TOKENS Candidate_FinishReason = 2
	// The response candidate content was flagged for safety reasons.
	Candidate_SAFETY Candidate_FinishReason = 3
	// The response candidate content was flagged for recitation reasons.
	Candidate_RECITATION Candidate_FinishReason = 4
	// The response candidate content was flagged for using an unsupported
	// language.
	Candidate_LANGUAGE Candidate_FinishReason = 6
	// Unknown reason.
	Candidate_OTHER Candidate_FinishReason = 5
	// Token generation stopped because the content contains forbidden terms.
	Candidate_BLOCKLIST Candidate_FinishReason = 7
	// Token generation stopped for potentially containing prohibited content.
	Candidate_PROHIBITED_CONTENT Candidate_FinishReason = 8
	// Token generation stopped because the content potentially contains
	// Sensitive Personally Identifiable Information (SPII).
	Candidate_SPII Candidate_FinishReason = 9
	// The function call generated by the model is invalid.
	Candidate_MALFORMED_FUNCTION_CALL Candidate_FinishReason = 10
)

// Enum value maps for Candidate_FinishReason.
var (
	Candidate_FinishReason_name = map[int32]string{
		0:  "FINISH_REASON_UNSPECIFIED",
		1:  "STOP",
		2:  "MAX_TOKENS",
		3:  "SAFETY",
		4:  "RECITATION",
		6:  "LANGUAGE",
		5:  "OTHER",
		7:  "BLOCKLIST",
		8:  "PROHIBITED_CONTENT",
		9:  "SPII",
		10: "MALFORMED_FUNCTION_CALL",
	}
	Candidate_FinishReason_value = map[string]int32{
		"FINISH_REASON_UNSPECIFIED": 0,
		"STOP":                      1,
		"MAX_TOKENS":                2,
		"SAFETY":                    3,
		"RECITATION":                4,
		"LANGUAGE":                  6,
		"OTHER":                     5,
		"BLOCKLIST":                 7,
		"PROHIBITED_CONTENT":        8,
		"SPII":                      9,
		"MALFORMED_FUNCTION_CALL":   10,
	}
)

func (x Candidate_FinishReason) Enum() *Candidate_FinishReason {
	p := new(Candidate_FinishReason)
	*p = x
	return p
}

func (x Candidate_FinishReason) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (Candidate_FinishReason) Descriptor() protoreflect.EnumDescriptor {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_enumTypes[2].Descriptor()
}

func (Candidate_FinishReason) Type() protoreflect.EnumType {
	return &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_enumTypes[2]
}

func (x Candidate_FinishReason) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use Candidate_FinishReason.Descriptor instead.
func (Candidate_FinishReason) EnumDescriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{4, 0}
}

// Style for grounded answers.
type GenerateAnswerRequest_AnswerStyle int32

const (
	// Unspecified answer style.
	GenerateAnswerRequest_ANSWER_STYLE_UNSPECIFIED GenerateAnswerRequest_AnswerStyle = 0
	// Succint but abstract style.
	GenerateAnswerRequest_ABSTRACTIVE GenerateAnswerRequest_AnswerStyle = 1
	// Very brief and extractive style.
	GenerateAnswerRequest_EXTRACTIVE GenerateAnswerRequest_AnswerStyle = 2
	// Verbose style including extra details. The response may be formatted as a
	// sentence, paragraph, multiple paragraphs, or bullet points, etc.
	GenerateAnswerRequest_VERBOSE GenerateAnswerRequest_AnswerStyle = 3
)

// Enum value maps for GenerateAnswerRequest_AnswerStyle.
var (
	GenerateAnswerRequest_AnswerStyle_name = map[int32]string{
		0: "ANSWER_STYLE_UNSPECIFIED",
		1: "ABSTRACTIVE",
		2: "EXTRACTIVE",
		3: "VERBOSE",
	}
	GenerateAnswerRequest_AnswerStyle_value = map[string]int32{
		"ANSWER_STYLE_UNSPECIFIED": 0,
		"ABSTRACTIVE":              1,
		"EXTRACTIVE":               2,
		"VERBOSE":                  3,
	}
)

func (x GenerateAnswerRequest_AnswerStyle) Enum() *GenerateAnswerRequest_AnswerStyle {
	p := new(GenerateAnswerRequest_AnswerStyle)
	*p = x
	return p
}

func (x GenerateAnswerRequest_AnswerStyle) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (GenerateAnswerRequest_AnswerStyle) Descriptor() protoreflect.EnumDescriptor {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_enumTypes[3].Descriptor()
}

func (GenerateAnswerRequest_AnswerStyle) Type() protoreflect.EnumType {
	return &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_enumTypes[3]
}

func (x GenerateAnswerRequest_AnswerStyle) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use GenerateAnswerRequest_AnswerStyle.Descriptor instead.
func (GenerateAnswerRequest_AnswerStyle) EnumDescriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{14, 0}
}

// Specifies what was the reason why input was blocked.
type GenerateAnswerResponse_InputFeedback_BlockReason int32

const (
	// Default value. This value is unused.
	GenerateAnswerResponse_InputFeedback_BLOCK_REASON_UNSPECIFIED GenerateAnswerResponse_InputFeedback_BlockReason = 0
	// Input was blocked due to safety reasons. Inspect
	// `safety_ratings` to understand which safety category blocked it.
	GenerateAnswerResponse_InputFeedback_SAFETY GenerateAnswerResponse_InputFeedback_BlockReason = 1
	// Input was blocked due to other reasons.
	GenerateAnswerResponse_InputFeedback_OTHER GenerateAnswerResponse_InputFeedback_BlockReason = 2
)

// Enum value maps for GenerateAnswerResponse_InputFeedback_BlockReason.
var (
	GenerateAnswerResponse_InputFeedback_BlockReason_name = map[int32]string{
		0: "BLOCK_REASON_UNSPECIFIED",
		1: "SAFETY",
		2: "OTHER",
	}
	GenerateAnswerResponse_InputFeedback_BlockReason_value = map[string]int32{
		"BLOCK_REASON_UNSPECIFIED": 0,
		"SAFETY":                   1,
		"OTHER":                    2,
	}
)

func (x GenerateAnswerResponse_InputFeedback_BlockReason) Enum() *GenerateAnswerResponse_InputFeedback_BlockReason {
	p := new(GenerateAnswerResponse_InputFeedback_BlockReason)
	*p = x
	return p
}

func (x GenerateAnswerResponse_InputFeedback_BlockReason) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (GenerateAnswerResponse_InputFeedback_BlockReason) Descriptor() protoreflect.EnumDescriptor {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_enumTypes[4].Descriptor()
}

func (GenerateAnswerResponse_InputFeedback_BlockReason) Type() protoreflect.EnumType {
	return &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_enumTypes[4]
}

func (x GenerateAnswerResponse_InputFeedback_BlockReason) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use GenerateAnswerResponse_InputFeedback_BlockReason.Descriptor instead.
func (GenerateAnswerResponse_InputFeedback_BlockReason) EnumDescriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{15, 0, 0}
}

// Request to generate a completion from the model.
type GenerateContentRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The name of the `Model` to use for generating the completion.
	//
	// Format: `name=models/{model}`.
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Optional. Developer set [system
	// instruction(s)](https://ai.google.dev/gemini-api/docs/system-instructions).
	// Currently, text only.
	SystemInstruction *Content `protobuf:"bytes,8,opt,name=system_instruction,json=systemInstruction,proto3,oneof" json:"system_instruction,omitempty"`
	// Required. The content of the current conversation with the model.
	//
	// For single-turn queries, this is a single instance. For multi-turn queries
	// like [chat](https://ai.google.dev/gemini-api/docs/text-generation#chat),
	// this is a repeated field that contains the conversation history and the
	// latest request.
	Contents []*Content `protobuf:"bytes,2,rep,name=contents,proto3" json:"contents,omitempty"`
	// Optional. A list of `Tools` the `Model` may use to generate the next
	// response.
	//
	// A `Tool` is a piece of code that enables the system to interact with
	// external systems to perform an action, or set of actions, outside of
	// knowledge and scope of the `Model`. Supported `Tool`s are `Function` and
	// `code_execution`. Refer to the [Function
	// calling](https://ai.google.dev/gemini-api/docs/function-calling) and the
	// [Code execution](https://ai.google.dev/gemini-api/docs/code-execution)
	// guides to learn more.
	Tools []*Tool `protobuf:"bytes,5,rep,name=tools,proto3" json:"tools,omitempty"`
	// Optional. Tool configuration for any `Tool` specified in the request. Refer
	// to the [Function calling
	// guide](https://ai.google.dev/gemini-api/docs/function-calling#function_calling_mode)
	// for a usage example.
	ToolConfig *ToolConfig `protobuf:"bytes,7,opt,name=tool_config,json=toolConfig,proto3" json:"tool_config,omitempty"`
	// Optional. A list of unique `SafetySetting` instances for blocking unsafe
	// content.
	//
	// This will be enforced on the `GenerateContentRequest.contents` and
	// `GenerateContentResponse.candidates`. There should not be more than one
	// setting for each `SafetyCategory` type. The API will block any contents and
	// responses that fail to meet the thresholds set by these settings. This list
	// overrides the default settings for each `SafetyCategory` specified in the
	// safety_settings. If there is no `SafetySetting` for a given
	// `SafetyCategory` provided in the list, the API will use the default safety
	// setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
	// HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
	// HARM_CATEGORY_HARASSMENT are supported. Refer to the
	// [guide](https://ai.google.dev/gemini-api/docs/safety-settings)
	// for detailed information on available safety settings. Also refer to the
	// [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance) to
	// learn how to incorporate safety considerations in your AI applications.
	SafetySettings []*SafetySetting `protobuf:"bytes,3,rep,name=safety_settings,json=safetySettings,proto3" json:"safety_settings,omitempty"`
	// Optional. Configuration options for model generation and outputs.
	GenerationConfig *GenerationConfig `protobuf:"bytes,4,opt,name=generation_config,json=generationConfig,proto3,oneof" json:"generation_config,omitempty"`
	// Optional. The name of the content
	// [cached](https://ai.google.dev/gemini-api/docs/caching) to use as context
	// to serve the prediction. Format: `cachedContents/{cachedContent}`
	CachedContent *string `protobuf:"bytes,9,opt,name=cached_content,json=cachedContent,proto3,oneof" json:"cached_content,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerateContentRequest) Reset() {
	*x = GenerateContentRequest{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateContentRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateContentRequest) ProtoMessage() {}

func (x *GenerateContentRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateContentRequest.ProtoReflect.Descriptor instead.
func (*GenerateContentRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{0}
}

func (x *GenerateContentRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *GenerateContentRequest) GetSystemInstruction() *Content {
	if x != nil {
		return x.SystemInstruction
	}
	return nil
}

func (x *GenerateContentRequest) GetContents() []*Content {
	if x != nil {
		return x.Contents
	}
	return nil
}

func (x *GenerateContentRequest) GetTools() []*Tool {
	if x != nil {
		return x.Tools
	}
	return nil
}

func (x *GenerateContentRequest) GetToolConfig() *ToolConfig {
	if x != nil {
		return x.ToolConfig
	}
	return nil
}

func (x *GenerateContentRequest) GetSafetySettings() []*SafetySetting {
	if x != nil {
		return x.SafetySettings
	}
	return nil
}

func (x *GenerateContentRequest) GetGenerationConfig() *GenerationConfig {
	if x != nil {
		return x.GenerationConfig
	}
	return nil
}

func (x *GenerateContentRequest) GetCachedContent() string {
	if x != nil && x.CachedContent != nil {
		return *x.CachedContent
	}
	return ""
}

// Configuration options for model generation and outputs. Not all parameters
// are configurable for every model.
type GenerationConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. Number of generated responses to return.
	//
	// Currently, this value can only be set to 1. If unset, this will default
	// to 1.
	CandidateCount *int32 `protobuf:"varint,1,opt,name=candidate_count,json=candidateCount,proto3,oneof" json:"candidate_count,omitempty"`
	// Optional. The set of character sequences (up to 5) that will stop output
	// generation. If specified, the API will stop at the first appearance of a
	// `stop_sequence`. The stop sequence will not be included as part of the
	// response.
	StopSequences []string `protobuf:"bytes,2,rep,name=stop_sequences,json=stopSequences,proto3" json:"stop_sequences,omitempty"`
	// Optional. The maximum number of tokens to include in a response candidate.
	//
	// Note: The default value varies by model, see the `Model.output_token_limit`
	// attribute of the `Model` returned from the `getModel` function.
	MaxOutputTokens *int32 `protobuf:"varint,4,opt,name=max_output_tokens,json=maxOutputTokens,proto3,oneof" json:"max_output_tokens,omitempty"`
	// Optional. Controls the randomness of the output.
	//
	// Note: The default value varies by model, see the `Model.temperature`
	// attribute of the `Model` returned from the `getModel` function.
	//
	// Values can range from [0.0, 2.0].
	Temperature *float32 `protobuf:"fixed32,5,opt,name=temperature,proto3,oneof" json:"temperature,omitempty"`
	// Optional. The maximum cumulative probability of tokens to consider when
	// sampling.
	//
	// The model uses combined Top-k and Top-p (nucleus) sampling.
	//
	// Tokens are sorted based on their assigned probabilities so that only the
	// most likely tokens are considered. Top-k sampling directly limits the
	// maximum number of tokens to consider, while Nucleus sampling limits the
	// number of tokens based on the cumulative probability.
	//
	// Note: The default value varies by `Model` and is specified by
	// the`Model.top_p` attribute returned from the `getModel` function. An empty
	// `top_k` attribute indicates that the model doesn't apply top-k sampling
	// and doesn't allow setting `top_k` on requests.
	TopP *float32 `protobuf:"fixed32,6,opt,name=top_p,json=topP,proto3,oneof" json:"top_p,omitempty"`
	// Optional. The maximum number of tokens to consider when sampling.
	//
	// Gemini models use Top-p (nucleus) sampling or a combination of Top-k and
	// nucleus sampling. Top-k sampling considers the set of `top_k` most probable
	// tokens. Models running with nucleus sampling don't allow top_k setting.
	//
	// Note: The default value varies by `Model` and is specified by
	// the`Model.top_p` attribute returned from the `getModel` function. An empty
	// `top_k` attribute indicates that the model doesn't apply top-k sampling
	// and doesn't allow setting `top_k` on requests.
	TopK *int32 `protobuf:"varint,7,opt,name=top_k,json=topK,proto3,oneof" json:"top_k,omitempty"`
	// Optional. MIME type of the generated candidate text.
	// Supported MIME types are:
	// `text/plain`: (default) Text output.
	// `application/json`: JSON response in the response candidates.
	// `text/x.enum`: ENUM as a string response in the response candidates.
	// Refer to the
	// [docs](https://ai.google.dev/gemini-api/docs/prompting_with_media#plain_text_formats)
	// for a list of all supported text MIME types.
	ResponseMimeType string `protobuf:"bytes,13,opt,name=response_mime_type,json=responseMimeType,proto3" json:"response_mime_type,omitempty"`
	// Optional. Output schema of the generated candidate text. Schemas must be a
	// subset of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schema)
	// and can be objects, primitives or arrays.
	//
	// If set, a compatible `response_mime_type` must also be set.
	// Compatible MIME types:
	// `application/json`: Schema for JSON response.
	// Refer to the [JSON text generation
	// guide](https://ai.google.dev/gemini-api/docs/json-mode) for more details.
	ResponseSchema *Schema `protobuf:"bytes,14,opt,name=response_schema,json=responseSchema,proto3" json:"response_schema,omitempty"`
	// Optional. Presence penalty applied to the next token's logprobs if the
	// token has already been seen in the response.
	//
	// This penalty is binary on/off and not dependant on the number of times the
	// token is used (after the first). Use
	// [frequency_penalty][qclaogui.generativelanguage.v1beta.GenerationConfig.frequency_penalty]
	// for a penalty that increases with each use.
	//
	// A positive penalty will discourage the use of tokens that have already
	// been used in the response, increasing the vocabulary.
	//
	// A negative penalty will encourage the use of tokens that have already been
	// used in the response, decreasing the vocabulary.
	PresencePenalty *float32 `protobuf:"fixed32,15,opt,name=presence_penalty,json=presencePenalty,proto3,oneof" json:"presence_penalty,omitempty"`
	// Optional. Frequency penalty applied to the next token's logprobs,
	// multiplied by the number of times each token has been seen in the respponse
	// so far.
	//
	// A positive penalty will discourage the use of tokens that have already
	// been used, proportional to the number of times the token has been used:
	// The more a token is used, the more dificult it is for the model to use
	// that token again increasing the vocabulary of responses.
	//
	// Caution: A _negative_ penalty will encourage the model to reuse tokens
	// proportional to the number of times the token has been used. Small
	// negative values will reduce the vocabulary of a response. Larger negative
	// values will cause the model to start repeating a common token  until it
	// hits the
	// [max_output_tokens][qclaogui.generativelanguage.v1beta.GenerationConfig.max_output_tokens]
	// limit: "...the the the the the...".
	FrequencyPenalty *float32 `protobuf:"fixed32,16,opt,name=frequency_penalty,json=frequencyPenalty,proto3,oneof" json:"frequency_penalty,omitempty"`
	// Optional. If true, export the logprobs results in response.
	ResponseLogprobs *bool `protobuf:"varint,17,opt,name=response_logprobs,json=responseLogprobs,proto3,oneof" json:"response_logprobs,omitempty"`
	// Optional. Only valid if
	// [response_logprobs=True][qclaogui.generativelanguage.v1beta.GenerationConfig.response_logprobs].
	// This sets the number of top logprobs to return at each decoding step in the
	// [Candidate.logprobs_result][qclaogui.generativelanguage.v1beta.Candidate.logprobs_result].
	Logprobs      *int32 `protobuf:"varint,18,opt,name=logprobs,proto3,oneof" json:"logprobs,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerationConfig) Reset() {
	*x = GenerationConfig{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerationConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerationConfig) ProtoMessage() {}

func (x *GenerationConfig) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerationConfig.ProtoReflect.Descriptor instead.
func (*GenerationConfig) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{1}
}

func (x *GenerationConfig) GetCandidateCount() int32 {
	if x != nil && x.CandidateCount != nil {
		return *x.CandidateCount
	}
	return 0
}

func (x *GenerationConfig) GetStopSequences() []string {
	if x != nil {
		return x.StopSequences
	}
	return nil
}

func (x *GenerationConfig) GetMaxOutputTokens() int32 {
	if x != nil && x.MaxOutputTokens != nil {
		return *x.MaxOutputTokens
	}
	return 0
}

func (x *GenerationConfig) GetTemperature() float32 {
	if x != nil && x.Temperature != nil {
		return *x.Temperature
	}
	return 0
}

func (x *GenerationConfig) GetTopP() float32 {
	if x != nil && x.TopP != nil {
		return *x.TopP
	}
	return 0
}

func (x *GenerationConfig) GetTopK() int32 {
	if x != nil && x.TopK != nil {
		return *x.TopK
	}
	return 0
}

func (x *GenerationConfig) GetResponseMimeType() string {
	if x != nil {
		return x.ResponseMimeType
	}
	return ""
}

func (x *GenerationConfig) GetResponseSchema() *Schema {
	if x != nil {
		return x.ResponseSchema
	}
	return nil
}

func (x *GenerationConfig) GetPresencePenalty() float32 {
	if x != nil && x.PresencePenalty != nil {
		return *x.PresencePenalty
	}
	return 0
}

func (x *GenerationConfig) GetFrequencyPenalty() float32 {
	if x != nil && x.FrequencyPenalty != nil {
		return *x.FrequencyPenalty
	}
	return 0
}

func (x *GenerationConfig) GetResponseLogprobs() bool {
	if x != nil && x.ResponseLogprobs != nil {
		return *x.ResponseLogprobs
	}
	return false
}

func (x *GenerationConfig) GetLogprobs() int32 {
	if x != nil && x.Logprobs != nil {
		return *x.Logprobs
	}
	return 0
}

// Configuration for retrieving grounding content from a `Corpus` or
// `Document` created using the Semantic Retriever API.
type SemanticRetrieverConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. Name of the resource for retrieval. Example: `corpora/123` or
	// `corpora/123/documents/abc`.
	Source string `protobuf:"bytes,1,opt,name=source,proto3" json:"source,omitempty"`
	// Required. Query to use for matching `Chunk`s in the given resource by
	// similarity.
	Query *Content `protobuf:"bytes,2,opt,name=query,proto3" json:"query,omitempty"`
	// Optional. Filters for selecting `Document`s and/or `Chunk`s from the
	// resource.
	MetadataFilters []*MetadataFilter `protobuf:"bytes,3,rep,name=metadata_filters,json=metadataFilters,proto3" json:"metadata_filters,omitempty"`
	// Optional. Maximum number of relevant `Chunk`s to retrieve.
	MaxChunksCount *int32 `protobuf:"varint,4,opt,name=max_chunks_count,json=maxChunksCount,proto3,oneof" json:"max_chunks_count,omitempty"`
	// Optional. Minimum relevance score for retrieved relevant `Chunk`s.
	MinimumRelevanceScore *float32 `protobuf:"fixed32,5,opt,name=minimum_relevance_score,json=minimumRelevanceScore,proto3,oneof" json:"minimum_relevance_score,omitempty"`
	unknownFields         protoimpl.UnknownFields
	sizeCache             protoimpl.SizeCache
}

func (x *SemanticRetrieverConfig) Reset() {
	*x = SemanticRetrieverConfig{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SemanticRetrieverConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SemanticRetrieverConfig) ProtoMessage() {}

func (x *SemanticRetrieverConfig) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SemanticRetrieverConfig.ProtoReflect.Descriptor instead.
func (*SemanticRetrieverConfig) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{2}
}

func (x *SemanticRetrieverConfig) GetSource() string {
	if x != nil {
		return x.Source
	}
	return ""
}

func (x *SemanticRetrieverConfig) GetQuery() *Content {
	if x != nil {
		return x.Query
	}
	return nil
}

func (x *SemanticRetrieverConfig) GetMetadataFilters() []*MetadataFilter {
	if x != nil {
		return x.MetadataFilters
	}
	return nil
}

func (x *SemanticRetrieverConfig) GetMaxChunksCount() int32 {
	if x != nil && x.MaxChunksCount != nil {
		return *x.MaxChunksCount
	}
	return 0
}

func (x *SemanticRetrieverConfig) GetMinimumRelevanceScore() float32 {
	if x != nil && x.MinimumRelevanceScore != nil {
		return *x.MinimumRelevanceScore
	}
	return 0
}

// Response from the model supporting multiple candidate responses.
//
// Safety ratings and content filtering are reported for both
// prompt in `GenerateContentResponse.prompt_feedback` and for each candidate
// in `finish_reason` and in `safety_ratings`. The API:
//   - Returns either all requested candidates or none of them
//   - Returns no candidates at all only if there was something wrong with the
//     prompt (check `prompt_feedback`)
//   - Reports feedback on each candidate in `finish_reason` and
//     `safety_ratings`.
type GenerateContentResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Candidate responses from the model.
	Candidates []*Candidate `protobuf:"bytes,1,rep,name=candidates,proto3" json:"candidates,omitempty"`
	// Returns the prompt's feedback related to the content filters.
	PromptFeedback *GenerateContentResponse_PromptFeedback `protobuf:"bytes,2,opt,name=prompt_feedback,json=promptFeedback,proto3" json:"prompt_feedback,omitempty"`
	// Output only. Metadata on the generation requests' token usage.
	UsageMetadata *GenerateContentResponse_UsageMetadata `protobuf:"bytes,3,opt,name=usage_metadata,json=usageMetadata,proto3" json:"usage_metadata,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerateContentResponse) Reset() {
	*x = GenerateContentResponse{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateContentResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateContentResponse) ProtoMessage() {}

func (x *GenerateContentResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateContentResponse.ProtoReflect.Descriptor instead.
func (*GenerateContentResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{3}
}

func (x *GenerateContentResponse) GetCandidates() []*Candidate {
	if x != nil {
		return x.Candidates
	}
	return nil
}

func (x *GenerateContentResponse) GetPromptFeedback() *GenerateContentResponse_PromptFeedback {
	if x != nil {
		return x.PromptFeedback
	}
	return nil
}

func (x *GenerateContentResponse) GetUsageMetadata() *GenerateContentResponse_UsageMetadata {
	if x != nil {
		return x.UsageMetadata
	}
	return nil
}

// A response candidate generated from the model.
type Candidate struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. Index of the candidate in the list of response candidates.
	Index *int32 `protobuf:"varint,3,opt,name=index,proto3,oneof" json:"index,omitempty"`
	// Output only. Generated content returned from the model.
	Content *Content `protobuf:"bytes,1,opt,name=content,proto3" json:"content,omitempty"`
	// Optional. Output only. The reason why the model stopped generating tokens.
	//
	// If empty, the model has not stopped generating tokens.
	FinishReason Candidate_FinishReason `protobuf:"varint,2,opt,name=finish_reason,json=finishReason,proto3,enum=qclaogui.generativelanguage.v1beta1.Candidate_FinishReason" json:"finish_reason,omitempty"`
	// List of ratings for the safety of a response candidate.
	//
	// There is at most one rating per category.
	SafetyRatings []*SafetyRating `protobuf:"bytes,5,rep,name=safety_ratings,json=safetyRatings,proto3" json:"safety_ratings,omitempty"`
	// Output only. Citation information for model-generated candidate.
	//
	// This field may be populated with recitation information for any text
	// included in the `content`. These are passages that are "recited" from
	// copyrighted material in the foundational LLM's training data.
	CitationMetadata *CitationMetadata `protobuf:"bytes,6,opt,name=citation_metadata,json=citationMetadata,proto3" json:"citation_metadata,omitempty"`
	// Output only. Token count for this candidate.
	TokenCount int32 `protobuf:"varint,7,opt,name=token_count,json=tokenCount,proto3" json:"token_count,omitempty"`
	// Output only. Attribution information for sources that contributed to a
	// grounded answer.
	//
	// This field is populated for `GenerateAnswer` calls.
	GroundingAttributions []*GroundingAttribution `protobuf:"bytes,8,rep,name=grounding_attributions,json=groundingAttributions,proto3" json:"grounding_attributions,omitempty"`
	// Output only. Grounding metadata for the candidate.
	//
	// This field is populated for `GenerateContent` calls.
	GroundingMetadata *GroundingMetadata `protobuf:"bytes,9,opt,name=grounding_metadata,json=groundingMetadata,proto3" json:"grounding_metadata,omitempty"`
	// Output only.
	AvgLogprobs float64 `protobuf:"fixed64,10,opt,name=avg_logprobs,json=avgLogprobs,proto3" json:"avg_logprobs,omitempty"`
	// Output only. Log-likelihood scores for the response tokens and top tokens
	LogprobsResult *LogprobsResult `protobuf:"bytes,11,opt,name=logprobs_result,json=logprobsResult,proto3" json:"logprobs_result,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *Candidate) Reset() {
	*x = Candidate{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Candidate) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Candidate) ProtoMessage() {}

func (x *Candidate) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Candidate.ProtoReflect.Descriptor instead.
func (*Candidate) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{4}
}

func (x *Candidate) GetIndex() int32 {
	if x != nil && x.Index != nil {
		return *x.Index
	}
	return 0
}

func (x *Candidate) GetContent() *Content {
	if x != nil {
		return x.Content
	}
	return nil
}

func (x *Candidate) GetFinishReason() Candidate_FinishReason {
	if x != nil {
		return x.FinishReason
	}
	return Candidate_FINISH_REASON_UNSPECIFIED
}

func (x *Candidate) GetSafetyRatings() []*SafetyRating {
	if x != nil {
		return x.SafetyRatings
	}
	return nil
}

func (x *Candidate) GetCitationMetadata() *CitationMetadata {
	if x != nil {
		return x.CitationMetadata
	}
	return nil
}

func (x *Candidate) GetTokenCount() int32 {
	if x != nil {
		return x.TokenCount
	}
	return 0
}

func (x *Candidate) GetGroundingAttributions() []*GroundingAttribution {
	if x != nil {
		return x.GroundingAttributions
	}
	return nil
}

func (x *Candidate) GetGroundingMetadata() *GroundingMetadata {
	if x != nil {
		return x.GroundingMetadata
	}
	return nil
}

func (x *Candidate) GetAvgLogprobs() float64 {
	if x != nil {
		return x.AvgLogprobs
	}
	return 0
}

func (x *Candidate) GetLogprobsResult() *LogprobsResult {
	if x != nil {
		return x.LogprobsResult
	}
	return nil
}

// Logprobs Result
type LogprobsResult struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Length = total number of decoding steps.
	TopCandidates []*LogprobsResult_TopCandidates `protobuf:"bytes,1,rep,name=top_candidates,json=topCandidates,proto3" json:"top_candidates,omitempty"`
	// Length = total number of decoding steps.
	// The chosen candidates may or may not be in top_candidates.
	ChosenCandidates []*LogprobsResult_Candidate `protobuf:"bytes,2,rep,name=chosen_candidates,json=chosenCandidates,proto3" json:"chosen_candidates,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *LogprobsResult) Reset() {
	*x = LogprobsResult{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LogprobsResult) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LogprobsResult) ProtoMessage() {}

func (x *LogprobsResult) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LogprobsResult.ProtoReflect.Descriptor instead.
func (*LogprobsResult) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{5}
}

func (x *LogprobsResult) GetTopCandidates() []*LogprobsResult_TopCandidates {
	if x != nil {
		return x.TopCandidates
	}
	return nil
}

func (x *LogprobsResult) GetChosenCandidates() []*LogprobsResult_Candidate {
	if x != nil {
		return x.ChosenCandidates
	}
	return nil
}

// Identifier for the source contributing to this attribution.
type AttributionSourceId struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Types that are valid to be assigned to Source:
	//
	//	*AttributionSourceId_GroundingPassage
	//	*AttributionSourceId_SemanticRetrieverChunk_
	Source        isAttributionSourceId_Source `protobuf_oneof:"source"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AttributionSourceId) Reset() {
	*x = AttributionSourceId{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AttributionSourceId) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AttributionSourceId) ProtoMessage() {}

func (x *AttributionSourceId) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AttributionSourceId.ProtoReflect.Descriptor instead.
func (*AttributionSourceId) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{6}
}

func (x *AttributionSourceId) GetSource() isAttributionSourceId_Source {
	if x != nil {
		return x.Source
	}
	return nil
}

func (x *AttributionSourceId) GetGroundingPassage() *AttributionSourceId_GroundingPassageId {
	if x != nil {
		if x, ok := x.Source.(*AttributionSourceId_GroundingPassage); ok {
			return x.GroundingPassage
		}
	}
	return nil
}

func (x *AttributionSourceId) GetSemanticRetrieverChunk() *AttributionSourceId_SemanticRetrieverChunk {
	if x != nil {
		if x, ok := x.Source.(*AttributionSourceId_SemanticRetrieverChunk_); ok {
			return x.SemanticRetrieverChunk
		}
	}
	return nil
}

type isAttributionSourceId_Source interface {
	isAttributionSourceId_Source()
}

type AttributionSourceId_GroundingPassage struct {
	// Identifier for an inline passage.
	GroundingPassage *AttributionSourceId_GroundingPassageId `protobuf:"bytes,1,opt,name=grounding_passage,json=groundingPassage,proto3,oneof"`
}

type AttributionSourceId_SemanticRetrieverChunk_ struct {
	// Identifier for a `Chunk` fetched via Semantic Retriever.
	SemanticRetrieverChunk *AttributionSourceId_SemanticRetrieverChunk `protobuf:"bytes,2,opt,name=semantic_retriever_chunk,json=semanticRetrieverChunk,proto3,oneof"`
}

func (*AttributionSourceId_GroundingPassage) isAttributionSourceId_Source() {}

func (*AttributionSourceId_SemanticRetrieverChunk_) isAttributionSourceId_Source() {}

// Attribution for a source that contributed to an answer.
type GroundingAttribution struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. Identifier for the source contributing to this attribution.
	SourceId *AttributionSourceId `protobuf:"bytes,3,opt,name=source_id,json=sourceId,proto3" json:"source_id,omitempty"`
	// Grounding source content that makes up this attribution.
	Content       *Content `protobuf:"bytes,2,opt,name=content,proto3" json:"content,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GroundingAttribution) Reset() {
	*x = GroundingAttribution{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GroundingAttribution) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GroundingAttribution) ProtoMessage() {}

func (x *GroundingAttribution) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GroundingAttribution.ProtoReflect.Descriptor instead.
func (*GroundingAttribution) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{7}
}

func (x *GroundingAttribution) GetSourceId() *AttributionSourceId {
	if x != nil {
		return x.SourceId
	}
	return nil
}

func (x *GroundingAttribution) GetContent() *Content {
	if x != nil {
		return x.Content
	}
	return nil
}

// Metadata related to retrieval in the grounding flow.
type RetrievalMetadata struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. Score indicating how likely information from google search could
	// help answer the prompt. The score is in the range [0, 1], where 0 is the
	// least likely and 1 is the most likely. This score is only populated when
	// google search grounding and dynamic retrieval is enabled. It will be
	// compared to the threshold to determine whether to trigger google search.
	GoogleSearchDynamicRetrievalScore float32 `protobuf:"fixed32,2,opt,name=google_search_dynamic_retrieval_score,json=googleSearchDynamicRetrievalScore,proto3" json:"google_search_dynamic_retrieval_score,omitempty"`
	unknownFields                     protoimpl.UnknownFields
	sizeCache                         protoimpl.SizeCache
}

func (x *RetrievalMetadata) Reset() {
	*x = RetrievalMetadata{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *RetrievalMetadata) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*RetrievalMetadata) ProtoMessage() {}

func (x *RetrievalMetadata) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use RetrievalMetadata.ProtoReflect.Descriptor instead.
func (*RetrievalMetadata) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{8}
}

func (x *RetrievalMetadata) GetGoogleSearchDynamicRetrievalScore() float32 {
	if x != nil {
		return x.GoogleSearchDynamicRetrievalScore
	}
	return 0
}

// Metadata returned to client when grounding is enabled.
type GroundingMetadata struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. Google search entry for the following-up web searches.
	SearchEntryPoint *SearchEntryPoint `protobuf:"bytes,1,opt,name=search_entry_point,json=searchEntryPoint,proto3,oneof" json:"search_entry_point,omitempty"`
	// List of supporting references retrieved from specified grounding source.
	GroundingChunks []*GroundingChunk `protobuf:"bytes,2,rep,name=grounding_chunks,json=groundingChunks,proto3" json:"grounding_chunks,omitempty"`
	// List of grounding support.
	GroundingSupports []*GroundingSupport `protobuf:"bytes,3,rep,name=grounding_supports,json=groundingSupports,proto3" json:"grounding_supports,omitempty"`
	// Metadata related to retrieval in the grounding flow.
	RetrievalMetadata *RetrievalMetadata `protobuf:"bytes,4,opt,name=retrieval_metadata,json=retrievalMetadata,proto3,oneof" json:"retrieval_metadata,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *GroundingMetadata) Reset() {
	*x = GroundingMetadata{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GroundingMetadata) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GroundingMetadata) ProtoMessage() {}

func (x *GroundingMetadata) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GroundingMetadata.ProtoReflect.Descriptor instead.
func (*GroundingMetadata) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{9}
}

func (x *GroundingMetadata) GetSearchEntryPoint() *SearchEntryPoint {
	if x != nil {
		return x.SearchEntryPoint
	}
	return nil
}

func (x *GroundingMetadata) GetGroundingChunks() []*GroundingChunk {
	if x != nil {
		return x.GroundingChunks
	}
	return nil
}

func (x *GroundingMetadata) GetGroundingSupports() []*GroundingSupport {
	if x != nil {
		return x.GroundingSupports
	}
	return nil
}

func (x *GroundingMetadata) GetRetrievalMetadata() *RetrievalMetadata {
	if x != nil {
		return x.RetrievalMetadata
	}
	return nil
}

// Google search entry point.
type SearchEntryPoint struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. Web content snippet that can be embedded in a web page or an app
	// webview.
	RenderedContent string `protobuf:"bytes,1,opt,name=rendered_content,json=renderedContent,proto3" json:"rendered_content,omitempty"`
	// Optional. Base64 encoded JSON representing array of <search term, search
	// url> tuple.
	SdkBlob       []byte `protobuf:"bytes,2,opt,name=sdk_blob,json=sdkBlob,proto3" json:"sdk_blob,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SearchEntryPoint) Reset() {
	*x = SearchEntryPoint{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SearchEntryPoint) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SearchEntryPoint) ProtoMessage() {}

func (x *SearchEntryPoint) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SearchEntryPoint.ProtoReflect.Descriptor instead.
func (*SearchEntryPoint) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{10}
}

func (x *SearchEntryPoint) GetRenderedContent() string {
	if x != nil {
		return x.RenderedContent
	}
	return ""
}

func (x *SearchEntryPoint) GetSdkBlob() []byte {
	if x != nil {
		return x.SdkBlob
	}
	return nil
}

// Grounding chunk.
type GroundingChunk struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Chunk type.
	//
	// Types that are valid to be assigned to ChunkType:
	//
	//	*GroundingChunk_Web_
	ChunkType     isGroundingChunk_ChunkType `protobuf_oneof:"chunk_type"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GroundingChunk) Reset() {
	*x = GroundingChunk{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[11]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GroundingChunk) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GroundingChunk) ProtoMessage() {}

func (x *GroundingChunk) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[11]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GroundingChunk.ProtoReflect.Descriptor instead.
func (*GroundingChunk) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{11}
}

func (x *GroundingChunk) GetChunkType() isGroundingChunk_ChunkType {
	if x != nil {
		return x.ChunkType
	}
	return nil
}

func (x *GroundingChunk) GetWeb() *GroundingChunk_Web {
	if x != nil {
		if x, ok := x.ChunkType.(*GroundingChunk_Web_); ok {
			return x.Web
		}
	}
	return nil
}

type isGroundingChunk_ChunkType interface {
	isGroundingChunk_ChunkType()
}

type GroundingChunk_Web_ struct {
	// Grounding chunk from the web.
	Web *GroundingChunk_Web `protobuf:"bytes,1,opt,name=web,proto3,oneof"`
}

func (*GroundingChunk_Web_) isGroundingChunk_ChunkType() {}

// Segment of the content.
type Segment struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. The index of a Part object within its parent Content object.
	PartIndex int32 `protobuf:"varint,1,opt,name=part_index,json=partIndex,proto3" json:"part_index,omitempty"`
	// Output only. Start index in the given Part, measured in bytes. Offset from
	// the start of the Part, inclusive, starting at zero.
	StartIndex int32 `protobuf:"varint,2,opt,name=start_index,json=startIndex,proto3" json:"start_index,omitempty"`
	// Output only. End index in the given Part, measured in bytes. Offset from
	// the start of the Part, exclusive, starting at zero.
	EndIndex int32 `protobuf:"varint,3,opt,name=end_index,json=endIndex,proto3" json:"end_index,omitempty"`
	// Output only. The text corresponding to the segment from the response.
	Text          string `protobuf:"bytes,4,opt,name=text,proto3" json:"text,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *Segment) Reset() {
	*x = Segment{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[12]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *Segment) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*Segment) ProtoMessage() {}

func (x *Segment) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[12]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use Segment.ProtoReflect.Descriptor instead.
func (*Segment) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{12}
}

func (x *Segment) GetPartIndex() int32 {
	if x != nil {
		return x.PartIndex
	}
	return 0
}

func (x *Segment) GetStartIndex() int32 {
	if x != nil {
		return x.StartIndex
	}
	return 0
}

func (x *Segment) GetEndIndex() int32 {
	if x != nil {
		return x.EndIndex
	}
	return 0
}

func (x *Segment) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

// Grounding support.
type GroundingSupport struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Segment of the content this support belongs to.
	Segment *Segment `protobuf:"bytes,1,opt,name=segment,proto3,oneof" json:"segment,omitempty"`
	// A list of indices (into 'grounding_chunk') specifying the
	// citations associated with the claim. For instance [1,3,4] means
	// that grounding_chunk[1], grounding_chunk[3],
	// grounding_chunk[4] are the retrieved content attributed to the claim.
	GroundingChunkIndices []int32 `protobuf:"varint,2,rep,packed,name=grounding_chunk_indices,json=groundingChunkIndices,proto3" json:"grounding_chunk_indices,omitempty"`
	// Confidence score of the support references. Ranges from 0 to 1. 1 is the
	// most confident. This list must have the same size as the
	// grounding_chunk_indices.
	ConfidenceScores []float32 `protobuf:"fixed32,3,rep,packed,name=confidence_scores,json=confidenceScores,proto3" json:"confidence_scores,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *GroundingSupport) Reset() {
	*x = GroundingSupport{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[13]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GroundingSupport) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GroundingSupport) ProtoMessage() {}

func (x *GroundingSupport) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[13]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GroundingSupport.ProtoReflect.Descriptor instead.
func (*GroundingSupport) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{13}
}

func (x *GroundingSupport) GetSegment() *Segment {
	if x != nil {
		return x.Segment
	}
	return nil
}

func (x *GroundingSupport) GetGroundingChunkIndices() []int32 {
	if x != nil {
		return x.GroundingChunkIndices
	}
	return nil
}

func (x *GroundingSupport) GetConfidenceScores() []float32 {
	if x != nil {
		return x.ConfidenceScores
	}
	return nil
}

// Request to generate a grounded answer from the `Model`.
type GenerateAnswerRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The sources in which to ground the answer.
	//
	// Types that are valid to be assigned to GroundingSource:
	//
	//	*GenerateAnswerRequest_InlinePassages
	//	*GenerateAnswerRequest_SemanticRetriever
	GroundingSource isGenerateAnswerRequest_GroundingSource `protobuf_oneof:"grounding_source"`
	// Required. The name of the `Model` to use for generating the grounded
	// response.
	//
	// Format: `model=models/{model}`.
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Required. The content of the current conversation with the `Model`. For
	// single-turn queries, this is a single question to answer. For multi-turn
	// queries, this is a repeated field that contains conversation history and
	// the last `Content` in the list containing the question.
	//
	// Note: `GenerateAnswer` only supports queries in English.
	Contents []*Content `protobuf:"bytes,2,rep,name=contents,proto3" json:"contents,omitempty"`
	// Required. Style in which answers should be returned.
	AnswerStyle GenerateAnswerRequest_AnswerStyle `protobuf:"varint,5,opt,name=answer_style,json=answerStyle,proto3,enum=qclaogui.generativelanguage.v1beta1.GenerateAnswerRequest_AnswerStyle" json:"answer_style,omitempty"`
	// Optional. A list of unique `SafetySetting` instances for blocking unsafe
	// content.
	//
	// This will be enforced on the `GenerateAnswerRequest.contents` and
	// `GenerateAnswerResponse.candidate`. There should not be more than one
	// setting for each `SafetyCategory` type. The API will block any contents and
	// responses that fail to meet the thresholds set by these settings. This list
	// overrides the default settings for each `SafetyCategory` specified in the
	// safety_settings. If there is no `SafetySetting` for a given
	// `SafetyCategory` provided in the list, the API will use the default safety
	// setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
	// HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
	// HARM_CATEGORY_HARASSMENT are supported.
	// Refer to the
	// [guide](https://ai.google.dev/gemini-api/docs/safety-settings)
	// for detailed information on available safety settings. Also refer to the
	// [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance) to
	// learn how to incorporate safety considerations in your AI applications.
	SafetySettings []*SafetySetting `protobuf:"bytes,3,rep,name=safety_settings,json=safetySettings,proto3" json:"safety_settings,omitempty"`
	// Optional. Controls the randomness of the output.
	//
	// Values can range from [0.0,1.0], inclusive. A value closer to 1.0 will
	// produce responses that are more varied and creative, while a value closer
	// to 0.0 will typically result in more straightforward responses from the
	// model. A low temperature (~0.2) is usually recommended for
	// Attributed-Question-Answering use cases.
	Temperature   *float32 `protobuf:"fixed32,4,opt,name=temperature,proto3,oneof" json:"temperature,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerateAnswerRequest) Reset() {
	*x = GenerateAnswerRequest{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[14]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateAnswerRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateAnswerRequest) ProtoMessage() {}

func (x *GenerateAnswerRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[14]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateAnswerRequest.ProtoReflect.Descriptor instead.
func (*GenerateAnswerRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{14}
}

func (x *GenerateAnswerRequest) GetGroundingSource() isGenerateAnswerRequest_GroundingSource {
	if x != nil {
		return x.GroundingSource
	}
	return nil
}

func (x *GenerateAnswerRequest) GetInlinePassages() *GroundingPassages {
	if x != nil {
		if x, ok := x.GroundingSource.(*GenerateAnswerRequest_InlinePassages); ok {
			return x.InlinePassages
		}
	}
	return nil
}

func (x *GenerateAnswerRequest) GetSemanticRetriever() *SemanticRetrieverConfig {
	if x != nil {
		if x, ok := x.GroundingSource.(*GenerateAnswerRequest_SemanticRetriever); ok {
			return x.SemanticRetriever
		}
	}
	return nil
}

func (x *GenerateAnswerRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *GenerateAnswerRequest) GetContents() []*Content {
	if x != nil {
		return x.Contents
	}
	return nil
}

func (x *GenerateAnswerRequest) GetAnswerStyle() GenerateAnswerRequest_AnswerStyle {
	if x != nil {
		return x.AnswerStyle
	}
	return GenerateAnswerRequest_ANSWER_STYLE_UNSPECIFIED
}

func (x *GenerateAnswerRequest) GetSafetySettings() []*SafetySetting {
	if x != nil {
		return x.SafetySettings
	}
	return nil
}

func (x *GenerateAnswerRequest) GetTemperature() float32 {
	if x != nil && x.Temperature != nil {
		return *x.Temperature
	}
	return 0
}

type isGenerateAnswerRequest_GroundingSource interface {
	isGenerateAnswerRequest_GroundingSource()
}

type GenerateAnswerRequest_InlinePassages struct {
	// Passages provided inline with the request.
	InlinePassages *GroundingPassages `protobuf:"bytes,6,opt,name=inline_passages,json=inlinePassages,proto3,oneof"`
}

type GenerateAnswerRequest_SemanticRetriever struct {
	// Content retrieved from resources created via the Semantic Retriever
	// API.
	SemanticRetriever *SemanticRetrieverConfig `protobuf:"bytes,7,opt,name=semantic_retriever,json=semanticRetriever,proto3,oneof"`
}

func (*GenerateAnswerRequest_InlinePassages) isGenerateAnswerRequest_GroundingSource() {}

func (*GenerateAnswerRequest_SemanticRetriever) isGenerateAnswerRequest_GroundingSource() {}

// Response from the model for a grounded answer.
type GenerateAnswerResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Candidate answer from the model.
	//
	// Note: The model *always* attempts to provide a grounded answer, even when
	// the answer is unlikely to be answerable from the given passages.
	// In that case, a low-quality or ungrounded answer may be provided, along
	// with a low `answerable_probability`.
	Answer *Candidate `protobuf:"bytes,1,opt,name=answer,proto3" json:"answer,omitempty"`
	// Output only. The model's estimate of the probability that its answer is
	// correct and grounded in the input passages.
	//
	// A low `answerable_probability` indicates that the answer might not be
	// grounded in the sources.
	//
	// When `answerable_probability` is low, you may want to:
	//
	// * Display a message to the effect of "We couldnt answer that question" to
	// the user.
	// * Fall back to a general-purpose LLM that answers the question from world
	// knowledge. The threshold and nature of such fallbacks will depend on
	// individual use cases. `0.5` is a good starting threshold.
	AnswerableProbability *float32 `protobuf:"fixed32,2,opt,name=answerable_probability,json=answerableProbability,proto3,oneof" json:"answerable_probability,omitempty"`
	// Output only. Feedback related to the input data used to answer the
	// question, as opposed to the model-generated response to the question.
	//
	// The input data can be one or more of the following:
	//
	// - Question specified by the last entry in `GenerateAnswerRequest.content`
	// - Conversation history specified by the other entries in
	// `GenerateAnswerRequest.content`
	// - Grounding sources (`GenerateAnswerRequest.semantic_retriever` or
	// `GenerateAnswerRequest.inline_passages`)
	InputFeedback *GenerateAnswerResponse_InputFeedback `protobuf:"bytes,3,opt,name=input_feedback,json=inputFeedback,proto3,oneof" json:"input_feedback,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerateAnswerResponse) Reset() {
	*x = GenerateAnswerResponse{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[15]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateAnswerResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateAnswerResponse) ProtoMessage() {}

func (x *GenerateAnswerResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[15]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateAnswerResponse.ProtoReflect.Descriptor instead.
func (*GenerateAnswerResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{15}
}

func (x *GenerateAnswerResponse) GetAnswer() *Candidate {
	if x != nil {
		return x.Answer
	}
	return nil
}

func (x *GenerateAnswerResponse) GetAnswerableProbability() float32 {
	if x != nil && x.AnswerableProbability != nil {
		return *x.AnswerableProbability
	}
	return 0
}

func (x *GenerateAnswerResponse) GetInputFeedback() *GenerateAnswerResponse_InputFeedback {
	if x != nil {
		return x.InputFeedback
	}
	return nil
}

// Request containing the `Content` for the model to embed.
type EmbedContentRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The model's resource name. This serves as an ID for the Model to
	// use.
	//
	// This name should match a model name returned by the `ListModels` method.
	//
	// Format: `models/{model}`
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Required. The content to embed. Only the `parts.text` fields will be
	// counted.
	Content *Content `protobuf:"bytes,2,opt,name=content,proto3" json:"content,omitempty"`
	// Optional. Optional task type for which the embeddings will be used. Can
	// only be set for `models/embedding-001`.
	TaskType *TaskType `protobuf:"varint,3,opt,name=task_type,json=taskType,proto3,enum=qclaogui.generativelanguage.v1beta1.TaskType,oneof" json:"task_type,omitempty"`
	// Optional. An optional title for the text. Only applicable when TaskType is
	// `RETRIEVAL_DOCUMENT`.
	//
	// Note: Specifying a `title` for `RETRIEVAL_DOCUMENT` provides better quality
	// embeddings for retrieval.
	Title *string `protobuf:"bytes,4,opt,name=title,proto3,oneof" json:"title,omitempty"`
	// Optional. Optional reduced dimension for the output embedding. If set,
	// excessive values in the output embedding are truncated from the end.
	// Supported by newer models since 2024 only. You cannot set this value if
	// using the earlier model (`models/embedding-001`).
	OutputDimensionality *int32 `protobuf:"varint,5,opt,name=output_dimensionality,json=outputDimensionality,proto3,oneof" json:"output_dimensionality,omitempty"`
	unknownFields        protoimpl.UnknownFields
	sizeCache            protoimpl.SizeCache
}

func (x *EmbedContentRequest) Reset() {
	*x = EmbedContentRequest{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[16]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *EmbedContentRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*EmbedContentRequest) ProtoMessage() {}

func (x *EmbedContentRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[16]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use EmbedContentRequest.ProtoReflect.Descriptor instead.
func (*EmbedContentRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{16}
}

func (x *EmbedContentRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *EmbedContentRequest) GetContent() *Content {
	if x != nil {
		return x.Content
	}
	return nil
}

func (x *EmbedContentRequest) GetTaskType() TaskType {
	if x != nil && x.TaskType != nil {
		return *x.TaskType
	}
	return TaskType_TASK_TYPE_UNSPECIFIED
}

func (x *EmbedContentRequest) GetTitle() string {
	if x != nil && x.Title != nil {
		return *x.Title
	}
	return ""
}

func (x *EmbedContentRequest) GetOutputDimensionality() int32 {
	if x != nil && x.OutputDimensionality != nil {
		return *x.OutputDimensionality
	}
	return 0
}

// A list of floats representing an embedding.
type ContentEmbedding struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The embedding values.
	Values        []float32 `protobuf:"fixed32,1,rep,packed,name=values,proto3" json:"values,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ContentEmbedding) Reset() {
	*x = ContentEmbedding{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[17]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ContentEmbedding) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ContentEmbedding) ProtoMessage() {}

func (x *ContentEmbedding) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[17]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ContentEmbedding.ProtoReflect.Descriptor instead.
func (*ContentEmbedding) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{17}
}

func (x *ContentEmbedding) GetValues() []float32 {
	if x != nil {
		return x.Values
	}
	return nil
}

// The response to an `EmbedContentRequest`.
type EmbedContentResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. The embedding generated from the input content.
	Embedding     *ContentEmbedding `protobuf:"bytes,1,opt,name=embedding,proto3" json:"embedding,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *EmbedContentResponse) Reset() {
	*x = EmbedContentResponse{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[18]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *EmbedContentResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*EmbedContentResponse) ProtoMessage() {}

func (x *EmbedContentResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[18]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use EmbedContentResponse.ProtoReflect.Descriptor instead.
func (*EmbedContentResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{18}
}

func (x *EmbedContentResponse) GetEmbedding() *ContentEmbedding {
	if x != nil {
		return x.Embedding
	}
	return nil
}

// Batch request to get embeddings from the model for a list of prompts.
type BatchEmbedContentsRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The model's resource name. This serves as an ID for the Model to
	// use.
	//
	// This name should match a model name returned by the `ListModels` method.
	//
	// Format: `models/{model}`
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Required. Embed requests for the batch. The model in each of these requests
	// must match the model specified `BatchEmbedContentsRequest.model`.
	Requests      []*EmbedContentRequest `protobuf:"bytes,2,rep,name=requests,proto3" json:"requests,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *BatchEmbedContentsRequest) Reset() {
	*x = BatchEmbedContentsRequest{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[19]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BatchEmbedContentsRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BatchEmbedContentsRequest) ProtoMessage() {}

func (x *BatchEmbedContentsRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[19]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BatchEmbedContentsRequest.ProtoReflect.Descriptor instead.
func (*BatchEmbedContentsRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{19}
}

func (x *BatchEmbedContentsRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *BatchEmbedContentsRequest) GetRequests() []*EmbedContentRequest {
	if x != nil {
		return x.Requests
	}
	return nil
}

// The response to a `BatchEmbedContentsRequest`.
type BatchEmbedContentsResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. The embeddings for each request, in the same order as provided
	// in the batch request.
	Embeddings    []*ContentEmbedding `protobuf:"bytes,1,rep,name=embeddings,proto3" json:"embeddings,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *BatchEmbedContentsResponse) Reset() {
	*x = BatchEmbedContentsResponse{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[20]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *BatchEmbedContentsResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*BatchEmbedContentsResponse) ProtoMessage() {}

func (x *BatchEmbedContentsResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[20]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use BatchEmbedContentsResponse.ProtoReflect.Descriptor instead.
func (*BatchEmbedContentsResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{20}
}

func (x *BatchEmbedContentsResponse) GetEmbeddings() []*ContentEmbedding {
	if x != nil {
		return x.Embeddings
	}
	return nil
}

// Counts the number of tokens in the `prompt` sent to a model.
//
// Models may tokenize text differently, so each model may return a different
// `token_count`.
type CountTokensRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Required. The model's resource name. This serves as an ID for the Model to
	// use.
	//
	// This name should match a model name returned by the `ListModels` method.
	//
	// Format: `models/{model}`
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Optional. The input given to the model as a prompt. This field is ignored
	// when `generate_content_request` is set.
	Contents []*Content `protobuf:"bytes,2,rep,name=contents,proto3" json:"contents,omitempty"`
	// Optional. The overall input given to the `Model`. This includes the prompt
	// as well as other model steering information like [system
	// instructions](https://ai.google.dev/gemini-api/docs/system-instructions),
	// and/or function declarations for [function
	// calling](https://ai.google.dev/gemini-api/docs/function-calling).
	// `Model`s/`Content`s and `generate_content_request`s are mutually
	// exclusive. You can either send `Model` + `Content`s or a
	// `generate_content_request`, but never both.
	GenerateContentRequest *GenerateContentRequest `protobuf:"bytes,3,opt,name=generate_content_request,json=generateContentRequest,proto3" json:"generate_content_request,omitempty"`
	unknownFields          protoimpl.UnknownFields
	sizeCache              protoimpl.SizeCache
}

func (x *CountTokensRequest) Reset() {
	*x = CountTokensRequest{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[21]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CountTokensRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CountTokensRequest) ProtoMessage() {}

func (x *CountTokensRequest) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[21]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CountTokensRequest.ProtoReflect.Descriptor instead.
func (*CountTokensRequest) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{21}
}

func (x *CountTokensRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *CountTokensRequest) GetContents() []*Content {
	if x != nil {
		return x.Contents
	}
	return nil
}

func (x *CountTokensRequest) GetGenerateContentRequest() *GenerateContentRequest {
	if x != nil {
		return x.GenerateContentRequest
	}
	return nil
}

// A response from `CountTokens`.
//
// It returns the model's `token_count` for the `prompt`.
type CountTokensResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The number of tokens that the `Model` tokenizes the `prompt` into. Always
	// non-negative.
	TotalTokens int32 `protobuf:"varint,1,opt,name=total_tokens,json=totalTokens,proto3" json:"total_tokens,omitempty"`
	// Number of tokens in the cached part of the prompt (the cached content).
	CachedContentTokenCount int32 `protobuf:"varint,5,opt,name=cached_content_token_count,json=cachedContentTokenCount,proto3" json:"cached_content_token_count,omitempty"`
	unknownFields           protoimpl.UnknownFields
	sizeCache               protoimpl.SizeCache
}

func (x *CountTokensResponse) Reset() {
	*x = CountTokensResponse{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[22]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CountTokensResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CountTokensResponse) ProtoMessage() {}

func (x *CountTokensResponse) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[22]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CountTokensResponse.ProtoReflect.Descriptor instead.
func (*CountTokensResponse) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{22}
}

func (x *CountTokensResponse) GetTotalTokens() int32 {
	if x != nil {
		return x.TotalTokens
	}
	return 0
}

func (x *CountTokensResponse) GetCachedContentTokenCount() int32 {
	if x != nil {
		return x.CachedContentTokenCount
	}
	return 0
}

// A set of the feedback metadata the prompt specified in
// `GenerateContentRequest.content`.
type GenerateContentResponse_PromptFeedback struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. If set, the prompt was blocked and no candidates are returned.
	// Rephrase the prompt.
	BlockReason GenerateContentResponse_PromptFeedback_BlockReason `protobuf:"varint,1,opt,name=block_reason,json=blockReason,proto3,enum=qclaogui.generativelanguage.v1beta1.GenerateContentResponse_PromptFeedback_BlockReason" json:"block_reason,omitempty"`
	// Ratings for safety of the prompt.
	// There is at most one rating per category.
	SafetyRatings []*SafetyRating `protobuf:"bytes,2,rep,name=safety_ratings,json=safetyRatings,proto3" json:"safety_ratings,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerateContentResponse_PromptFeedback) Reset() {
	*x = GenerateContentResponse_PromptFeedback{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[23]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateContentResponse_PromptFeedback) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateContentResponse_PromptFeedback) ProtoMessage() {}

func (x *GenerateContentResponse_PromptFeedback) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[23]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateContentResponse_PromptFeedback.ProtoReflect.Descriptor instead.
func (*GenerateContentResponse_PromptFeedback) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{3, 0}
}

func (x *GenerateContentResponse_PromptFeedback) GetBlockReason() GenerateContentResponse_PromptFeedback_BlockReason {
	if x != nil {
		return x.BlockReason
	}
	return GenerateContentResponse_PromptFeedback_BLOCK_REASON_UNSPECIFIED
}

func (x *GenerateContentResponse_PromptFeedback) GetSafetyRatings() []*SafetyRating {
	if x != nil {
		return x.SafetyRatings
	}
	return nil
}

// Metadata on the generation request's token usage.
type GenerateContentResponse_UsageMetadata struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Number of tokens in the prompt. When `cached_content` is set, this is
	// still the total effective prompt size meaning this includes the number of
	// tokens in the cached content.
	PromptTokenCount int32 `protobuf:"varint,1,opt,name=prompt_token_count,json=promptTokenCount,proto3" json:"prompt_token_count,omitempty"`
	// Number of tokens in the cached part of the prompt (the cached content)
	CachedContentTokenCount int32 `protobuf:"varint,4,opt,name=cached_content_token_count,json=cachedContentTokenCount,proto3" json:"cached_content_token_count,omitempty"`
	// Total number of tokens across all the generated response candidates.
	CandidatesTokenCount int32 `protobuf:"varint,2,opt,name=candidates_token_count,json=candidatesTokenCount,proto3" json:"candidates_token_count,omitempty"`
	// Total token count for the generation request (prompt + response
	// candidates).
	TotalTokenCount int32 `protobuf:"varint,3,opt,name=total_token_count,json=totalTokenCount,proto3" json:"total_token_count,omitempty"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *GenerateContentResponse_UsageMetadata) Reset() {
	*x = GenerateContentResponse_UsageMetadata{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[24]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateContentResponse_UsageMetadata) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateContentResponse_UsageMetadata) ProtoMessage() {}

func (x *GenerateContentResponse_UsageMetadata) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[24]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateContentResponse_UsageMetadata.ProtoReflect.Descriptor instead.
func (*GenerateContentResponse_UsageMetadata) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{3, 1}
}

func (x *GenerateContentResponse_UsageMetadata) GetPromptTokenCount() int32 {
	if x != nil {
		return x.PromptTokenCount
	}
	return 0
}

func (x *GenerateContentResponse_UsageMetadata) GetCachedContentTokenCount() int32 {
	if x != nil {
		return x.CachedContentTokenCount
	}
	return 0
}

func (x *GenerateContentResponse_UsageMetadata) GetCandidatesTokenCount() int32 {
	if x != nil {
		return x.CandidatesTokenCount
	}
	return 0
}

func (x *GenerateContentResponse_UsageMetadata) GetTotalTokenCount() int32 {
	if x != nil {
		return x.TotalTokenCount
	}
	return 0
}

// Candidate for the logprobs token and score.
type LogprobsResult_Candidate struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The candidates token string value.
	Token *string `protobuf:"bytes,1,opt,name=token,proto3,oneof" json:"token,omitempty"`
	// The candidates token id value.
	TokenId *int32 `protobuf:"varint,3,opt,name=token_id,json=tokenId,proto3,oneof" json:"token_id,omitempty"`
	// The candidate's log probability.
	LogProbability *float32 `protobuf:"fixed32,2,opt,name=log_probability,json=logProbability,proto3,oneof" json:"log_probability,omitempty"`
	unknownFields  protoimpl.UnknownFields
	sizeCache      protoimpl.SizeCache
}

func (x *LogprobsResult_Candidate) Reset() {
	*x = LogprobsResult_Candidate{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[25]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LogprobsResult_Candidate) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LogprobsResult_Candidate) ProtoMessage() {}

func (x *LogprobsResult_Candidate) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[25]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LogprobsResult_Candidate.ProtoReflect.Descriptor instead.
func (*LogprobsResult_Candidate) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{5, 0}
}

func (x *LogprobsResult_Candidate) GetToken() string {
	if x != nil && x.Token != nil {
		return *x.Token
	}
	return ""
}

func (x *LogprobsResult_Candidate) GetTokenId() int32 {
	if x != nil && x.TokenId != nil {
		return *x.TokenId
	}
	return 0
}

func (x *LogprobsResult_Candidate) GetLogProbability() float32 {
	if x != nil && x.LogProbability != nil {
		return *x.LogProbability
	}
	return 0
}

// Candidates with top log probabilities at each decoding step.
type LogprobsResult_TopCandidates struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Sorted by log probability in descending order.
	Candidates    []*LogprobsResult_Candidate `protobuf:"bytes,1,rep,name=candidates,proto3" json:"candidates,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *LogprobsResult_TopCandidates) Reset() {
	*x = LogprobsResult_TopCandidates{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[26]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *LogprobsResult_TopCandidates) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*LogprobsResult_TopCandidates) ProtoMessage() {}

func (x *LogprobsResult_TopCandidates) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[26]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use LogprobsResult_TopCandidates.ProtoReflect.Descriptor instead.
func (*LogprobsResult_TopCandidates) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{5, 1}
}

func (x *LogprobsResult_TopCandidates) GetCandidates() []*LogprobsResult_Candidate {
	if x != nil {
		return x.Candidates
	}
	return nil
}

// Identifier for a part within a `GroundingPassage`.
type AttributionSourceId_GroundingPassageId struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. ID of the passage matching the `GenerateAnswerRequest`'s
	// `GroundingPassage.id`.
	PassageId string `protobuf:"bytes,1,opt,name=passage_id,json=passageId,proto3" json:"passage_id,omitempty"`
	// Output only. Index of the part within the `GenerateAnswerRequest`'s
	// `GroundingPassage.content`.
	PartIndex     int32 `protobuf:"varint,2,opt,name=part_index,json=partIndex,proto3" json:"part_index,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AttributionSourceId_GroundingPassageId) Reset() {
	*x = AttributionSourceId_GroundingPassageId{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[27]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AttributionSourceId_GroundingPassageId) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AttributionSourceId_GroundingPassageId) ProtoMessage() {}

func (x *AttributionSourceId_GroundingPassageId) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[27]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AttributionSourceId_GroundingPassageId.ProtoReflect.Descriptor instead.
func (*AttributionSourceId_GroundingPassageId) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{6, 0}
}

func (x *AttributionSourceId_GroundingPassageId) GetPassageId() string {
	if x != nil {
		return x.PassageId
	}
	return ""
}

func (x *AttributionSourceId_GroundingPassageId) GetPartIndex() int32 {
	if x != nil {
		return x.PartIndex
	}
	return 0
}

// Identifier for a `Chunk` retrieved via Semantic Retriever specified in the
// `GenerateAnswerRequest` using `SemanticRetrieverConfig`.
type AttributionSourceId_SemanticRetrieverChunk struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Output only. Name of the source matching the request's
	// `SemanticRetrieverConfig.source`. Example: `corpora/123` or
	// `corpora/123/documents/abc`
	Source string `protobuf:"bytes,1,opt,name=source,proto3" json:"source,omitempty"`
	// Output only. Name of the `Chunk` containing the attributed text.
	// Example: `corpora/123/documents/abc/chunks/xyz`
	Chunk         string `protobuf:"bytes,2,opt,name=chunk,proto3" json:"chunk,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *AttributionSourceId_SemanticRetrieverChunk) Reset() {
	*x = AttributionSourceId_SemanticRetrieverChunk{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[28]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *AttributionSourceId_SemanticRetrieverChunk) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*AttributionSourceId_SemanticRetrieverChunk) ProtoMessage() {}

func (x *AttributionSourceId_SemanticRetrieverChunk) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[28]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use AttributionSourceId_SemanticRetrieverChunk.ProtoReflect.Descriptor instead.
func (*AttributionSourceId_SemanticRetrieverChunk) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{6, 1}
}

func (x *AttributionSourceId_SemanticRetrieverChunk) GetSource() string {
	if x != nil {
		return x.Source
	}
	return ""
}

func (x *AttributionSourceId_SemanticRetrieverChunk) GetChunk() string {
	if x != nil {
		return x.Chunk
	}
	return ""
}

// Chunk from the web.
type GroundingChunk_Web struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// URI reference of the chunk.
	Uri *string `protobuf:"bytes,1,opt,name=uri,proto3,oneof" json:"uri,omitempty"`
	// Title of the chunk.
	Title         *string `protobuf:"bytes,2,opt,name=title,proto3,oneof" json:"title,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GroundingChunk_Web) Reset() {
	*x = GroundingChunk_Web{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[29]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GroundingChunk_Web) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GroundingChunk_Web) ProtoMessage() {}

func (x *GroundingChunk_Web) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[29]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GroundingChunk_Web.ProtoReflect.Descriptor instead.
func (*GroundingChunk_Web) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{11, 0}
}

func (x *GroundingChunk_Web) GetUri() string {
	if x != nil && x.Uri != nil {
		return *x.Uri
	}
	return ""
}

func (x *GroundingChunk_Web) GetTitle() string {
	if x != nil && x.Title != nil {
		return *x.Title
	}
	return ""
}

// Feedback related to the input data used to answer the question, as opposed
// to the model-generated response to the question.
type GenerateAnswerResponse_InputFeedback struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Optional. If set, the input was blocked and no candidates are returned.
	// Rephrase the input.
	BlockReason *GenerateAnswerResponse_InputFeedback_BlockReason `protobuf:"varint,1,opt,name=block_reason,json=blockReason,proto3,enum=qclaogui.generativelanguage.v1beta1.GenerateAnswerResponse_InputFeedback_BlockReason,oneof" json:"block_reason,omitempty"`
	// Ratings for safety of the input.
	// There is at most one rating per category.
	SafetyRatings []*SafetyRating `protobuf:"bytes,2,rep,name=safety_ratings,json=safetyRatings,proto3" json:"safety_ratings,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GenerateAnswerResponse_InputFeedback) Reset() {
	*x = GenerateAnswerResponse_InputFeedback{}
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[30]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GenerateAnswerResponse_InputFeedback) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GenerateAnswerResponse_InputFeedback) ProtoMessage() {}

func (x *GenerateAnswerResponse_InputFeedback) ProtoReflect() protoreflect.Message {
	mi := &file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[30]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GenerateAnswerResponse_InputFeedback.ProtoReflect.Descriptor instead.
func (*GenerateAnswerResponse_InputFeedback) Descriptor() ([]byte, []int) {
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP(), []int{15, 0}
}

func (x *GenerateAnswerResponse_InputFeedback) GetBlockReason() GenerateAnswerResponse_InputFeedback_BlockReason {
	if x != nil && x.BlockReason != nil {
		return *x.BlockReason
	}
	return GenerateAnswerResponse_InputFeedback_BLOCK_REASON_UNSPECIFIED
}

func (x *GenerateAnswerResponse_InputFeedback) GetSafetyRatings() []*SafetyRating {
	if x != nil {
		return x.SafetyRatings
	}
	return nil
}

var File_qclaogui_generativelanguage_v1beta1_generative_service_proto protoreflect.FileDescriptor

const file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDesc = "" +
	"\n" +
	"<qclaogui/generativelanguage/v1beta1/generative_service.proto\x12#qclaogui.generativelanguage.v1beta1\x1a\x1cgoogle/api/annotations.proto\x1a\x17google/api/client.proto\x1a\x1fgoogle/api/field_behavior.proto\x1a\x19google/api/resource.proto\x1a2qclaogui/generativelanguage/v1beta1/citation.proto\x1a1qclaogui/generativelanguage/v1beta1/content.proto\x1a3qclaogui/generativelanguage/v1beta1/retriever.proto\x1a0qclaogui/generativelanguage/v1beta1/safety.proto\"\xab\x06\n" +
	"\x16GenerateContentRequest\x12D\n" +
	"\x05model\x18\x01 \x01(\tB.\xe2A\x01\x02\xfaA'\n" +
	"%generativelanguage.qclaogui.com/ModelR\x05model\x12f\n" +
	"\x12system_instruction\x18\b \x01(\v2,.qclaogui.generativelanguage.v1beta1.ContentB\x04\xe2A\x01\x01H\x00R\x11systemInstruction\x88\x01\x01\x12N\n" +
	"\bcontents\x18\x02 \x03(\v2,.qclaogui.generativelanguage.v1beta1.ContentB\x04\xe2A\x01\x02R\bcontents\x12E\n" +
	"\x05tools\x18\x05 \x03(\v2).qclaogui.generativelanguage.v1beta1.ToolB\x04\xe2A\x01\x01R\x05tools\x12V\n" +
	"\vtool_config\x18\a \x01(\v2/.qclaogui.generativelanguage.v1beta1.ToolConfigB\x04\xe2A\x01\x01R\n" +
	"toolConfig\x12a\n" +
	"\x0fsafety_settings\x18\x03 \x03(\v22.qclaogui.generativelanguage.v1beta1.SafetySettingB\x04\xe2A\x01\x01R\x0esafetySettings\x12m\n" +
	"\x11generation_config\x18\x04 \x01(\v25.qclaogui.generativelanguage.v1beta1.GenerationConfigB\x04\xe2A\x01\x01H\x01R\x10generationConfig\x88\x01\x01\x12b\n" +
	"\x0ecached_content\x18\t \x01(\tB6\xe2A\x01\x01\xfaA/\n" +
	"-generativelanguage.qclaogui.com/CachedContentH\x02R\rcachedContent\x88\x01\x01B\x15\n" +
	"\x13_system_instructionB\x14\n" +
	"\x12_generation_configB\x11\n" +
	"\x0f_cached_content\"\x90\x06\n" +
	"\x10GenerationConfig\x122\n" +
	"\x0fcandidate_count\x18\x01 \x01(\x05B\x04\xe2A\x01\x01H\x00R\x0ecandidateCount\x88\x01\x01\x12+\n" +
	"\x0estop_sequences\x18\x02 \x03(\tB\x04\xe2A\x01\x01R\rstopSequences\x125\n" +
	"\x11max_output_tokens\x18\x04 \x01(\x05B\x04\xe2A\x01\x01H\x01R\x0fmaxOutputTokens\x88\x01\x01\x12+\n" +
	"\vtemperature\x18\x05 \x01(\x02B\x04\xe2A\x01\x01H\x02R\vtemperature\x88\x01\x01\x12\x1e\n" +
	"\x05top_p\x18\x06 \x01(\x02B\x04\xe2A\x01\x01H\x03R\x04topP\x88\x01\x01\x12\x1e\n" +
	"\x05top_k\x18\a \x01(\x05B\x04\xe2A\x01\x01H\x04R\x04topK\x88\x01\x01\x122\n" +
	"\x12response_mime_type\x18\r \x01(\tB\x04\xe2A\x01\x01R\x10responseMimeType\x12Z\n" +
	"\x0fresponse_schema\x18\x0e \x01(\v2+.qclaogui.generativelanguage.v1beta1.SchemaB\x04\xe2A\x01\x01R\x0eresponseSchema\x124\n" +
	"\x10presence_penalty\x18\x0f \x01(\x02B\x04\xe2A\x01\x01H\x05R\x0fpresencePenalty\x88\x01\x01\x126\n" +
	"\x11frequency_penalty\x18\x10 \x01(\x02B\x04\xe2A\x01\x01H\x06R\x10frequencyPenalty\x88\x01\x01\x126\n" +
	"\x11response_logprobs\x18\x11 \x01(\bB\x04\xe2A\x01\x01H\aR\x10responseLogprobs\x88\x01\x01\x12%\n" +
	"\blogprobs\x18\x12 \x01(\x05B\x04\xe2A\x01\x01H\bR\blogprobs\x88\x01\x01B\x12\n" +
	"\x10_candidate_countB\x14\n" +
	"\x12_max_output_tokensB\x0e\n" +
	"\f_temperatureB\b\n" +
	"\x06_top_pB\b\n" +
	"\x06_top_kB\x13\n" +
	"\x11_presence_penaltyB\x14\n" +
	"\x12_frequency_penaltyB\x14\n" +
	"\x12_response_logprobsB\v\n" +
	"\t_logprobs\"\x90\x03\n" +
	"\x17SemanticRetrieverConfig\x12\x1c\n" +
	"\x06source\x18\x01 \x01(\tB\x04\xe2A\x01\x02R\x06source\x12H\n" +
	"\x05query\x18\x02 \x01(\v2,.qclaogui.generativelanguage.v1beta1.ContentB\x04\xe2A\x01\x02R\x05query\x12d\n" +
	"\x10metadata_filters\x18\x03 \x03(\v23.qclaogui.generativelanguage.v1beta1.MetadataFilterB\x04\xe2A\x01\x01R\x0fmetadataFilters\x123\n" +
	"\x10max_chunks_count\x18\x04 \x01(\x05B\x04\xe2A\x01\x01H\x00R\x0emaxChunksCount\x88\x01\x01\x12A\n" +
	"\x17minimum_relevance_score\x18\x05 \x01(\x02B\x04\xe2A\x01\x01H\x01R\x15minimumRelevanceScore\x88\x01\x01B\x13\n" +
	"\x11_max_chunks_countB\x1a\n" +
	"\x18_minimum_relevance_score\"\x92\a\n" +
	"\x17GenerateContentResponse\x12N\n" +
	"\n" +
	"candidates\x18\x01 \x03(\v2..qclaogui.generativelanguage.v1beta1.CandidateR\n" +
	"candidates\x12t\n" +
	"\x0fprompt_feedback\x18\x02 \x01(\v2K.qclaogui.generativelanguage.v1beta1.GenerateContentResponse.PromptFeedbackR\x0epromptFeedback\x12w\n" +
	"\x0eusage_metadata\x18\x03 \x01(\v2J.qclaogui.generativelanguage.v1beta1.GenerateContentResponse.UsageMetadataB\x04\xe2A\x01\x03R\rusageMetadata\x1a\xd8\x02\n" +
	"\x0ePromptFeedback\x12\x80\x01\n" +
	"\fblock_reason\x18\x01 \x01(\x0e2W.qclaogui.generativelanguage.v1beta1.GenerateContentResponse.PromptFeedback.BlockReasonB\x04\xe2A\x01\x01R\vblockReason\x12X\n" +
	"\x0esafety_ratings\x18\x02 \x03(\v21.qclaogui.generativelanguage.v1beta1.SafetyRatingR\rsafetyRatings\"i\n" +
	"\vBlockReason\x12\x1c\n" +
	"\x18BLOCK_REASON_UNSPECIFIED\x10\x00\x12\n" +
	"\n" +
	"\x06SAFETY\x10\x01\x12\t\n" +
	"\x05OTHER\x10\x02\x12\r\n" +
	"\tBLOCKLIST\x10\x03\x12\x16\n" +
	"\x12PROHIBITED_CONTENT\x10\x04\x1a\xdc\x01\n" +
	"\rUsageMetadata\x12,\n" +
	"\x12prompt_token_count\x18\x01 \x01(\x05R\x10promptTokenCount\x12;\n" +
	"\x1acached_content_token_count\x18\x04 \x01(\x05R\x17cachedContentTokenCount\x124\n" +
	"\x16candidates_token_count\x18\x02 \x01(\x05R\x14candidatesTokenCount\x12*\n" +
	"\x11total_token_count\x18\x03 \x01(\x05R\x0ftotalTokenCount\"\x97\b\n" +
	"\tCandidate\x12\x1f\n" +
	"\x05index\x18\x03 \x01(\x05B\x04\xe2A\x01\x03H\x00R\x05index\x88\x01\x01\x12L\n" +
	"\acontent\x18\x01 \x01(\v2,.qclaogui.generativelanguage.v1beta1.ContentB\x04\xe2A\x01\x03R\acontent\x12g\n" +
	"\rfinish_reason\x18\x02 \x01(\x0e2;.qclaogui.generativelanguage.v1beta1.Candidate.FinishReasonB\x05\xe2A\x02\x01\x03R\ffinishReason\x12X\n" +
	"\x0esafety_ratings\x18\x05 \x03(\v21.qclaogui.generativelanguage.v1beta1.SafetyRatingR\rsafetyRatings\x12h\n" +
	"\x11citation_metadata\x18\x06 \x01(\v25.qclaogui.generativelanguage.v1beta1.CitationMetadataB\x04\xe2A\x01\x03R\x10citationMetadata\x12%\n" +
	"\vtoken_count\x18\a \x01(\x05B\x04\xe2A\x01\x03R\n" +
	"tokenCount\x12v\n" +
	"\x16grounding_attributions\x18\b \x03(\v29.qclaogui.generativelanguage.v1beta1.GroundingAttributionB\x04\xe2A\x01\x03R\x15groundingAttributions\x12k\n" +
	"\x12grounding_metadata\x18\t \x01(\v26.qclaogui.generativelanguage.v1beta1.GroundingMetadataB\x04\xe2A\x01\x03R\x11groundingMetadata\x12'\n" +
	"\favg_logprobs\x18\n" +
	" \x01(\x01B\x04\xe2A\x01\x03R\vavgLogprobs\x12b\n" +
	"\x0flogprobs_result\x18\v \x01(\v23.qclaogui.generativelanguage.v1beta1.LogprobsResultB\x04\xe2A\x01\x03R\x0elogprobsResult\"\xca\x01\n" +
	"\fFinishReason\x12\x1d\n" +
	"\x19FINISH_REASON_UNSPECIFIED\x10\x00\x12\b\n" +
	"\x04STOP\x10\x01\x12\x0e\n" +
	"\n" +
	"MAX_TOKENS\x10\x02\x12\n" +
	"\n" +
	"\x06SAFETY\x10\x03\x12\x0e\n" +
	"\n" +
	"RECITATION\x10\x04\x12\f\n" +
	"\bLANGUAGE\x10\x06\x12\t\n" +
	"\x05OTHER\x10\x05\x12\r\n" +
	"\tBLOCKLIST\x10\a\x12\x16\n" +
	"\x12PROHIBITED_CONTENT\x10\b\x12\b\n" +
	"\x04SPII\x10\t\x12\x1b\n" +
	"\x17MALFORMED_FUNCTION_CALL\x10\n" +
	"B\b\n" +
	"\x06_index\"\xf8\x03\n" +
	"\x0eLogprobsResult\x12h\n" +
	"\x0etop_candidates\x18\x01 \x03(\v2A.qclaogui.generativelanguage.v1beta1.LogprobsResult.TopCandidatesR\rtopCandidates\x12j\n" +
	"\x11chosen_candidates\x18\x02 \x03(\v2=.qclaogui.generativelanguage.v1beta1.LogprobsResult.CandidateR\x10chosenCandidates\x1a\x9f\x01\n" +
	"\tCandidate\x12\x19\n" +
	"\x05token\x18\x01 \x01(\tH\x00R\x05token\x88\x01\x01\x12\x1e\n" +
	"\btoken_id\x18\x03 \x01(\x05H\x01R\atokenId\x88\x01\x01\x12,\n" +
	"\x0flog_probability\x18\x02 \x01(\x02H\x02R\x0elogProbability\x88\x01\x01B\b\n" +
	"\x06_tokenB\v\n" +
	"\t_token_idB\x12\n" +
	"\x10_log_probability\x1an\n" +
	"\rTopCandidates\x12]\n" +
	"\n" +
	"candidates\x18\x01 \x03(\v2=.qclaogui.generativelanguage.v1beta1.LogprobsResult.CandidateR\n" +
	"candidates\"\xdd\x03\n" +
	"\x13AttributionSourceId\x12z\n" +
	"\x11grounding_passage\x18\x01 \x01(\v2K.qclaogui.generativelanguage.v1beta1.AttributionSourceId.GroundingPassageIdH\x00R\x10groundingPassage\x12\x8b\x01\n" +
	"\x18semantic_retriever_chunk\x18\x02 \x01(\v2O.qclaogui.generativelanguage.v1beta1.AttributionSourceId.SemanticRetrieverChunkH\x00R\x16semanticRetrieverChunk\x1a^\n" +
	"\x12GroundingPassageId\x12#\n" +
	"\n" +
	"passage_id\x18\x01 \x01(\tB\x04\xe2A\x01\x03R\tpassageId\x12#\n" +
	"\n" +
	"part_index\x18\x02 \x01(\x05B\x04\xe2A\x01\x03R\tpartIndex\x1aR\n" +
	"\x16SemanticRetrieverChunk\x12\x1c\n" +
	"\x06source\x18\x01 \x01(\tB\x04\xe2A\x01\x03R\x06source\x12\x1a\n" +
	"\x05chunk\x18\x02 \x01(\tB\x04\xe2A\x01\x03R\x05chunkB\b\n" +
	"\x06source\"\xbb\x01\n" +
	"\x14GroundingAttribution\x12[\n" +
	"\tsource_id\x18\x03 \x01(\v28.qclaogui.generativelanguage.v1beta1.AttributionSourceIdB\x04\xe2A\x01\x03R\bsourceId\x12F\n" +
	"\acontent\x18\x02 \x01(\v2,.qclaogui.generativelanguage.v1beta1.ContentR\acontent\"k\n" +
	"\x11RetrievalMetadata\x12V\n" +
	"%google_search_dynamic_retrieval_score\x18\x02 \x01(\x02B\x04\xe2A\x01\x01R!googleSearchDynamicRetrievalScore\"\xe3\x03\n" +
	"\x11GroundingMetadata\x12n\n" +
	"\x12search_entry_point\x18\x01 \x01(\v25.qclaogui.generativelanguage.v1beta1.SearchEntryPointB\x04\xe2A\x01\x01H\x00R\x10searchEntryPoint\x88\x01\x01\x12^\n" +
	"\x10grounding_chunks\x18\x02 \x03(\v23.qclaogui.generativelanguage.v1beta1.GroundingChunkR\x0fgroundingChunks\x12d\n" +
	"\x12grounding_supports\x18\x03 \x03(\v25.qclaogui.generativelanguage.v1beta1.GroundingSupportR\x11groundingSupports\x12j\n" +
	"\x12retrieval_metadata\x18\x04 \x01(\v26.qclaogui.generativelanguage.v1beta1.RetrievalMetadataH\x01R\x11retrievalMetadata\x88\x01\x01B\x15\n" +
	"\x13_search_entry_pointB\x15\n" +
	"\x13_retrieval_metadata\"d\n" +
	"\x10SearchEntryPoint\x12/\n" +
	"\x10rendered_content\x18\x01 \x01(\tB\x04\xe2A\x01\x01R\x0frenderedContent\x12\x1f\n" +
	"\bsdk_blob\x18\x02 \x01(\fB\x04\xe2A\x01\x01R\asdkBlob\"\xb6\x01\n" +
	"\x0eGroundingChunk\x12K\n" +
	"\x03web\x18\x01 \x01(\v27.qclaogui.generativelanguage.v1beta1.GroundingChunk.WebH\x00R\x03web\x1aI\n" +
	"\x03Web\x12\x15\n" +
	"\x03uri\x18\x01 \x01(\tH\x00R\x03uri\x88\x01\x01\x12\x19\n" +
	"\x05title\x18\x02 \x01(\tH\x01R\x05title\x88\x01\x01B\x06\n" +
	"\x04_uriB\b\n" +
	"\x06_titleB\f\n" +
	"\n" +
	"chunk_type\"\x92\x01\n" +
	"\aSegment\x12#\n" +
	"\n" +
	"part_index\x18\x01 \x01(\x05B\x04\xe2A\x01\x03R\tpartIndex\x12%\n" +
	"\vstart_index\x18\x02 \x01(\x05B\x04\xe2A\x01\x03R\n" +
	"startIndex\x12!\n" +
	"\tend_index\x18\x03 \x01(\x05B\x04\xe2A\x01\x03R\bendIndex\x12\x18\n" +
	"\x04text\x18\x04 \x01(\tB\x04\xe2A\x01\x03R\x04text\"\xd0\x01\n" +
	"\x10GroundingSupport\x12K\n" +
	"\asegment\x18\x01 \x01(\v2,.qclaogui.generativelanguage.v1beta1.SegmentH\x00R\asegment\x88\x01\x01\x126\n" +
	"\x17grounding_chunk_indices\x18\x02 \x03(\x05R\x15groundingChunkIndices\x12+\n" +
	"\x11confidence_scores\x18\x03 \x03(\x02R\x10confidenceScoresB\n" +
	"\n" +
	"\b_segment\"\xff\x05\n" +
	"\x15GenerateAnswerRequest\x12a\n" +
	"\x0finline_passages\x18\x06 \x01(\v26.qclaogui.generativelanguage.v1beta1.GroundingPassagesH\x00R\x0einlinePassages\x12m\n" +
	"\x12semantic_retriever\x18\a \x01(\v2<.qclaogui.generativelanguage.v1beta1.SemanticRetrieverConfigH\x00R\x11semanticRetriever\x12D\n" +
	"\x05model\x18\x01 \x01(\tB.\xe2A\x01\x02\xfaA'\n" +
	"%generativelanguage.qclaogui.com/ModelR\x05model\x12N\n" +
	"\bcontents\x18\x02 \x03(\v2,.qclaogui.generativelanguage.v1beta1.ContentB\x04\xe2A\x01\x02R\bcontents\x12o\n" +
	"\fanswer_style\x18\x05 \x01(\x0e2F.qclaogui.generativelanguage.v1beta1.GenerateAnswerRequest.AnswerStyleB\x04\xe2A\x01\x02R\vanswerStyle\x12a\n" +
	"\x0fsafety_settings\x18\x03 \x03(\v22.qclaogui.generativelanguage.v1beta1.SafetySettingB\x04\xe2A\x01\x01R\x0esafetySettings\x12+\n" +
	"\vtemperature\x18\x04 \x01(\x02B\x04\xe2A\x01\x01H\x01R\vtemperature\x88\x01\x01\"Y\n" +
	"\vAnswerStyle\x12\x1c\n" +
	"\x18ANSWER_STYLE_UNSPECIFIED\x10\x00\x12\x0f\n" +
	"\vABSTRACTIVE\x10\x01\x12\x0e\n" +
	"\n" +
	"EXTRACTIVE\x10\x02\x12\v\n" +
	"\aVERBOSE\x10\x03B\x12\n" +
	"\x10grounding_sourceB\x0e\n" +
	"\f_temperature\"\x94\x05\n" +
	"\x16GenerateAnswerResponse\x12F\n" +
	"\x06answer\x18\x01 \x01(\v2..qclaogui.generativelanguage.v1beta1.CandidateR\x06answer\x12@\n" +
	"\x16answerable_probability\x18\x02 \x01(\x02B\x04\xe2A\x01\x03H\x00R\x15answerableProbability\x88\x01\x01\x12{\n" +
	"\x0einput_feedback\x18\x03 \x01(\v2I.qclaogui.generativelanguage.v1beta1.GenerateAnswerResponse.InputFeedbackB\x04\xe2A\x01\x03H\x01R\rinputFeedback\x88\x01\x01\x1a\xc4\x02\n" +
	"\rInputFeedback\x12\x83\x01\n" +
	"\fblock_reason\x18\x01 \x01(\x0e2U.qclaogui.generativelanguage.v1beta1.GenerateAnswerResponse.InputFeedback.BlockReasonB\x04\xe2A\x01\x01H\x00R\vblockReason\x88\x01\x01\x12X\n" +
	"\x0esafety_ratings\x18\x02 \x03(\v21.qclaogui.generativelanguage.v1beta1.SafetyRatingR\rsafetyRatings\"B\n" +
	"\vBlockReason\x12\x1c\n" +
	"\x18BLOCK_REASON_UNSPECIFIED\x10\x00\x12\n" +
	"\n" +
	"\x06SAFETY\x10\x01\x12\t\n" +
	"\x05OTHER\x10\x02B\x0f\n" +
	"\r_block_reasonB\x19\n" +
	"\x17_answerable_probabilityB\x11\n" +
	"\x0f_input_feedback\"\x93\x03\n" +
	"\x13EmbedContentRequest\x12D\n" +
	"\x05model\x18\x01 \x01(\tB.\xe2A\x01\x02\xfaA'\n" +
	"%generativelanguage.qclaogui.com/ModelR\x05model\x12L\n" +
	"\acontent\x18\x02 \x01(\v2,.qclaogui.generativelanguage.v1beta1.ContentB\x04\xe2A\x01\x02R\acontent\x12U\n" +
	"\ttask_type\x18\x03 \x01(\x0e2-.qclaogui.generativelanguage.v1beta1.TaskTypeB\x04\xe2A\x01\x01H\x00R\btaskType\x88\x01\x01\x12\x1f\n" +
	"\x05title\x18\x04 \x01(\tB\x04\xe2A\x01\x01H\x01R\x05title\x88\x01\x01\x12>\n" +
	"\x15output_dimensionality\x18\x05 \x01(\x05B\x04\xe2A\x01\x01H\x02R\x14outputDimensionality\x88\x01\x01B\f\n" +
	"\n" +
	"_task_typeB\b\n" +
	"\x06_titleB\x18\n" +
	"\x16_output_dimensionality\"*\n" +
	"\x10ContentEmbedding\x12\x16\n" +
	"\x06values\x18\x01 \x03(\x02R\x06values\"q\n" +
	"\x14EmbedContentResponse\x12Y\n" +
	"\tembedding\x18\x01 \x01(\v25.qclaogui.generativelanguage.v1beta1.ContentEmbeddingB\x04\xe2A\x01\x03R\tembedding\"\xbd\x01\n" +
	"\x19BatchEmbedContentsRequest\x12D\n" +
	"\x05model\x18\x01 \x01(\tB.\xe2A\x01\x02\xfaA'\n" +
	"%generativelanguage.qclaogui.com/ModelR\x05model\x12Z\n" +
	"\brequests\x18\x02 \x03(\v28.qclaogui.generativelanguage.v1beta1.EmbedContentRequestB\x04\xe2A\x01\x02R\brequests\"y\n" +
	"\x1aBatchEmbedContentsResponse\x12[\n" +
	"\n" +
	"embeddings\x18\x01 \x03(\v25.qclaogui.generativelanguage.v1beta1.ContentEmbeddingB\x04\xe2A\x01\x03R\n" +
	"embeddings\"\xa7\x02\n" +
	"\x12CountTokensRequest\x12D\n" +
	"\x05model\x18\x01 \x01(\tB.\xe2A\x01\x02\xfaA'\n" +
	"%generativelanguage.qclaogui.com/ModelR\x05model\x12N\n" +
	"\bcontents\x18\x02 \x03(\v2,.qclaogui.generativelanguage.v1beta1.ContentB\x04\xe2A\x01\x01R\bcontents\x12{\n" +
	"\x18generate_content_request\x18\x03 \x01(\v2;.qclaogui.generativelanguage.v1beta1.GenerateContentRequestB\x04\xe2A\x01\x01R\x16generateContentRequest\"u\n" +
	"\x13CountTokensResponse\x12!\n" +
	"\ftotal_tokens\x18\x01 \x01(\x05R\vtotalTokens\x12;\n" +
	"\x1acached_content_token_count\x18\x05 \x01(\x05R\x17cachedContentTokenCount*\xbe\x01\n" +
	"\bTaskType\x12\x19\n" +
	"\x15TASK_TYPE_UNSPECIFIED\x10\x00\x12\x13\n" +
	"\x0fRETRIEVAL_QUERY\x10\x01\x12\x16\n" +
	"\x12RETRIEVAL_DOCUMENT\x10\x02\x12\x17\n" +
	"\x13SEMANTIC_SIMILARITY\x10\x03\x12\x12\n" +
	"\x0eCLASSIFICATION\x10\x04\x12\x0e\n" +
	"\n" +
	"CLUSTERING\x10\x05\x12\x16\n" +
	"\x12QUESTION_ANSWERING\x10\x06\x12\x15\n" +
	"\x11FACT_VERIFICATION\x10\a2\xff\n" +
	"\n" +
	"\x11GenerativeService\x12\x86\x02\n" +
	"\x0fGenerateContent\x12;.qclaogui.generativelanguage.v1beta1.GenerateContentRequest\x1a<.qclaogui.generativelanguage.v1beta1.GenerateContentResponse\"x\xdaA\x0emodel,contents\x82\xd3\xe4\x93\x02a:\x01*Z2:\x01*\"-/v1beta/{model=tunedModels/*}:generateContent\"(/v1beta/{model=models/*}:generateContent\x12\xeb\x01\n" +
	"\x0eGenerateAnswer\x12:.qclaogui.generativelanguage.v1beta1.GenerateAnswerRequest\x1a;.qclaogui.generativelanguage.v1beta1.GenerateAnswerResponse\"`\xdaA+model,contents,safety_settings,answer_style\x82\xd3\xe4\x93\x02,:\x01*\"'/v1beta/{model=models/*}:generateAnswer\x12\xe0\x01\n" +
	"\x15StreamGenerateContent\x12;.qclaogui.generativelanguage.v1beta1.GenerateContentRequest\x1a<.qclaogui.generativelanguage.v1beta1.GenerateContentResponse\"J\xdaA\x0emodel,contents\x82\xd3\xe4\x93\x023:\x01*\"./v1beta/{model=models/*}:streamGenerateContent0\x01\x12\xc5\x01\n" +
	"\fEmbedContent\x128.qclaogui.generativelanguage.v1beta1.EmbedContentRequest\x1a9.qclaogui.generativelanguage.v1beta1.EmbedContentResponse\"@\xdaA\rmodel,content\x82\xd3\xe4\x93\x02*:\x01*\"%/v1beta/{model=models/*}:embedContent\x12\xde\x01\n" +
	"\x12BatchEmbedContents\x12>.qclaogui.generativelanguage.v1beta1.BatchEmbedContentsRequest\x1a?.qclaogui.generativelanguage.v1beta1.BatchEmbedContentsResponse\"G\xdaA\x0emodel,requests\x82\xd3\xe4\x93\x020:\x01*\"+/v1beta/{model=models/*}:batchEmbedContents\x12\xc2\x01\n" +
	"\vCountTokens\x127.qclaogui.generativelanguage.v1beta1.CountTokensRequest\x1a8.qclaogui.generativelanguage.v1beta1.CountTokensResponse\"@\xdaA\x0emodel,contents\x82\xd3\xe4\x93\x02):\x01*\"$/v1beta/{model=models/*}:countTokens\x1a\"\xcaA\x1fgenerativelanguage.qclaogui.comBVZTgithub.com/qclaogui/gaip/genproto/generativelanguage/apiv1beta1/generativelanguagepbb\x06proto3"

var (
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescOnce sync.Once
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescData []byte
)

func file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescGZIP() []byte {
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescOnce.Do(func() {
		file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDesc), len(file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDesc)))
	})
	return file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDescData
}

var (
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_enumTypes = make([]protoimpl.EnumInfo, 5)
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes  = make([]protoimpl.MessageInfo, 31)
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_goTypes   = []any{
		(TaskType)(0), // 0: qclaogui.generativelanguage.v1beta1.TaskType
		(GenerateContentResponse_PromptFeedback_BlockReason)(0), // 1: qclaogui.generativelanguage.v1beta1.GenerateContentResponse.PromptFeedback.BlockReason
		(Candidate_FinishReason)(0),                             // 2: qclaogui.generativelanguage.v1beta1.Candidate.FinishReason
		(GenerateAnswerRequest_AnswerStyle)(0),                  // 3: qclaogui.generativelanguage.v1beta1.GenerateAnswerRequest.AnswerStyle
		(GenerateAnswerResponse_InputFeedback_BlockReason)(0),   // 4: qclaogui.generativelanguage.v1beta1.GenerateAnswerResponse.InputFeedback.BlockReason
		(*GenerateContentRequest)(nil),                          // 5: qclaogui.generativelanguage.v1beta1.GenerateContentRequest
		(*GenerationConfig)(nil),                                // 6: qclaogui.generativelanguage.v1beta1.GenerationConfig
		(*SemanticRetrieverConfig)(nil),                         // 7: qclaogui.generativelanguage.v1beta1.SemanticRetrieverConfig
		(*GenerateContentResponse)(nil),                         // 8: qclaogui.generativelanguage.v1beta1.GenerateContentResponse
		(*Candidate)(nil),                                       // 9: qclaogui.generativelanguage.v1beta1.Candidate
		(*LogprobsResult)(nil),                                  // 10: qclaogui.generativelanguage.v1beta1.LogprobsResult
		(*AttributionSourceId)(nil),                             // 11: qclaogui.generativelanguage.v1beta1.AttributionSourceId
		(*GroundingAttribution)(nil),                            // 12: qclaogui.generativelanguage.v1beta1.GroundingAttribution
		(*RetrievalMetadata)(nil),                               // 13: qclaogui.generativelanguage.v1beta1.RetrievalMetadata
		(*GroundingMetadata)(nil),                               // 14: qclaogui.generativelanguage.v1beta1.GroundingMetadata
		(*SearchEntryPoint)(nil),                                // 15: qclaogui.generativelanguage.v1beta1.SearchEntryPoint
		(*GroundingChunk)(nil),                                  // 16: qclaogui.generativelanguage.v1beta1.GroundingChunk
		(*Segment)(nil),                                         // 17: qclaogui.generativelanguage.v1beta1.Segment
		(*GroundingSupport)(nil),                                // 18: qclaogui.generativelanguage.v1beta1.GroundingSupport
		(*GenerateAnswerRequest)(nil),                           // 19: qclaogui.generativelanguage.v1beta1.GenerateAnswerRequest
		(*GenerateAnswerResponse)(nil),                          // 20: qclaogui.generativelanguage.v1beta1.GenerateAnswerResponse
		(*EmbedContentRequest)(nil),                             // 21: qclaogui.generativelanguage.v1beta1.EmbedContentRequest
		(*ContentEmbedding)(nil),                                // 22: qclaogui.generativelanguage.v1beta1.ContentEmbedding
		(*EmbedContentResponse)(nil),                            // 23: qclaogui.generativelanguage.v1beta1.EmbedContentResponse
		(*BatchEmbedContentsRequest)(nil),                       // 24: qclaogui.generativelanguage.v1beta1.BatchEmbedContentsRequest
		(*BatchEmbedContentsResponse)(nil),                      // 25: qclaogui.generativelanguage.v1beta1.BatchEmbedContentsResponse
		(*CountTokensRequest)(nil),                              // 26: qclaogui.generativelanguage.v1beta1.CountTokensRequest
		(*CountTokensResponse)(nil),                             // 27: qclaogui.generativelanguage.v1beta1.CountTokensResponse
		(*GenerateContentResponse_PromptFeedback)(nil),          // 28: qclaogui.generativelanguage.v1beta1.GenerateContentResponse.PromptFeedback
		(*GenerateContentResponse_UsageMetadata)(nil),           // 29: qclaogui.generativelanguage.v1beta1.GenerateContentResponse.UsageMetadata
		(*LogprobsResult_Candidate)(nil),                        // 30: qclaogui.generativelanguage.v1beta1.LogprobsResult.Candidate
		(*LogprobsResult_TopCandidates)(nil),                    // 31: qclaogui.generativelanguage.v1beta1.LogprobsResult.TopCandidates
		(*AttributionSourceId_GroundingPassageId)(nil),          // 32: qclaogui.generativelanguage.v1beta1.AttributionSourceId.GroundingPassageId
		(*AttributionSourceId_SemanticRetrieverChunk)(nil),      // 33: qclaogui.generativelanguage.v1beta1.AttributionSourceId.SemanticRetrieverChunk
		(*GroundingChunk_Web)(nil),                              // 34: qclaogui.generativelanguage.v1beta1.GroundingChunk.Web
		(*GenerateAnswerResponse_InputFeedback)(nil),            // 35: qclaogui.generativelanguage.v1beta1.GenerateAnswerResponse.InputFeedback
		(*Content)(nil),                                         // 36: qclaogui.generativelanguage.v1beta1.Content
		(*Tool)(nil),                                            // 37: qclaogui.generativelanguage.v1beta1.Tool
		(*ToolConfig)(nil),                                      // 38: qclaogui.generativelanguage.v1beta1.ToolConfig
		(*SafetySetting)(nil),                                   // 39: qclaogui.generativelanguage.v1beta1.SafetySetting
		(*Schema)(nil),                                          // 40: qclaogui.generativelanguage.v1beta1.Schema
		(*MetadataFilter)(nil),                                  // 41: qclaogui.generativelanguage.v1beta1.MetadataFilter
		(*SafetyRating)(nil),                                    // 42: qclaogui.generativelanguage.v1beta1.SafetyRating
		(*CitationMetadata)(nil),                                // 43: qclaogui.generativelanguage.v1beta1.CitationMetadata
		(*GroundingPassages)(nil),                               // 44: qclaogui.generativelanguage.v1beta1.GroundingPassages
	}
)

var file_qclaogui_generativelanguage_v1beta1_generative_service_proto_depIdxs = []int32{
	36, // 0: qclaogui.generativelanguage.v1beta1.GenerateContentRequest.system_instruction:type_name -> qclaogui.generativelanguage.v1beta1.Content
	36, // 1: qclaogui.generativelanguage.v1beta1.GenerateContentRequest.contents:type_name -> qclaogui.generativelanguage.v1beta1.Content
	37, // 2: qclaogui.generativelanguage.v1beta1.GenerateContentRequest.tools:type_name -> qclaogui.generativelanguage.v1beta1.Tool
	38, // 3: qclaogui.generativelanguage.v1beta1.GenerateContentRequest.tool_config:type_name -> qclaogui.generativelanguage.v1beta1.ToolConfig
	39, // 4: qclaogui.generativelanguage.v1beta1.GenerateContentRequest.safety_settings:type_name -> qclaogui.generativelanguage.v1beta1.SafetySetting
	6,  // 5: qclaogui.generativelanguage.v1beta1.GenerateContentRequest.generation_config:type_name -> qclaogui.generativelanguage.v1beta1.GenerationConfig
	40, // 6: qclaogui.generativelanguage.v1beta1.GenerationConfig.response_schema:type_name -> qclaogui.generativelanguage.v1beta1.Schema
	36, // 7: qclaogui.generativelanguage.v1beta1.SemanticRetrieverConfig.query:type_name -> qclaogui.generativelanguage.v1beta1.Content
	41, // 8: qclaogui.generativelanguage.v1beta1.SemanticRetrieverConfig.metadata_filters:type_name -> qclaogui.generativelanguage.v1beta1.MetadataFilter
	9,  // 9: qclaogui.generativelanguage.v1beta1.GenerateContentResponse.candidates:type_name -> qclaogui.generativelanguage.v1beta1.Candidate
	28, // 10: qclaogui.generativelanguage.v1beta1.GenerateContentResponse.prompt_feedback:type_name -> qclaogui.generativelanguage.v1beta1.GenerateContentResponse.PromptFeedback
	29, // 11: qclaogui.generativelanguage.v1beta1.GenerateContentResponse.usage_metadata:type_name -> qclaogui.generativelanguage.v1beta1.GenerateContentResponse.UsageMetadata
	36, // 12: qclaogui.generativelanguage.v1beta1.Candidate.content:type_name -> qclaogui.generativelanguage.v1beta1.Content
	2,  // 13: qclaogui.generativelanguage.v1beta1.Candidate.finish_reason:type_name -> qclaogui.generativelanguage.v1beta1.Candidate.FinishReason
	42, // 14: qclaogui.generativelanguage.v1beta1.Candidate.safety_ratings:type_name -> qclaogui.generativelanguage.v1beta1.SafetyRating
	43, // 15: qclaogui.generativelanguage.v1beta1.Candidate.citation_metadata:type_name -> qclaogui.generativelanguage.v1beta1.CitationMetadata
	12, // 16: qclaogui.generativelanguage.v1beta1.Candidate.grounding_attributions:type_name -> qclaogui.generativelanguage.v1beta1.GroundingAttribution
	14, // 17: qclaogui.generativelanguage.v1beta1.Candidate.grounding_metadata:type_name -> qclaogui.generativelanguage.v1beta1.GroundingMetadata
	10, // 18: qclaogui.generativelanguage.v1beta1.Candidate.logprobs_result:type_name -> qclaogui.generativelanguage.v1beta1.LogprobsResult
	31, // 19: qclaogui.generativelanguage.v1beta1.LogprobsResult.top_candidates:type_name -> qclaogui.generativelanguage.v1beta1.LogprobsResult.TopCandidates
	30, // 20: qclaogui.generativelanguage.v1beta1.LogprobsResult.chosen_candidates:type_name -> qclaogui.generativelanguage.v1beta1.LogprobsResult.Candidate
	32, // 21: qclaogui.generativelanguage.v1beta1.AttributionSourceId.grounding_passage:type_name -> qclaogui.generativelanguage.v1beta1.AttributionSourceId.GroundingPassageId
	33, // 22: qclaogui.generativelanguage.v1beta1.AttributionSourceId.semantic_retriever_chunk:type_name -> qclaogui.generativelanguage.v1beta1.AttributionSourceId.SemanticRetrieverChunk
	11, // 23: qclaogui.generativelanguage.v1beta1.GroundingAttribution.source_id:type_name -> qclaogui.generativelanguage.v1beta1.AttributionSourceId
	36, // 24: qclaogui.generativelanguage.v1beta1.GroundingAttribution.content:type_name -> qclaogui.generativelanguage.v1beta1.Content
	15, // 25: qclaogui.generativelanguage.v1beta1.GroundingMetadata.search_entry_point:type_name -> qclaogui.generativelanguage.v1beta1.SearchEntryPoint
	16, // 26: qclaogui.generativelanguage.v1beta1.GroundingMetadata.grounding_chunks:type_name -> qclaogui.generativelanguage.v1beta1.GroundingChunk
	18, // 27: qclaogui.generativelanguage.v1beta1.GroundingMetadata.grounding_supports:type_name -> qclaogui.generativelanguage.v1beta1.GroundingSupport
	13, // 28: qclaogui.generativelanguage.v1beta1.GroundingMetadata.retrieval_metadata:type_name -> qclaogui.generativelanguage.v1beta1.RetrievalMetadata
	34, // 29: qclaogui.generativelanguage.v1beta1.GroundingChunk.web:type_name -> qclaogui.generativelanguage.v1beta1.GroundingChunk.Web
	17, // 30: qclaogui.generativelanguage.v1beta1.GroundingSupport.segment:type_name -> qclaogui.generativelanguage.v1beta1.Segment
	44, // 31: qclaogui.generativelanguage.v1beta1.GenerateAnswerRequest.inline_passages:type_name -> qclaogui.generativelanguage.v1beta1.GroundingPassages
	7,  // 32: qclaogui.generativelanguage.v1beta1.GenerateAnswerRequest.semantic_retriever:type_name -> qclaogui.generativelanguage.v1beta1.SemanticRetrieverConfig
	36, // 33: qclaogui.generativelanguage.v1beta1.GenerateAnswerRequest.contents:type_name -> qclaogui.generativelanguage.v1beta1.Content
	3,  // 34: qclaogui.generativelanguage.v1beta1.GenerateAnswerRequest.answer_style:type_name -> qclaogui.generativelanguage.v1beta1.GenerateAnswerRequest.AnswerStyle
	39, // 35: qclaogui.generativelanguage.v1beta1.GenerateAnswerRequest.safety_settings:type_name -> qclaogui.generativelanguage.v1beta1.SafetySetting
	9,  // 36: qclaogui.generativelanguage.v1beta1.GenerateAnswerResponse.answer:type_name -> qclaogui.generativelanguage.v1beta1.Candidate
	35, // 37: qclaogui.generativelanguage.v1beta1.GenerateAnswerResponse.input_feedback:type_name -> qclaogui.generativelanguage.v1beta1.GenerateAnswerResponse.InputFeedback
	36, // 38: qclaogui.generativelanguage.v1beta1.EmbedContentRequest.content:type_name -> qclaogui.generativelanguage.v1beta1.Content
	0,  // 39: qclaogui.generativelanguage.v1beta1.EmbedContentRequest.task_type:type_name -> qclaogui.generativelanguage.v1beta1.TaskType
	22, // 40: qclaogui.generativelanguage.v1beta1.EmbedContentResponse.embedding:type_name -> qclaogui.generativelanguage.v1beta1.ContentEmbedding
	21, // 41: qclaogui.generativelanguage.v1beta1.BatchEmbedContentsRequest.requests:type_name -> qclaogui.generativelanguage.v1beta1.EmbedContentRequest
	22, // 42: qclaogui.generativelanguage.v1beta1.BatchEmbedContentsResponse.embeddings:type_name -> qclaogui.generativelanguage.v1beta1.ContentEmbedding
	36, // 43: qclaogui.generativelanguage.v1beta1.CountTokensRequest.contents:type_name -> qclaogui.generativelanguage.v1beta1.Content
	5,  // 44: qclaogui.generativelanguage.v1beta1.CountTokensRequest.generate_content_request:type_name -> qclaogui.generativelanguage.v1beta1.GenerateContentRequest
	1,  // 45: qclaogui.generativelanguage.v1beta1.GenerateContentResponse.PromptFeedback.block_reason:type_name -> qclaogui.generativelanguage.v1beta1.GenerateContentResponse.PromptFeedback.BlockReason
	42, // 46: qclaogui.generativelanguage.v1beta1.GenerateContentResponse.PromptFeedback.safety_ratings:type_name -> qclaogui.generativelanguage.v1beta1.SafetyRating
	30, // 47: qclaogui.generativelanguage.v1beta1.LogprobsResult.TopCandidates.candidates:type_name -> qclaogui.generativelanguage.v1beta1.LogprobsResult.Candidate
	4,  // 48: qclaogui.generativelanguage.v1beta1.GenerateAnswerResponse.InputFeedback.block_reason:type_name -> qclaogui.generativelanguage.v1beta1.GenerateAnswerResponse.InputFeedback.BlockReason
	42, // 49: qclaogui.generativelanguage.v1beta1.GenerateAnswerResponse.InputFeedback.safety_ratings:type_name -> qclaogui.generativelanguage.v1beta1.SafetyRating
	5,  // 50: qclaogui.generativelanguage.v1beta1.GenerativeService.GenerateContent:input_type -> qclaogui.generativelanguage.v1beta1.GenerateContentRequest
	19, // 51: qclaogui.generativelanguage.v1beta1.GenerativeService.GenerateAnswer:input_type -> qclaogui.generativelanguage.v1beta1.GenerateAnswerRequest
	5,  // 52: qclaogui.generativelanguage.v1beta1.GenerativeService.StreamGenerateContent:input_type -> qclaogui.generativelanguage.v1beta1.GenerateContentRequest
	21, // 53: qclaogui.generativelanguage.v1beta1.GenerativeService.EmbedContent:input_type -> qclaogui.generativelanguage.v1beta1.EmbedContentRequest
	24, // 54: qclaogui.generativelanguage.v1beta1.GenerativeService.BatchEmbedContents:input_type -> qclaogui.generativelanguage.v1beta1.BatchEmbedContentsRequest
	26, // 55: qclaogui.generativelanguage.v1beta1.GenerativeService.CountTokens:input_type -> qclaogui.generativelanguage.v1beta1.CountTokensRequest
	8,  // 56: qclaogui.generativelanguage.v1beta1.GenerativeService.GenerateContent:output_type -> qclaogui.generativelanguage.v1beta1.GenerateContentResponse
	20, // 57: qclaogui.generativelanguage.v1beta1.GenerativeService.GenerateAnswer:output_type -> qclaogui.generativelanguage.v1beta1.GenerateAnswerResponse
	8,  // 58: qclaogui.generativelanguage.v1beta1.GenerativeService.StreamGenerateContent:output_type -> qclaogui.generativelanguage.v1beta1.GenerateContentResponse
	23, // 59: qclaogui.generativelanguage.v1beta1.GenerativeService.EmbedContent:output_type -> qclaogui.generativelanguage.v1beta1.EmbedContentResponse
	25, // 60: qclaogui.generativelanguage.v1beta1.GenerativeService.BatchEmbedContents:output_type -> qclaogui.generativelanguage.v1beta1.BatchEmbedContentsResponse
	27, // 61: qclaogui.generativelanguage.v1beta1.GenerativeService.CountTokens:output_type -> qclaogui.generativelanguage.v1beta1.CountTokensResponse
	56, // [56:62] is the sub-list for method output_type
	50, // [50:56] is the sub-list for method input_type
	50, // [50:50] is the sub-list for extension type_name
	50, // [50:50] is the sub-list for extension extendee
	0,  // [0:50] is the sub-list for field type_name
}

func init() { file_qclaogui_generativelanguage_v1beta1_generative_service_proto_init() }
func file_qclaogui_generativelanguage_v1beta1_generative_service_proto_init() {
	if File_qclaogui_generativelanguage_v1beta1_generative_service_proto != nil {
		return
	}
	file_qclaogui_generativelanguage_v1beta1_citation_proto_init()
	file_qclaogui_generativelanguage_v1beta1_content_proto_init()
	file_qclaogui_generativelanguage_v1beta1_retriever_proto_init()
	file_qclaogui_generativelanguage_v1beta1_safety_proto_init()
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[0].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[1].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[2].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[4].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[6].OneofWrappers = []any{
		(*AttributionSourceId_GroundingPassage)(nil),
		(*AttributionSourceId_SemanticRetrieverChunk_)(nil),
	}
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[9].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[11].OneofWrappers = []any{
		(*GroundingChunk_Web_)(nil),
	}
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[13].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[14].OneofWrappers = []any{
		(*GenerateAnswerRequest_InlinePassages)(nil),
		(*GenerateAnswerRequest_SemanticRetriever)(nil),
	}
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[15].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[16].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[25].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[29].OneofWrappers = []any{}
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes[30].OneofWrappers = []any{}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDesc), len(file_qclaogui_generativelanguage_v1beta1_generative_service_proto_rawDesc)),
			NumEnums:      5,
			NumMessages:   31,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_qclaogui_generativelanguage_v1beta1_generative_service_proto_goTypes,
		DependencyIndexes: file_qclaogui_generativelanguage_v1beta1_generative_service_proto_depIdxs,
		EnumInfos:         file_qclaogui_generativelanguage_v1beta1_generative_service_proto_enumTypes,
		MessageInfos:      file_qclaogui_generativelanguage_v1beta1_generative_service_proto_msgTypes,
	}.Build()
	File_qclaogui_generativelanguage_v1beta1_generative_service_proto = out.File
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_goTypes = nil
	file_qclaogui_generativelanguage_v1beta1_generative_service_proto_depIdxs = nil
}
